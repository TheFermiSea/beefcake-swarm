{"id":"beefcake-swarm-0ko","title":"Add GBNF grammar-constrained structured output for implementer patches","description":"The apply_implementer_changes stub (main.rs:91-110) is the #1 blocker identified by all 4 consulted models. llama.cpp supports GBNF grammars and automatic JSON schema conversion via the json_schema body field in /v1/chat/completions. Define an ImplementerOutput JSON schema (list of file edits with paths, search/replace blocks or unified hunks, optional reasoning). Pass schema via json_schema field to force structured output. Implement real patch application by parsing the validated JSON response. Fallback: search-replace blocks (Aider pattern). Files: crates/swarm-agents/src/main.rs (apply_implementer_changes), crates/swarm-agents/src/implementer.rs, new file crates/swarm-agents/src/patch_applicator.rs. Reference: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md","status":"open","priority":0,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:48:49.241001-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:48:49.241001-06:00"}
{"id":"beefcake-swarm-0l5","title":"Make WorkPacket constraints configurable","description":"WorkPacketGenerator has hardcoded default constraints (line 46: 'No new dependencies without explicit approval', 'Don't break existing public API'). These are baked into the binary and cannot be overridden per-task.\n\nThis is problematic for tasks that intentionally require adding dependencies, changing public API, or have different LOC limits.\n\nFix: Move default constraints into SwarmConfig (or a new PackerConfig). Allow per-issue constraint overrides via beads issue labels or metadata. The ContextPacker should accept optional extra constraints that merge with (or replace) the defaults.\n\nConsider: constraints could come from beads issue fields, a .beefcake.toml project config, or CLI flags.\n\nFiles: coordination/src/work_packet/generator.rs, crates/swarm-agents/src/config.rs, coordination/src/context_packer/packer.rs\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:20.875919-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:20.875919-06:00"}
{"id":"beefcake-swarm-0lh","title":"Study Goose/Aider/OpenHands patch engines for apply_implementer_changes","description":"Research patch application patterns from existing tools: (1) Aider: search-replace blocks with \u003c\u003c\u003c\u003c SEARCH / ==== REPLACE / \u003e\u003e\u003e\u003e markers, (2) OpenHands/SWE-agent: unified diff with patch command, (3) Goose by Block: closest existing product to our swarm (study their architecture and patch engine). Document pros/cons of each approach for our structured output pipeline. Determine which pattern best complements GBNF-constrained JSON output. G3-Pro identified Goose as most architecturally similar. GPT-5.2-Codex recommended OpenHands diff-structured outputs. Deliverable: design doc + recommendation.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:50.77437-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:50.77437-06:00","dependencies":[{"issue_id":"beefcake-swarm-0lh","depends_on_id":"beefcake-swarm-0ko","type":"blocks","created_at":"2026-02-11T16:50:22.704981-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-0wn","title":"Phase 1: Wire coordination/ harness into swarm-agents orchestrator","status":"closed","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-15T10:10:19.999712-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T12:40:55.524693-06:00","closed_at":"2026-02-15T12:40:55.524693-06:00","close_reason":"Phase 1 complete: PR #2 squash-merged to main"}
{"id":"beefcake-swarm-10m","title":"Upgrade ContextPacker to span-aware error-driven retrieval (Potpie pattern)","description":"All 4 models identified that current context packing (first 30 lines per file) misses critical information. Steal Potpie pattern: on verifier failures, use rustc error spans to retrieve relevant code regions. Specifically: (1) Parse rustc JSON error spans (file + line range), (2) Pack ±80 lines around each error span, (3) Include trait definitions and impl blocks referenced in trait bound errors, (4) Include callers/callees of symbols in error messages. Depends on tree-sitter integration for symbol graph. Files: coordination/src/context_packer/packer.rs (pack_retry enhancement), coordination/src/verifier/report.rs (span extraction from rustc JSON). Depends on tree-sitter issue.","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:13.808572-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:13.808572-06:00","dependencies":[{"issue_id":"beefcake-swarm-10m","depends_on_id":"beefcake-swarm-5bk","type":"blocks","created_at":"2026-02-11T16:50:22.460109-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-1ep","title":"Enforce max_turns on manager agent (rig doesn't enforce on outer agent)","description":"Job 1608: rig's default_max_turns(25) is only enforced on nested tool-agents (workers get MaxTurnError at 25). The outer manager agent exceeds its limit (logs show depth 27/25, 30/25) and keeps spawning workers indefinitely. Fix: wrap manager.prompt() with a tokio timeout + manual turn enforcement. The manager at 50 turns × workers at 50 turns = 2500 potential LLM calls per iteration without enforcement.","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-16T08:53:40.834064-06:00","created_by":"TheFermiSea","updated_at":"2026-02-16T08:55:24.774898-06:00","closed_at":"2026-02-16T08:55:24.774898-06:00","close_reason":"Added 10-min tokio::time::timeout around manager.prompt() to hard-cap runaway agents"}
{"id":"beefcake-swarm-2fd","title":"Add integration test for orchestrator loop","description":"There is no test that validates the full orchestrator loop data flow: ContextPacker -\u003e format_work_packet -\u003e Implementer -\u003e apply_changes -\u003e Verifier -\u003e Validator. Each component is tested in isolation but the wiring between them is only exercised by running the real binary against a live inference server.\n\nFix: Create a test in crates/swarm-agents/tests/ that:\n1. Uses mock Implementer (returns a fixed code change)\n2. Uses mock Validator (returns PASS)\n3. Sets up a real temp git repo with a known-broken Rust file\n4. Runs the loop for 1 iteration\n5. Verifies: pack_initial called, implementer received formatted prompt, verifier ran, validator received diff, issue closed\n\nThis depends on extracting agent traits (beefcake-swarm-AGENT_TRAITS_ID) so mocks can be injected.\n\nFiles: crates/swarm-agents/tests/ (new), crates/swarm-agents/src/main.rs\nDepends on: agent traits extraction\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:35.314013-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:35.314013-06:00","dependencies":[{"issue_id":"beefcake-swarm-2fd","depends_on_id":"beefcake-swarm-7ny","type":"blocks","created_at":"2026-02-11T16:15:38.805195-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-2jk","title":"Extract shared SourceFileProvider to deduplicate file reading","description":"Both ContextPacker::build_file_contexts() and WorkPacketGenerator::extract_symbols_from_files() independently read files from disk and iterate lines. This duplicates I/O when both are called in sequence (e.g. pack_initial calls the generator then builds its own contexts).\n\nFix: Extract a SourceFileProvider struct that:\n1. Caches file content in a HashMap\u003cPathBuf, String\u003e on first read\n2. Provides methods: get_content(path) -\u003e \u0026str, get_header(path, lines: usize) -\u003e \u0026str, get_lines_around(path, line, context) -\u003e \u0026str\n3. Is shared between WorkPacketGenerator and ContextPacker via reference\n\nThis eliminates redundant disk reads and provides a single point for file access patterns.\n\nFiles: coordination/src/work_packet/generator.rs, coordination/src/context_packer/packer.rs\nFound by: G3-Pro deep review","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:24.141768-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:14:24.141768-06:00"}
{"id":"beefcake-swarm-2w0","title":"Increase worker agent depth limits (10→25)","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-15T18:45:30.638034-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T19:47:28.377886-06:00","closed_at":"2026-02-15T19:47:28.377886-06:00","close_reason":"Fixed in dogfood round 2"}
{"id":"beefcake-swarm-3is","title":"Unified Knowledge Graph Integration","description":"Root epic: Bridge documentation and code for the agent swarm via a Unified Knowledge Graph backed by SurrealDB (RocksDB storage engine). Covers infrastructure, code graph ingestion, document ingestion, WorkPacket integration, self-learning loop, benchmarking, and MCP tool exposure.","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:01.118138-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.068925-06:00","labels":["epic:ukg"]}
{"id":"beefcake-swarm-3is.1","title":"UKG: Infrastructure \u0026 Deployment","description":"Sub-epic 1: Deploy SurrealDB on ai-proxy LXC, design schema, create Rust client, investigate SurrealDB vs direct RocksDB for existing stores.","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:18.779902-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.156635-06:00","labels":["epic:ukg","infrastructure"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:18.780758-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.1.1","title":"Research SurrealDB deployment for HPC/LXC","description":"Research SurrealDB deployment best practices via PAL consensus. Compare embedded (RocksDB backend) vs networked (TiKV), auth/TLS setup, systemd config, resource requirements for ai-proxy LXC. Include SurrealDB 2.x architecture (compute/storage separation). Output: deployment decision document.","notes":"PAL consensus prompt: Compare SurrealDB embedded (RocksDB) vs networked (TiKV) for a single-node LXC deployment. Consider: memory footprint, persistence guarantees, auth setup, systemd integration. Expected output: config template + systemd unit.\n\n## RESEARCH OUTPUT: SurrealDB Deployment for HPC/LXC\n\n### Executive Decision\n**RECOMMENDATION: Embedded RocksDB mode on ai-proxy LXC**\n\nTiKV is unnecessary for a single-node deployment. RocksDB embedded mode provides the best performance with zero network overhead, simpler operations, and lower resource consumption. TiKV only makes sense for multi-node HA deployments.\n\n---\n\n### 1. Architecture Overview\n\nSurrealDB separates compute (query layer) and storage layers:\n- **Query Layer**: Parser → Executor → Iterator → Document Processor (stateless)\n- **Storage Layer**: RocksDB (default), SurrealKV (experimental), TiKV (distributed)\n\nFor single-node: compute + storage run in the same process. No external dependencies.\n\n### 2. Storage Engine Comparison (Single-Node Context)\n\n| Aspect | RocksDB (Embedded) | TiKV (Single-Node) | SurrealKV |\n|--------|-------------------|--------------------|-----------| \n| Maturity | Industry standard (Meta) | Production-ready but overkill | Experimental, Rust-native |\n| Dependencies | C++ lib (compiles with SurrealDB) | Separate PD + TiKV services | None (pure Rust) |\n| Performance | ~508k reads/s, ~155k writes/s | Same (TiKV uses RocksDB internally) + network overhead | Not yet benchmarked |\n| Operational complexity | Minimal — single binary | High — 3 separate daemons | Minimal |\n| HA/Replication | None (single-node) | Multi-raft (overkill for 1 node) | None |\n| Features | Standard KV | Distributed transactions | Time-travel queries, versioned data |\n\n**Decision: RocksDB.** TiKV adds PD + TiKV daemon overhead with zero benefit on single node. SurrealKV is still experimental.\n\n### 3. Resource Requirements\n\n**Minimum (ai-proxy LXC):**\n- CPU: 1+ vCPU (SurrealDB Cloud free tier runs on 0.25 vCPU)\n- RAM: 512 MB minimum, 2-4 GB recommended for knowledge graph + vector indexes\n- Disk: SSD/NVMe preferred (RocksDB LSM-tree optimized for SSDs)\n- The HNSW vector index uses a bounded memory cache (default 256 MiB, configurable)\n\n**Production recommended (for reference):**\n- 4+ cores, 8+ GB RAM, NVMe/SSD, no swap\n\nFor our UKG use case on ai-proxy, 2-4 GB RAM should be sufficient. RocksDB dynamically calculates block cache size based on available memory (`SURREAL_ROCKSDB_BLOCK_CACHE_SIZE`).\n\n### 4. Installation\n\n```bash\n# Install SurrealDB binary\ncurl --proto '=https' --tlsv1.2 -sSf https://install.surrealdb.com | sh\n\n# Verify\nsurreal version\n```\n\n### 5. Start Command (Production Config)\n\n```bash\nsurreal start \\\n  --log info \\\n  --user root \\\n  --pass '\u003cSTRONG_PASSWORD\u003e' \\\n  --bind 0.0.0.0:8000 \\\n  rocksdb:///var/lib/surrealdb/data\n```\n\nKey flags:\n- `--bind 0.0.0.0:8000` — Listen on all interfaces (required for 10.0.0.0/24 access from compute nodes)\n- `--log info` — Production log level (debug/trace impact performance)\n- `--query-timeout \u003cduration\u003e` — Optional max query execution time\n- `--transaction-timeout \u003cduration\u003e` — Optional max transaction time\n- `--web-crt` / `--web-key` — TLS certificate/key paths (optional for internal network)\n- `--allow-net` / `--deny-net` — Control outbound network capabilities\n- `--allow-funcs` / `--deny-funcs` — Restrict available SurrealQL functions\n\n### 6. Systemd Unit File\n\n```ini\n[Unit]\nDescription=SurrealDB Server\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=surrealdb\nGroup=surrealdb\nExecStart=/usr/local/bin/surreal start \\\n  --log info \\\n  --bind 0.0.0.0:8000 \\\n  rocksdb:///var/lib/surrealdb/data\nRestart=always\nRestartSec=5\nLimitNOFILE=65536\nEnvironment=SURREAL_ROCKSDB_BLOCK_CACHE_SIZE=536870912\nEnvironment=SURREAL_SYNC_DATA=true\n\n[Install]\nWantedBy=multi-user.target\n```\n\nNotes:\n- Root user credentials persist after first start (no need in subsequent starts)\n- `LimitNOFILE=65536` — RocksDB opens many file descriptors\n- `SURREAL_SYNC_DATA=true` — Ensures write durability (default)\n- `SURREAL_ROCKSDB_BLOCK_CACHE_SIZE=536870912` — 512 MB block cache (tune based on available RAM)\n\n### 7. Environment Variables (Key Tuning)\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `SURREAL_BIND` | 127.0.0.1:8000 | Bind address |\n| `SURREAL_LOG` | info | Log level |\n| `SURREAL_ROCKSDB_BLOCK_CACHE_SIZE` | Auto (based on RAM) | Read cache size |\n| `SURREAL_ROCKSDB_WRITE_BUFFER_SIZE` | Default | Write buffer memory |\n| `SURREAL_ROCKSDB_MAX_WRITE_BUFFER_NUMBER` | Default | Max concurrent write buffers |\n| `SURREAL_SYNC_DATA` | true | Fsync writes to disk |\n| `SURREAL_QUERY_TIMEOUT` | None | Max query execution time |\n\n### 8. Authentication \u0026 Security\n\n- **Auth enabled by default** since SurrealDB 2.0 (good)\n- First start with `--user`/`--pass` creates OWNER-role root user, persisted in storage\n- For internal HPC network: TLS optional but recommended if traversing untrusted segments\n- Restrict capabilities with `--deny-net`, `--deny-funcs` to minimize attack surface\n\n### 9. Network Access from Compute Nodes\n\nai-proxy LXC (100.105.113.58) is accessible from compute nodes on 10.0.0.0/24.\n- Bind to `0.0.0.0:8000` (or specific interface)\n- Compute nodes connect via: `http://100.105.113.58:8000` or the 10.x.x.x address\n- SurrealDB exposes HTTP/WebSocket APIs on the bind port\n- Rust SDK connects via: `Surreal::new::\u003cWs\u003e(\"100.105.113.58:8000\")`\n\n### 10. Graph + Vector Capabilities (UKG-Relevant)\n\n**Graph:**\n- `RELATE` statement creates typed edges between records (vertex → edge → vertex)\n- Graph traversal via `-\u003e` syntax: `SELECT * FROM crate:tokio-\u003edepends_on-\u003ecrate`\n- Edges are first-class records with metadata fields\n- No JOINs needed — record links provide direct traversal\n\n**Vector Search:**\n- MTREE index: `DEFINE INDEX idx ON table FIELDS embedding MTREE DIMENSION 768 DIST COSINE TYPE F32`\n- HNSW index: Faster ANN with tunable M and EFC parameters, 256 MiB default cache\n- kNN queries: `WHERE embedding \u003c|K|\u003e $query_vector`\n- Similarity functions: `vector::similarity::cosine()`, `vector::similarity::euclidean()`, etc.\n\n**Combined Graph+Vector (GraphRAG):**\n- Can traverse graph edges AND do vector similarity in the same query\n- Ideal for knowledge graph enrichment: find related code entities via graph, rank by vector similarity\n\n### 11. Deployment Steps\n\n```bash\n# 1. On ai-proxy LXC:\ncurl --proto '=https' --tlsv1.2 -sSf https://install.surrealdb.com | sh\n\n# 2. Create service user + data dir\nuseradd -r -s /bin/false surrealdb\nmkdir -p /var/lib/surrealdb/data\nchown surrealdb:surrealdb /var/lib/surrealdb/data\n\n# 3. Install systemd unit (see above)\n# 4. Enable and start\nsystemctl enable surrealdb\nsystemctl start surrealdb\n\n# 5. Initialize root credentials\nsurreal start --user root --pass '\u003cPASSWORD\u003e' --bind 0.0.0.0:8000 rocksdb:///var/lib/surrealdb/data\n# (first run only, then ctrl-c and let systemd manage)\n\n# 6. Verify from compute node\ncurl http://100.105.113.58:8000/health\n```\n\n### Sources\n- SurrealDB Architecture: https://surrealdb.com/docs/surrealdb/introduction/architecture\n- Storage \u0026 Deployment: https://surrealdb.com/learn/fundamentals/performance/deployment-storage\n- CLI Start Command: https://surrealdb.com/docs/surrealdb/cli/start\n- Environment Variables: https://surrealdb.com/docs/surrealdb/cli/env\n- Performance Best Practices: https://surrealdb.com/docs/surrealdb/reference-guide/performance-best-practices\n- Vector Search: https://surrealdb.com/docs/surrealdb/models/vector\n- SurrealDB Scalability: https://surrealdb.com/blog/surrealdb-scalability","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:17.677849-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:31:15.842302-06:00","closed_at":"2026-02-13T08:31:15.842302-06:00","close_reason":"Research complete. Recommendation: Embedded RocksDB mode on ai-proxy LXC. Full deployment decision document with systemd config, tuning params, and graph+vector capabilities written to issue notes.","labels":["delegate:pal-consensus","epic:ukg","infrastructure","research"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1.1","depends_on_id":"beefcake-swarm-3is.1","type":"parent-child","created_at":"2026-02-13T08:11:17.678895-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.1.2","title":"Deploy SurrealDB on ai-proxy LXC","description":"Install SurrealDB on ai-proxy (100.105.113.58). Systemd service, network binding for 10.0.0.0/24 + Tailscale. Namespaces: beefcake/knowledge_graph (prod), beefcake/benchmark. Verify connectivity from vasp nodes.","notes":"Human task — SSH required. Share creds via /cluster/shared/ai/surreal.env. Bind to 0.0.0.0:8000 with auth enabled.","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:17.907875-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:45:49.689598-06:00","closed_at":"2026-02-13T08:45:49.689598-06:00","close_reason":"SurrealDB v2.6.1 deployed on ai-proxy LXC (100.105.113.58). RocksDB backend, systemd enabled, namespaces created (beefcake/knowledge_graph, beefcake/benchmark), credentials at /cluster/shared/ai/surreal.env, health verified externally.","labels":["delegate:human","epic:ukg","infrastructure"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1.2","depends_on_id":"beefcake-swarm-3is.1","type":"parent-child","created_at":"2026-02-13T08:11:17.908763-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.1.2","depends_on_id":"beefcake-swarm-3is.1.1","type":"blocks","created_at":"2026-02-13T08:11:57.889269-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.1.3","title":"Design \u0026 deploy SurrealDB schema","description":"Create SurrealQL schema for code graph (function, struct, trait, module, file + calls/defines/imports/implements edges), doc graph (document, chunk + contains edges), and bridge edges (documents, mentions). Vector indexes for 3584-dim nomic embeddings (MTREE cosine). Deploy to prod namespace.","notes":"Use PAL chat with g3-pro (Librarian role). Output: schema.surql file. Consider: SCHEMAFULL tables, DEFINE INDEX for vector search, DEFINE FIELD with type constraints.","status":"in_progress","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:18.129206-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:48:08.245894-06:00","labels":["delegate:g3-pro","design","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1.3","depends_on_id":"beefcake-swarm-3is.1","type":"parent-child","created_at":"2026-02-13T08:11:18.130299-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.1.3","depends_on_id":"beefcake-swarm-3is.1.2","type":"blocks","created_at":"2026-02-13T08:11:58.387109-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.1.4","title":"Add Rust SurrealDB client to coordination crate","description":"Add surrealdb crate to coordination/Cargo.toml. New module coordination/src/knowledge_graph/client.rs with KgClient struct (connection pool, health check, CRUD). Follow coordination/src/state/store.rs Arc pattern for SharedKgClient. Config via SURREALDB_URL env var.","notes":"Pattern reference: coordination/src/state/store.rs for Arc\u003cRwLock\u003c\u003e\u003e shared state. The KgClient should support both embedded and remote connections for testing vs production.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:18.359372-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:18.359372-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1.4","depends_on_id":"beefcake-swarm-3is.1","type":"parent-child","created_at":"2026-02-13T08:11:18.360181-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.1.4","depends_on_id":"beefcake-swarm-3is.1.2","type":"blocks","created_at":"2026-02-13T08:11:58.738076-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.1.5","title":"Investigate SurrealDB vs direct RocksDB for Error KB and Archival Memory","description":"Investigate whether SurrealDB (which uses RocksDB as its default storage backend) should replace the direct RocksDB approach proposed in beefcake-swarm-3r9 (Error Pattern KB) and beefcake-swarm-b30 (Archival Memory). Compare: query flexibility (SurrealQL graph+vector vs manual KV), latency overhead (SurrealDB query layer vs direct RocksDB), operational simplicity (one DB vs two). Reference SurrealDB architecture.","notes":"PAL consensus. Key insight: SurrealDB uses RocksDB under the hood — so the question is whether the query layer overhead is worth the graph/vector capabilities. If yes, 3r9 and b30 migrate to SurrealDB. If no, keep direct RocksDB for latency-sensitive operations (per-issue state) and use SurrealDB for cross-issue knowledge only.\n\n## RESEARCH OUTPUT: SurrealDB vs Direct RocksDB for Error KB \u0026 Archival Memory\n\n### Executive Decision\n**RECOMMENDATION: Hybrid Architecture — SurrealDB for Knowledge Graph, keep direct RocksDB for hot-path state**\n\nThe key insight is that SurrealDB *uses* RocksDB under the hood. The question is whether the SurrealQL query layer overhead is justified by the graph+vector capabilities. The answer is **yes for cross-issue knowledge, no for per-issue hot-path state**.\n\n---\n\n### 1. Current State Analysis\n\n**Existing Direct RocksDB (coordination/src/state/store.rs):**\n- 6 column families: sessions, tasks, results, voting, context, events\n- Simple KV access pattern: `put(cf, key, bincode_value)` / `get(cf, key)` / `list_keys(cf, prefix)`\n- Bincode serialization for speed\n- ~400 LOC, clean abstraction\n- Sub-millisecond operations (direct RocksDB = no query parsing, no optimization, no transaction overhead)\n\n**Proposed Error KB (beefcake-swarm-3r9) needs:**\n- Store: `(error_signature, code_snippet, fix_diff, strategy, model_tier, iterations)`\n- Query: Find similar past errors by ErrorCategory + key tokens + file structure\n- Aggregate: Per-model success rates by error category\n- Feed top-3 matching strategies into WorkPacket.relevant_playbooks\n\n**Proposed Archival Memory (beefcake-swarm-b30) needs:**\n- Store: Prior diffs, verifier reports, repeated errors, decisions, design notes per bead_id\n- Query: Semantic recall of relevant prior approaches that failed\n- Promote: Successful fix patterns → higher recall priority\n- Summarize: Compress old memories to save context window tokens\n\n---\n\n### 2. Latency Analysis\n\n| Operation | Direct RocksDB | SurrealDB (Embedded) | SurrealDB (Networked) |\n|-----------|---------------|---------------------|----------------------|\n| Point read | ~1-10 μs | ~50-500 μs (parse+execute+KV) | ~1-5 ms (+ network) |\n| Point write | ~1-10 μs | ~100-1000 μs | ~2-10 ms |\n| Prefix scan | ~10-100 μs | ~200-2000 μs | ~5-20 ms |\n| Graph traversal | N/A (manual) | ~500 μs-5 ms | ~5-50 ms |\n| Vector similarity | N/A (manual) | ~1-50 ms (HNSW) | ~5-100 ms |\n| Complex join | N/A (manual) | ~1-10 ms | ~10-100 ms |\n\n**Key finding:** SurrealDB query layer adds ~10-100x overhead for simple KV operations due to SurrealQL parsing, query planning, and transaction management. For the hot-path (ensemble coordination during compilation loops), this matters. For knowledge retrieval (once per issue start or escalation), it doesn't.\n\n**Note:** One GitHub issue (#4767) reported embedded SurrealDB taking 17s vs 5s standalone for 69K record queries, suggesting the embedded Rust SDK may have performance anomalies. Worth benchmarking in our specific environment.\n\n---\n\n### 3. Feature Comparison for Error KB\n\n| Requirement | Direct RocksDB | SurrealDB |\n|-------------|---------------|-----------|\n| Store error patterns | ✅ Column family + bincode | ✅ `CREATE error_pattern SET ...` |\n| Exact match (ErrorCategory) | ✅ Prefix scan on compound key | ✅ `WHERE category = 'BorrowChecker'` |\n| Fuzzy match (key tokens) | ❌ Must implement manually | ✅ Full-text search + vector similarity |\n| Graph: \"errors → fixes → crates → files\" | ❌ Manual adjacency lists in KV | ✅ `RELATE error:e1-\u003efixed_by-\u003efix:f1` + traversal |\n| Aggregation (success rates) | ❌ Manual iteration + counting | ✅ `SELECT model_tier, math::mean(success) ... GROUP BY model_tier, category` |\n| Top-K similar patterns | ❌ Must build custom index | ✅ `WHERE embedding \u003c|3|\u003e $query_vec` |\n| Temporal queries | ⚠️ Manual timestamp keys | ✅ SurrealKV time-travel (experimental) |\n\n**Direct RocksDB wins on:** simplicity for simple KV, zero overhead, existing code reuse.\n**SurrealDB wins on:** every cross-issue query the Error KB needs.\n\n---\n\n### 4. Feature Comparison for Archival Memory\n\n| Requirement | Direct RocksDB | SurrealDB |\n|-------------|---------------|-----------|\n| Store per-bead memories | ✅ CF + `mem:{bead_id}:{type}:{ts}` | ✅ `CREATE memory SET bead_id = ...` |\n| Semantic recall | ❌ Must build vector index | ✅ MTREE/HNSW + cosine similarity |\n| Failed approach index | ⚠️ Manual compound keys | ✅ `SELECT * FROM memory WHERE bead_id = $id AND outcome = 'failed'` |\n| Memory promotion | ⚠️ Manual update + re-index | ✅ `UPDATE memory SET priority += 1 WHERE ...` |\n| Summarization storage | ✅ Simple KV | ✅ `UPDATE memory SET summary = ...` |\n| Cross-bead pattern matching | ❌ Full scan required | ✅ Graph traversal + vector search |\n| Letta-style tiered memory | ⚠️ Build from scratch | ✅ Natural fit: working=WorkPacket, archival=SurrealDB |\n\n---\n\n### 5. Operational Comparison\n\n| Aspect | Direct RocksDB (keep existing) | SurrealDB (add new) |\n|--------|-------------------------------|---------------------|\n| Dependencies | Already in Cargo.toml | +surrealdb crate (~slow compile due to C++ RocksDB dep) |\n| Data location | Local file (same process) | Network (ai-proxy LXC) OR embedded |\n| Backup | RocksDB checkpoint | SurrealDB export + RocksDB backup |\n| Monitoring | Manual | SurrealDB health endpoint |\n| Schema evolution | Manual migration code | SurrealQL `DEFINE TABLE/FIELD` |\n| Multi-agent access | ⚠️ Single process only (RwLock) | ✅ Network API, concurrent connections |\n| Failure isolation | Crash takes down coordination | Separate process, can restart independently |\n\n**Critical finding on multi-agent access:** The current RocksDB store uses `RwLock\u003cDB\u003e` — only one process can open it. When the swarm scales to multiple agents on different nodes, they can't share a direct RocksDB instance. SurrealDB's network API solves this inherently.\n\n---\n\n### 6. Architecture Recommendation: Hybrid\n\n```\n┌─────────────────────────────┐     ┌──────────────────────────┐\n│  coordination process       │     │  ai-proxy LXC            │\n│                             │     │                          │\n│  ┌─────────────────┐        │     │  ┌────────────────────┐  │\n│  │ Direct RocksDB  │        │     │  │ SurrealDB          │  │\n│  │ (hot-path state)│        │     │  │ (RocksDB backend)  │  │\n│  │                 │        │     │  │                    │  │\n│  │ • sessions      │        │     │  │ • error_pattern KB │  │\n│  │ • tasks         │◄───────┼─────┼──│ • archival_memory  │  │\n│  │ • results       │ query  │     │  │ • fix_strategies   │  │\n│  │ • voting        │ on     │     │  │ • model_stats      │  │\n│  │ • context       │ issue  │     │  │ • crate_graph      │  │\n│  │ • events        │ start  │     │  │ • code_embeddings  │  │\n│  └─────────────────┘        │     │  └────────────────────┘  │\n│                             │     │    :8000 HTTP/WS API     │\n└─────────────────────────────┘     └──────────────────────────┘\n```\n\n**Keep in direct RocksDB (hot path, per-issue):**\n- Ensemble sessions, tasks, results, voting, context, events\n- These are written/read hundreds of times per compilation cycle\n- Microsecond latency matters here\n- Single-process access is fine (one coordination MCP server)\n\n**Move to SurrealDB (knowledge path, cross-issue):**\n- Error Pattern KB (beefcake-swarm-3r9) → `error_pattern` table + graph edges\n- Archival Memory (beefcake-swarm-b30) → `memory` table + vector index\n- Model performance stats → `model_stats` table with aggregations\n- Code structure graph (future UKG) → full graph model\n\n**Why not embedded SurrealDB?**\n1. Multi-agent access: future swarm agents on different nodes need shared KB\n2. Failure isolation: KB crash shouldn't kill coordination loop\n3. ai-proxy LXC has the resources and is already the gateway node\n4. Operational clarity: one SurrealDB instance serves the whole cluster\n\n---\n\n### 7. SurrealDB Schema Design for Error KB\n\n```sql\n-- Error Pattern KB\nDEFINE TABLE error_pattern SCHEMAFULL;\nDEFINE FIELD category ON error_pattern TYPE string;  -- ErrorCategory enum\nDEFINE FIELD rustc_code ON error_pattern TYPE option\u003cstring\u003e;  -- E0505, E0106, etc.\nDEFINE FIELD key_tokens ON error_pattern TYPE array\u003cstring\u003e;  -- ['cannot borrow', 'mutable']\nDEFINE FIELD original_code ON error_pattern TYPE string;\nDEFINE FIELD fix_diff ON error_pattern TYPE string;\nDEFINE FIELD strategy ON error_pattern TYPE string;\nDEFINE FIELD model_tier ON error_pattern TYPE string;  -- strand/hydra/behemoth/council\nDEFINE FIELD iterations ON error_pattern TYPE int;\nDEFINE FIELD success ON error_pattern TYPE bool;\nDEFINE FIELD embedding ON error_pattern TYPE option\u003carray\u003cfloat\u003e\u003e;  -- for vector similarity\nDEFINE FIELD created_at ON error_pattern TYPE datetime DEFAULT time::now();\n\n-- Indexes\nDEFINE INDEX idx_category ON error_pattern FIELDS category;\nDEFINE INDEX idx_rustc_code ON error_pattern FIELDS rustc_code;\nDEFINE INDEX idx_embedding ON error_pattern FIELDS embedding MTREE DIMENSION 384 DIST COSINE TYPE F32;\n\n-- Archival Memory (Letta/MemGPT pattern)\nDEFINE TABLE memory SCHEMAFULL;\nDEFINE FIELD bead_id ON memory TYPE string;\nDEFINE FIELD memory_type ON memory TYPE string;  -- diff, verifier_report, error, decision, note\nDEFINE FIELD content ON memory TYPE string;\nDEFINE FIELD summary ON memory TYPE option\u003cstring\u003e;\nDEFINE FIELD outcome ON memory TYPE option\u003cstring\u003e;  -- success, failed, partial\nDEFINE FIELD priority ON memory TYPE int DEFAULT 0;\nDEFINE FIELD embedding ON memory TYPE option\u003carray\u003cfloat\u003e\u003e;\nDEFINE FIELD created_at ON memory TYPE datetime DEFAULT time::now();\n\nDEFINE INDEX idx_bead ON memory FIELDS bead_id;\nDEFINE INDEX idx_type ON memory FIELDS memory_type;\nDEFINE INDEX idx_mem_embedding ON memory FIELDS embedding MTREE DIMENSION 384 DIST COSINE TYPE F32;\n\n-- Graph edges: error → fixed_by → fix_strategy\nDEFINE TABLE fixed_by SCHEMAFULL;\nDEFINE FIELD strategy_used ON fixed_by TYPE string;\nDEFINE FIELD confidence ON fixed_by TYPE float;\n\n-- Model performance tracking\nDEFINE TABLE model_stats SCHEMAFULL;\nDEFINE FIELD model_id ON model_stats TYPE string;\nDEFINE FIELD category ON model_stats TYPE string;\nDEFINE FIELD attempts ON model_stats TYPE int DEFAULT 0;\nDEFINE FIELD successes ON model_stats TYPE int DEFAULT 0;\nDEFINE FIELD avg_iterations ON model_stats TYPE float DEFAULT 0.0;\nDEFINE FIELD updated_at ON model_stats TYPE datetime DEFAULT time::now();\n\nDEFINE INDEX idx_model_cat ON model_stats FIELDS model_id, category UNIQUE;\n```\n\n### 8. Example Queries\n\n```sql\n-- Find similar error patterns (top 3 by vector similarity)\nSELECT *, vector::similarity::cosine(embedding, $query_embedding) AS sim\nFROM error_pattern\nWHERE category = 'BorrowChecker'\n  AND embedding \u003c|3|\u003e $query_embedding;\n\n-- Model success rate by category (for routing decisions)\nSELECT model_id, category, \n       (successes * 1.0 / attempts) AS success_rate,\n       avg_iterations\nFROM model_stats\nWHERE attempts \u003e 5\nORDER BY success_rate DESC;\n\n-- Recall failed approaches for a bead (Letta pattern)\nSELECT content, summary, outcome\nFROM memory\nWHERE bead_id = $current_bead\n  AND outcome = 'failed'\nORDER BY created_at DESC\nLIMIT 5;\n\n-- Graph: what fixes worked for errors in similar crates?\nSELECT -\u003efixed_by-\u003eerror_pattern.strategy AS strategies\nFROM error_pattern\nWHERE category = $category\n  AND rustc_code = $code;\n```\n\n### 9. Migration Path\n\n1. **Phase 1 (now):** Deploy SurrealDB on ai-proxy (beefcake-swarm-3is.1.2)\n2. **Phase 2:** Implement Error KB in SurrealDB (beefcake-swarm-3is.5.2) — new code, no migration\n3. **Phase 3:** Implement Archival Memory in SurrealDB (beefcake-swarm-3is.5.4) — new code, no migration  \n4. **Phase 4:** Keep existing RocksDB for ensemble coordination (no change to store.rs)\n5. **Future:** If coordination needs multi-agent access, migrate remaining state to SurrealDB\n\n### 10. Risk Mitigation\n\n| Risk | Mitigation |\n|------|-----------|\n| SurrealDB latency on hot path | Don't use it for hot path — keep direct RocksDB |\n| SurrealDB instability (young project) | Isolated on ai-proxy, can fallback to direct RocksDB KB |\n| Network failure (ai-proxy down) | Degrade gracefully — skip KB lookup, use empty playbooks |\n| Embedding generation overhead | Use small model (384-dim) or hash-based signatures initially |\n| Schema evolution | SurrealQL supports `DEFINE FIELD` additions without migration |\n\n### 11. Conclusion\n\nThe hybrid approach gives us the best of both worlds:\n- **Direct RocksDB** stays for what it does well: fast, simple, per-issue state (μs latency)\n- **SurrealDB** handles what RocksDB can't: graph traversals, vector similarity, aggregations, multi-agent access, and the full UKG vision\n\nBoth beefcake-swarm-3r9 (Error KB) and beefcake-swarm-b30 (Archival Memory) should target SurrealDB, NOT add more column families to the existing RocksDB store. The existing store.rs remains untouched.\n\n### Sources\n- SurrealDB Architecture: https://surrealdb.com/docs/surrealdb/introduction/architecture\n- SurrealDB Graph Model: https://surrealdb.com/docs/surrealdb/models/graph\n- SurrealDB Vector Model: https://surrealdb.com/docs/surrealdb/models/vector\n- RELATE Statement: https://surrealdb.com/docs/surrealql/statements/relate\n- Performance Best Practices: https://surrealdb.com/docs/surrealdb/reference-guide/performance-best-practices\n- Embedded SurrealDB Perf Issue: https://github.com/surrealdb/surrealdb/issues/4767\n- SurrealDB Scalability: https://surrealdb.com/blog/surrealdb-scalability\n- crud-bench: https://github.com/surrealdb/crud-bench\n- MemGPT Paper: https://arxiv.org/abs/2310.08560\n- Letta/MemGPT Docs: https://docs.letta.com/concepts/memgpt/\n- Long-Term Memory Patterns: https://serokell.io/blog/design-patterns-for-long-term-memory-in-llm-powered-architectures","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:18.591062-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:34:41.17544-06:00","closed_at":"2026-02-13T08:34:41.17544-06:00","close_reason":"Research complete. Recommendation: Hybrid architecture — keep direct RocksDB for hot-path ensemble state, use networked SurrealDB on ai-proxy for Error KB and Archival Memory. Full comparison, schema design, and migration path documented in notes.","labels":["delegate:pal-consensus","epic:ukg","research"],"dependencies":[{"issue_id":"beefcake-swarm-3is.1.5","depends_on_id":"beefcake-swarm-3is.1","type":"parent-child","created_at":"2026-02-13T08:11:18.597752-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.2","title":"UKG: Code Graph Ingestion (codegraph-rust)","description":"Sub-epic 2: Research and deploy codegraph-rust for AST-level code graph ingestion into SurrealDB. Share tree-sitter parsing with context_packer.","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:19.002308-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.158694-06:00","labels":["epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.2","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:19.003111-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.2.1","title":"Research codegraph-rust capabilities","description":"Deep-dive jakedismo/codegraph-rust: node/edge types, SurrealDB storage support (look for surrealdb_storage.rs), incremental update support, tree-sitter dependency, performance on ~21K LOC Rust codebase. Determine if fork needed.","notes":"PAL chat with g3-pro (Librarian). Output: feature matrix + config example. Check GitHub repo for SurrealDB native support vs needing a custom adapter.\n## codegraph-rust Research Findings (2026-02-13)\n\n### Overview\n- **Repo**: github.com/Jakedismo/codegraph-rust (73 stars, 8 forks)\n- **Language**: 100% Rust, multi-crate workspace\n- **License**: Open source\n- **Key Tech**: tree-sitter + FastML parsing, RocksDB + FAISS (primary), SurrealDB (experimental graph backend)\n\n### Dual Storage Backends\n\n**Primary: RocksDB + FAISS**\n- Graph structure stored in RocksDB (embedded KV store)\n- Vector similarity via FAISS\n- High performance, no external DB dependency\n- Supports incremental updates (reprocess only changed files)\n- FAISS limitation: adding new vectors may require partial index rebuild\n\n**Experimental: SurrealDB Graph Backend**\n- File: `crates/codegraph-graph/src/surrealdb_storage.rs` (CONFIRMED EXISTS)\n- Uses official SurrealDB Rust SDK (`Surreal\u003cAny\u003e` type)\n- Connects via WebSocket (ws://localhost:3004 default)\n- Namespace: \"ouroboros\", Database: \"codegraph\"\n- Schema files: `schema/codegraph.surql` (relational), `schema/codegraph_graph_experimental.surql` (graph)\n- Supports auth (username/password)\n- Migrations: `crates/codegraph-graph/src/surrealdb_migrations.rs`\n\n### SurrealDB Tables\n| Table | Purpose |\n|-------|---------|\n| `nodes` | Code entities (functions, classes, methods, variables) |\n| `edges` | Relationships (calls, imports, inherits, contains, defines, uses, flows_to, returns, mutates, extends, implements, references, depends_on, violates_boundary — ~20 types) |\n| `chunks` | Text chunks linked to parent nodes |\n| `file_metadata` | Per-file stats (content_hash, language, node_count, edge_count) |\n| `symbol_embeddings` | Cached identifier-level embeddings |\n| `project_metadata` | Project-level metadata |\n\n### Embedding Support\n- Multi-dimensional: 384, 768, 1024, 1536, 2048, 3072, 4096\n- HNSW indexes on all embedding dimensions (DIST COSINE, EFC 100, M 16)\n- Providers: Ollama, LM Studio, OpenAI, Jina AI\n- Full-text search indexes on `content` and `name` fields\n\n### Node Types\nFunction, Class, Method, Variable (core AST types from tree-sitter)\n\n### Edge Types (~20 total)\ncalls, defines, imports, uses, extends, implements, references, contains, inherits, flows_to, returns, mutates, depends_on, violates_boundary, and more\n\n### Indexing Tiers\n| Tier | Features | Use Case |\n|------|----------|----------|\n| fast | AST nodes, core edges only. Disables build context, LSP, enrichment, module linking, dataflow | Quick indexing, CI |\n| balanced | + build context, LSP symbols, enrichment, module linking, docs/contracts | Development use |\n| full | All analyzers, LSP definitions, dataflow, architecture analysis, no edge filtering | Complete analysis |\n\n### MCP Tools (4 agentic tools)\n1. **agentic_context** — Semantic search, context building, question answering. Focus: search|builder|question\n2. **agentic_impact** — Dependency chains, call flows, change impact. Focus: dependencies|call_chain\n3. **agentic_architecture** — System structure, API surfaces, architectural patterns. Focus: structure|api_surface\n4. **agentic_quality** — Complexity hotspots, coupling metrics, refactoring priorities. Focus: complexity|coupling|hotspots\n\nAll tools run reasoning agents internally (plan → search → analyze graph → synthesize).\n\n### SurrealDB Graph Functions\n- `semantic_search_nodes_via_chunks` — Vector search over code chunks\n- `get_transitive_dependencies` — Transitive dependency resolution\n- `trace_call_chain` — Call chain tracing\n- `calculate_coupling_metrics` — Module coupling analysis\n\n### Performance Reference\n- Sample run: 1,505 files, 2.4M lines, 30K functions, 880 classes, 539K embeddings\n- HNSW query: 2-5ms\n- Supports 14 languages via tree-sitter: Rust, Python, TypeScript, JavaScript, Go, Java, C++, C, Swift, Kotlin, C#, Ruby, PHP, Dart\n\n### Agent Architecture\n- Uses **Rig** framework (same as our swarm-agents crate!)\n- Reasoning strategies: LATS (tree search), ReAct (linear), Reflexion (self-correcting)\n- Context-adaptive: adjusts depth based on LLM context window size\n\n### Assessment: Fork Needed?\n**Likely NO fork needed** — codegraph-rust already has:\n- ✅ Native SurrealDB storage backend (`surrealdb_storage.rs`)\n- ✅ SurrealDB graph experimental schema\n- ✅ Multi-dimensional embeddings compatible with our Nomic 3584-dim (closest: 4096)\n- ✅ Rust language with advanced analysis\n- ✅ Rig framework (same as our codebase)\n- ✅ MCP server for tool exposure\n- ✅ Incremental indexing\n\n**Potential customization points (config, not fork):**\n- Embedding dimension: may need to add 3584 HNSW index to schema (currently supports 384-4096)\n- SurrealDB connection config: point to our SurrealDB instance\n- Indexing tier: use \"full\" for ~21K LOC codebase (small enough)\n- Could run as MCP server alongside our coordination MCP\n\n**Risk factors:**\n- SurrealDB backend is labeled \"experimental\"\n- 73 stars = small community, may need to contribute fixes upstream\n- FAISS incremental updates have limitations (partial rebuild needed)\n- No published benchmarks for Rust-specific analysis quality","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:18.835498-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:33:00.65189-06:00","closed_at":"2026-02-13T08:33:00.65189-06:00","close_reason":"Research complete. codegraph-rust has native SurrealDB support via experimental backend. No fork needed — configure connection and add 3584-dim HNSW index.","labels":["delegate:g3-pro","epic:ukg","research"],"dependencies":[{"issue_id":"beefcake-swarm-3is.2.1","depends_on_id":"beefcake-swarm-3is.2","type":"parent-child","created_at":"2026-02-13T08:11:18.8364-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.2.2","title":"Deploy codegraph-rust indexer for beefcake-swarm","description":"Configure codegraph-rust to index coordination/ and crates/swarm-agents/ into SurrealDB. If no native SurrealDB support, write Rust adapter in indexing/. Must be idempotent. Validate: ~300+ functions, ~80+ structs/enums, ~20+ traits.","notes":"Idempotency via deterministic IDs (hash of file path + symbol name). Run as CLI or integrate into build pipeline.","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:19.062001-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:19.062001-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.2.2","depends_on_id":"beefcake-swarm-3is.2","type":"parent-child","created_at":"2026-02-13T08:11:19.062906-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.2.2","depends_on_id":"beefcake-swarm-3is.1.3","type":"blocks","created_at":"2026-02-13T08:11:58.98291-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.2.2","depends_on_id":"beefcake-swarm-3is.2.1","type":"blocks","created_at":"2026-02-13T08:11:59.09046-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.2.3","title":"Share tree-sitter parser between codegraph and context_packer","description":"Ensure one tree-sitter parse per file shared between codegraph ingestion and context packing (avoid duplicate parsing). Create shared AstIndex module in coordination. Coordinate with beefcake-swarm-5bk.","notes":"Synergy with beefcake-swarm-5bk (tree-sitter-rust for AST-aware context packing). Both need parsed ASTs — share the cache.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:19.294317-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:19.294317-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.2.3","depends_on_id":"beefcake-swarm-3is.2","type":"parent-child","created_at":"2026-02-13T08:11:19.295423-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.2.3","depends_on_id":"beefcake-swarm-3is.2.2","type":"blocks","created_at":"2026-02-13T08:11:59.31202-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.3","title":"UKG: Document/Semantic Ingestion (CocoIndex → SurrealDB)","description":"Sub-epic 3: Extend CocoIndex pipeline with SurrealDB target for dual-write (pgvector + SurrealDB). Build bridge edges linking doc chunks to code symbols.","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:19.225419-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.160477-06:00","labels":["epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.3","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:19.226271-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.3.1","title":"Research CocoIndex custom Target for SurrealDB","description":"Research CocoIndex Target interface for custom SurrealDB target. Can we run dual-write (Postgres + SurrealDB) in a single flow? Validate against actual CocoIndex v1 API (not the outdated sketch in COCOINDEX_GRAPH_RAG.md).","notes":"PAL consensus. The existing indexing/index_flow_v2.py uses @flow_def pattern. Output: validated implementation approach. Check CocoIndex docs for custom target/sink interface.\n## CocoIndex Custom SurrealDB Target — Research Findings (2026-02-13)\n\n### Current Pipeline Analysis\n- File: `indexing/index_flow_v2.py`\n- Pattern: `@flow_def(name=\"code_indexing\")` with `FlowBuilder` + `DataScope`\n- Source: `LocalFile` from rust-daq repo\n- Transform: `DetectProgrammingLanguage` → `SplitRecursively` (1000 chars) → `EmbedText` (Nomic 3584-dim via vasp-02)\n- Target: `Postgres` with pgvector HNSW index (cosine similarity)\n- Live updates: 30-second refresh interval\n- Query handler: `semantic_search` over `code_chunks` table\n\n### COCOINDEX_GRAPH_RAG.md Assessment\n**OUTDATED** — The document references `cocoindex.typing.Target` and `batch_write()` method, which do NOT match the current CocoIndex v1 API. The actual API uses:\n- `cocoindex.op.TargetSpec` (not `cocoindex.typing.Target`)\n- `@cocoindex.op.target_connector(spec_cls=...)` decorator\n- `mutate()` method (not `batch_write()`)\n- `apply_setup_change()` for infrastructure lifecycle\n- `get_persistent_key()` for target identity\n\n### CocoIndex v1 Custom Target API (Validated)\n\n#### 1. TargetSpec (Configuration)\n```python\nclass SurrealDBTarget(cocoindex.op.TargetSpec):\n    url: str           # ws://localhost:8000\n    namespace: str     # \"ouroboros\"\n    database: str      # \"codegraph\"\n    table_name: str    # \"doc_chunks\"\n    auth_ref: cocoindex.AuthEntryReference | None = None\n```\n\n#### 2. TargetConnector (Implementation)\n```python\n@cocoindex.op.target_connector(spec_cls=SurrealDBTarget)\nclass SurrealDBTargetConnector:\n    @staticmethod\n    def get_persistent_key(spec, target_name) -\u003e str:\n        return f\"{spec.url}/{spec.namespace}/{spec.database}/{spec.table_name}\"\n\n    @staticmethod\n    def apply_setup_change(key, previous, current):\n        # Create/delete SurrealDB table + HNSW indexes\n        if previous is None and current is not None:\n            # CREATE TABLE, DEFINE INDEX for embeddings\n            pass\n        if previous is not None and current is None:\n            # DROP TABLE\n            pass\n\n    @staticmethod\n    def mutate(*all_mutations):\n        for spec, mutations in all_mutations:\n            for key, value in mutations.items():\n                if value is None:\n                    # DELETE chunk:key\n                    pass\n                else:\n                    # UPSERT chunk:key CONTENT {...}\n                    pass\n```\n\n#### 3. Key Methods\n| Method | Purpose |\n|--------|---------|\n| `get_persistent_key()` | Returns unique identifier for target instance |\n| `apply_setup_change()` | Creates/destroys target infrastructure (tables, indexes) |\n| `mutate()` | Applies data changes (insert/update/delete) — REQUIRED |\n| `prepare()` | Optional: pre-connect, validate, cache connections |\n| `describe()` | Optional: logging/debugging |\n\n### Dual-Write Architecture: CONFIRMED POSSIBLE\n\nCocoIndex supports multiple collectors and multiple export() calls in a single flow. Two approaches:\n\n#### Option A: Two Collectors (Recommended)\n```python\n@flow_def(name=\"code_indexing\")\ndef my_flow(builder, scope):\n    files = builder.add_source(LocalFile(...))\n    row = files.row()\n    row[\"language\"] = row[\"filename\"].transform(DetectProgrammingLanguage())\n    row[\"chunks\"] = row[\"content\"].transform(SplitRecursively(), ...)\n    chunks = row[\"chunks\"].row()\n    chunks[\"embedding\"] = text_to_embedding(chunks[\"text\"])\n\n    # Collector 1: Postgres (existing)\n    pg_collector = scope.add_collector()\n    pg_collector.collect(filename=row[\"filename\"], ...)\n    pg_collector.export(\"postgres_export\", Postgres(...), ...)\n\n    # Collector 2: SurrealDB (new)\n    surreal_collector = scope.add_collector()\n    surreal_collector.collect(filename=row[\"filename\"], ...)\n    surreal_collector.export(\"surreal_export\", SurrealDBTarget(...), ...)\n```\n\n#### Option B: Single Collector, Two Exports\nMay also work — two export() calls from same collector. Less documented but architecturally plausible.\n\n### SurrealDB Python SDK\n- Package: `surrealdb` on PyPI (v1.0.8, Jan 2026)\n- Protocols: HTTP, WebSocket, embedded\n- Async + sync support\n- CBOR serialization (binary, efficient)\n- Compatible: SurrealDB v2.0.0 — v2.3.6\n- Mature: 227 tests, 100% coverage, Python 3.9-3.13\n\n### Built-in Target Precedent\nCocoIndex already has a **Neo4j** target (graph database), proving graph DB targets are first-class citizens. The SurrealDB target follows the same pattern:\n- Nodes → `collector.export(Neo4j(mapping=Nodes(label=\"...\")))`\n- Could do similar: `collector.export(SurrealDBTarget(table_name=\"doc_chunks\"))`\n\n### Implementation Approach (Recommended)\n\n1. **Create `indexing/targets/surreal_target.py`** — Custom TargetSpec + Connector\n2. **SurrealDB connection**: Use `surrealdb` Python SDK with WebSocket\n3. **Schema**: `apply_setup_change()` creates table with HNSW vector indexes matching codegraph-rust schema (embedding_3584 field, COSINE distance)\n4. **Mutations**: `mutate()` does UPSERT/DELETE via SurrealQL parameterized queries\n5. **Dual-write**: Add second collector to existing flow in `index_flow_v2.py`\n6. **Bridge edges**: Post-process to create `documents` edges between doc chunks and code nodes (from codegraph-rust)\n\n### Schema Alignment with codegraph-rust\nThe SurrealDB target should write to tables compatible with codegraph-rust's experimental schema:\n- Table: `chunks` (matching codegraph-rust's chunk table)\n- Fields: `project_id`, `parent_node`, `text`, `embedding_3584` (new dimension)\n- Or separate table: `doc_chunks` to avoid collision with code chunks\n\n### Risk Assessment\n| Risk | Mitigation |\n|------|------------|\n| SurrealDB Python SDK maturity | Stable v1.0.8, well-tested |\n| Custom Target API stability | CocoIndex v1 API, blog + docs consistent |\n| Dual-write performance | Async SurrealDB writes, CocoIndex handles batching |\n| Schema collision with codegraph-rust | Use separate table (`doc_chunks`) or namespace |\n| 3584-dim not in codegraph schema | Add custom HNSW index definition |\n\n### Conclusion\n**Dual-write is fully supported and straightforward.** The custom target requires ~100 lines of Python. The existing pipeline needs minimal modification (add second collector + export). No fork of CocoIndex needed — the custom target API is designed for exactly this use case.","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:19.526154-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:35:55.249095-06:00","closed_at":"2026-02-13T08:35:55.249095-06:00","close_reason":"Research complete. CocoIndex custom Target API validated — dual-write (Postgres + SurrealDB) confirmed possible via multiple collectors. Custom SurrealDB target needs ~100 LOC Python. COCOINDEX_GRAPH_RAG.md is outdated and needs updating.","labels":["delegate:pal-consensus","epic:ukg","research"],"dependencies":[{"issue_id":"beefcake-swarm-3is.3.1","depends_on_id":"beefcake-swarm-3is.3","type":"parent-child","created_at":"2026-02-13T08:11:19.527018-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.3.2","title":"Implement dual-write CocoIndex flow (pgvector + SurrealDB)","description":"Implement SurrealDBTarget for CocoIndex. Create indexing/targets/surreal_target.py. Modify or create index_flow_v3.py for dual-write. Python (keep working pipeline). Deterministic IDs for idempotency. Config via env vars.","notes":"Keep existing pgvector pipeline working. Dual-write means both stores get identical data for benchmarking. SURREALDB_URL, SURREALDB_NS, SURREALDB_DB env vars.","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:19.752501-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:19.752501-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.3.2","depends_on_id":"beefcake-swarm-3is.3","type":"parent-child","created_at":"2026-02-13T08:11:19.753306-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.3.2","depends_on_id":"beefcake-swarm-3is.1.3","type":"blocks","created_at":"2026-02-13T08:11:59.533936-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.3.2","depends_on_id":"beefcake-swarm-3is.3.1","type":"blocks","created_at":"2026-02-13T08:11:59.639447-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.3.3","title":"Build bridge edge processor (doc↔code linking)","description":"Post-processing step creating bridge edges between doc chunks and code symbols. Strategy 1: regex-based symbol extraction from doc text. Strategy 2 (future): embedding similarity. Write in Rust (indexing/bridge-linker/). SurrealQL: RELATE chunk-\u003edocuments-\u003efunction.","notes":"Start with regex strategy (cheaper, deterministic). Match function/struct/trait names found in doc text to code graph nodes. Run after both code and doc ingestion complete.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:19.979691-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:19.979691-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.3.3","depends_on_id":"beefcake-swarm-3is.3","type":"parent-child","created_at":"2026-02-13T08:11:19.980519-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.3.3","depends_on_id":"beefcake-swarm-3is.2.2","type":"blocks","created_at":"2026-02-13T08:11:59.874972-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.3.3","depends_on_id":"beefcake-swarm-3is.3.2","type":"blocks","created_at":"2026-02-13T08:11:59.976578-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.4","title":"UKG: WorkPacket \u0026 Coordination Integration","description":"Sub-epic 4: Wire knowledge graph into WorkPacket enrichment, Router task classification, and Escalation signals. THE critical integration layer.","status":"in_progress","priority":0,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:19.441072-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.162554-06:00","labels":["epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.4","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:19.441894-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.4.1","title":"Design KG query interface for WorkPacket enrichment","description":"Design how KG queries populate WorkPacket.relevant_heuristics and relevant_playbooks. Define: SurrealQL queries per error category, token budget allocation (2K implementer, 4K integrator, 8K cloud), integration point in pipeline (pack_retry vs generate). Output: Rust trait + SurrealQL query library.","notes":"PAL consensus — this is THE critical architectural decision. All 3 frontier models weigh in. Files: coordination/src/context_packer/packer.rs, coordination/src/work_packet/types.rs\n# KG Query Interface Design for WorkPacket Enrichment\n\n**Author**: Opus 4.6 (direct analysis — PAL MCP consensus unavailable in teammate env)\n**Date**: 2026-02-13\n**Status**: DESIGN COMPLETE\n\n---\n\n## 1. Executive Summary\n\nThe WorkPacket struct has two empty fields — `relevant_heuristics` and `relevant_playbooks` — that should be populated by querying a SurrealDB knowledge graph containing code structure, documentation embeddings, bridge edges, and fix patterns from past successful error resolutions. This design defines the Rust trait interface, SurrealQL query templates, token budget allocation, integration point, and degradation strategy.\n\n---\n\n## 2. Trait Definition: `KgEnricher`\n\n```rust\n// coordination/src/kg/enricher.rs\n\nuse crate::escalation::state::SwarmTier;\nuse crate::feedback::error_parser::ErrorCategory;\nuse crate::work_packet::types::KeySymbol;\n\n/// Result of knowledge graph enrichment for a WorkPacket.\n/// All fields are populated on best-effort basis; empty = no enrichment available.\n#[derive(Debug, Clone, Default)]\npub struct KgEnrichment {\n    /// General heuristics: short actionable rules (1-2 sentences each).\n    /// Example: \"E0382 (use after move) — consider Clone, Rc\u003cT\u003e, or ownership restructuring\"\n    pub heuristics: Vec\u003cString\u003e,\n    /// Playbooks: step-by-step procedures from past successful fixes.\n    /// Example: \"BorrowChecker cascade in parser.rs: (1) Made field Clone (2) Changed \u0026mut → \u0026self...\"\n    pub playbooks: Vec\u003cString\u003e,\n    /// Additional symbols discovered via graph traversal (callers/callees/trait impls).\n    pub related_symbols: Vec\u003cKeySymbol\u003e,\n    /// Wall-clock time spent querying KG, in milliseconds.\n    pub query_time_ms: u64,\n    /// Whether KG was reachable. False = degraded mode, all vecs empty.\n    pub kg_available: bool,\n}\n\nimpl KgEnrichment {\n    /// Estimate token count of heuristics + playbooks combined.\n    /// Uses 4 chars ≈ 1 token heuristic, matching WorkPacket::estimated_tokens().\n    pub fn estimated_tokens(\u0026self) -\u003e usize {\n        let chars: usize = self.heuristics.iter().map(|h| h.len()).sum::\u003cusize\u003e()\n            + self.playbooks.iter().map(|p| p.len()).sum::\u003cusize\u003e();\n        chars / 4\n    }\n\n    /// Trim to fit within a token budget by dropping lowest-priority items.\n    /// Heuristics are trimmed from the end first (least relevant),\n    /// then playbooks from the end.\n    pub fn trim_to_budget(\u0026mut self, max_tokens: usize) {\n        while self.estimated_tokens() \u003e max_tokens \u0026\u0026 !self.heuristics.is_empty() {\n            self.heuristics.pop();\n        }\n        while self.estimated_tokens() \u003e max_tokens \u0026\u0026 !self.playbooks.is_empty() {\n            self.playbooks.pop();\n        }\n    }\n}\n\n/// Trait for querying the knowledge graph to enrich WorkPackets.\n///\n/// Implementations handle SurrealDB connectivity, query construction,\n/// timeout enforcement, and graceful degradation internally.\n/// The caller never sees an error — degraded mode returns empty enrichment.\n#[async_trait::async_trait]\npub trait KgEnricher: Send + Sync {\n    /// Query the KG for heuristics and playbooks relevant to the current error context.\n    ///\n    /// # Arguments\n    /// - `error_categories`: Active error types from VerifierReport\n    /// - `error_codes`: Specific rustc error codes (E0382, E0277, etc.)\n    /// - `affected_files`: Files with errors or modifications\n    /// - `key_symbols`: Symbols extracted from affected files\n    /// - `tier`: Target model tier (determines token budget and timeout)\n    /// - `iteration`: Current iteration number (higher = more aggressive retrieval)\n    ///\n    /// # Returns\n    /// KgEnrichment with heuristics and playbooks, already trimmed to tier budget.\n    /// On KG unavailability, returns KgEnrichment::default() with kg_available=false.\n    async fn enrich(\n        \u0026self,\n        error_categories: \u0026[ErrorCategory],\n        error_codes: \u0026[String],\n        affected_files: \u0026[String],\n        key_symbols: \u0026[KeySymbol],\n        tier: SwarmTier,\n        iteration: u32,\n    ) -\u003e KgEnrichment;\n}\n```\n\n### NoopKgEnricher (for testing / Phase 3 migration)\n\n```rust\n/// No-op implementation that returns empty enrichment.\n/// Use during testing or before SurrealDB is deployed.\npub struct NoopKgEnricher;\n\n#[async_trait::async_trait]\nimpl KgEnricher for NoopKgEnricher {\n    async fn enrich(\n        \u0026self, _: \u0026[ErrorCategory], _: \u0026[String], _: \u0026[String],\n        _: \u0026[KeySymbol], _: SwarmTier, _: u32,\n    ) -\u003e KgEnrichment {\n        KgEnrichment {\n            kg_available: false,\n            ..Default::default()\n        }\n    }\n}\n```\n\n---\n\n## 3. SurrealQL Query Templates\n\n### 3.1 Fix Pattern Retrieval (highest value)\n\n```surql\n-- Query A: Find past successful fixes for matching error categories\n-- Input: $error_categories (array of strings), $symbols (array of strings), $file_stem (string)\n-- Ordered by: match quality (category + symbol overlap) × success_count\n\nSELECT\n    id,\n    error_category,\n    error_code,\n    affected_symbol,\n    file_path,\n    fix_description,\n    fix_diff_summary,\n    success_count,\n    last_applied\nFROM fix_pattern\nWHERE error_category IN $error_categories\nORDER BY\n    (IF affected_symbol IN $symbols THEN 10 ELSE 0 END)\n    + (IF string::contains(file_path, $file_stem) THEN 5 ELSE 0 END)\n    + success_count\n    DESC\nLIMIT $max_patterns;\n```\n\n**`$max_patterns`** per tier: Implementer=3, Integrator=5, Cloud=8\n\n### 3.2 Symbol Neighborhood (graph traversal)\n\n```surql\n-- Query B: Find call graph neighborhood of affected symbols\n-- Returns callers, callees, trait implementations, and containing module\n\nSELECT\n    name,\n    kind,\n    file_path,\n    line_number,\n    \u003c-calls\u003c-symbol.name AS callers,\n    -\u003ecalls-\u003esymbol.name AS callees,\n    -\u003eimplements-\u003esymbol.name AS trait_impls,\n    \u003c-defines\u003c-symbol.name AS parent_mod\nFROM symbol\nWHERE name IN $symbols\nLIMIT 10;\n```\n\n### 3.3 Documentation Semantic Search\n\n```surql\n-- Query C: Vector similarity search on doc chunks\n-- $query_embedding is computed from: error_message + affected_symbol names\n-- Uses 3584-dim embeddings (Nomic Embed V2 or similar)\n\nSELECT\n    id,\n    title,\n    content,\n    source_file,\n    vector::similarity::cosine(embedding, $query_embedding) AS relevance_score\nFROM doc_chunk\nWHERE vector::similarity::cosine(embedding, $query_embedding) \u003e 0.65\nORDER BY relevance_score DESC\nLIMIT $max_docs;\n```\n\n**`$max_docs`** per tier: Implementer=1, Integrator=2, Cloud=4\n\n### 3.4 Bridge Edge Lookup (docs ↔ code)\n\n```surql\n-- Query D: Find documentation linked to affected symbols\n-- Bridge edges connect code symbols to their doc chunks\n\nSELECT\n    \u003c-documents\u003c-doc_chunk.{id, title, content} AS docs,\n    name,\n    kind\nFROM symbol\nWHERE name IN $symbols\n    AND count(-\u003edocumented_by-\u003edoc_chunk) \u003e 0;\n```\n\n### 3.5 Error Code Pattern Lookup\n\n```surql\n-- Query E: Canonical fix patterns keyed by rustc error code\n-- These are curated heuristics, not learned from past fixes\n\nSELECT\n    error_code,\n    category,\n    heuristic_text,\n    common_fixes,\n    complexity_level\nFROM error_heuristic\nWHERE error_code IN $error_codes\nORDER BY frequency DESC;\n```\n\n---\n\n## 4. Heuristics vs Playbooks: Content Semantics\n\n### `relevant_heuristics: Vec\u003cString\u003e`\n\n**Definition**: Short, general, reusable rules. Each heuristic is 1-2 sentences describing a pattern or principle the model should consider.\n\n**Sources** (priority order):\n1. `error_heuristic` table (Query E) — curated per error code\n2. Semantic doc search (Query C) — distilled to actionable tips\n3. Bridge docs (Query D) — API usage notes linked to symbols\n\n**Format per entry**:\n```\n\"[ErrorCode] Category — Actionable guidance. Example: ...\"\n```\n\n**Examples**:\n```\n\"[E0382] BorrowChecker — Value used after move. Consider: (a) add .clone() if type is Clone, (b) use \u0026ref instead of move, (c) restructure to avoid reuse after move.\"\n\"[E0277] TraitBound — Missing trait impl. Check if the bound is on a generic parameter you control (add where clause) vs a concrete type (impl the trait or use a wrapper).\"\n\"[E0106] Lifetime — Missing lifetime annotation. The compiler needs explicit lifetimes when a function returns a reference. Add lifetime parameter to fn signature and annotate the return type.\"\n```\n\n### `relevant_playbooks: Vec\u003cString\u003e`\n\n**Definition**: Multi-step procedures derived from past successful fixes in this codebase. Each playbook describes a specific fix sequence that resolved a similar error on similar code.\n\n**Sources** (priority order):\n1. `fix_pattern` table (Query A) — past successful fixes with highest success_count\n2. Symbol neighborhood context (Query B) — to qualify which playbook applies\n\n**Format per entry**:\n```\n\"[Pattern: \u003cname\u003e] Applied \u003cN\u003e times, last: \u003cdate\u003e. Steps: (1)... (2)... (3)... Files: \u003cfiles\u003e\"\n```\n\n**Examples**:\n```\n\"[Pattern: borrow-cascade-parser] Applied 4 times, last: 2026-02-10. Steps: (1) Clone the borrowed field before entering the loop. (2) Use a local variable to hold the clone. (3) Replace all \u0026mut self methods that only read with \u0026self. Files: src/parser.rs, src/lexer.rs\"\n\"[Pattern: async-send-bound] Applied 2 times, last: 2026-02-08. Steps: (1) Box the future with Pin\u003cBox\u003cdyn Future + Send\u003e\u003e. (2) Add Send bound to the async trait method. (3) Ensure all captured variables are Send. Files: src/handler.rs\"\n```\n\n---\n\n## 5. Token Budget Allocation\n\nToken budgets for KG enrichment are a SUBSET of the total WorkPacket context budget.\n\n| Tier | Total Context Budget | KG Enrichment Budget | Heuristic Allocation | Playbook Allocation | Timeout |\n|------|---------------------|---------------------|---------------------|--------------------|---------| \n| Implementer | 8,000 tok | 2,000 tok | 500 tok (~2-3 items) | 1,500 tok (~2-3 items) | 500ms |\n| Integrator | 24,000 tok | 4,000 tok | 1,500 tok (~5-6 items) | 2,500 tok (~4-5 items) | 1,000ms |\n| Adversary | 24,000 tok | 4,000 tok | 2,000 tok (~6-8 items) | 2,000 tok (~3-4 items) | 1,000ms |\n| Cloud | 32,000 tok | 8,000 tok | 3,000 tok (~10-12 items) | 5,000 tok (~6-8 items) | 2,000ms |\n\n**Rationale for allocation split**:\n- **Implementer** (14B): Gets proportionally MORE playbooks (concrete steps) because smaller models benefit more from explicit step-by-step instructions than general principles.\n- **Integrator** (72B): Balanced split — can use both principles and examples.\n- **Adversary** (80B MoE): More heuristics — adversarial review benefits from knowing the general patterns to check against.\n- **Cloud**: More playbooks — frontier models use concrete codebase examples to ground their architecture decisions in local context.\n\n**Iteration scaling**: On iteration \u003e 1, increase budget by 25% (capped at tier max) to provide more context for stuck problems:\n\n```rust\nfn kg_token_budget(tier: SwarmTier, iteration: u32) -\u003e usize {\n    let base = match tier {\n        SwarmTier::Implementer =\u003e 2_000,\n        SwarmTier::Integrator | SwarmTier::Adversary =\u003e 4_000,\n        SwarmTier::Cloud =\u003e 8_000,\n    };\n    if iteration \u003e 1 {\n        (base as f64 * 1.25).min(base as f64 * 1.5) as usize\n    } else {\n        base\n    }\n}\n```\n\n---\n\n## 6. Integration Point: `ContextPacker::pack_retry()`\n\n### Decision: Enrich in `ContextPacker`, NOT in `WorkPacketGenerator`\n\n**Rationale**:\n- `WorkPacketGenerator::generate()` is synchronous (no async) — adding SurrealDB queries would require making it async, which cascades through the call chain.\n- `ContextPacker` is the orchestration layer that already knows the tier and manages token budgets.\n- Enrichment should happen AFTER `generate()` returns (so we have the packet's error categories and symbols) but BEFORE `trim_to_budget()` (so enrichment content participates in budget trimming).\n\n### Modified `ContextPacker`\n\n```rust\npub struct ContextPacker {\n    working_dir: PathBuf,\n    generator: WorkPacketGenerator,\n    file_walker: FileWalker,\n    tier: SwarmTier,\n    max_context_tokens: usize,\n    kg_enricher: Arc\u003cdyn KgEnricher\u003e,  // NEW: injected dependency\n}\n\nimpl ContextPacker {\n    pub fn new(working_dir: impl AsRef\u003cPath\u003e, tier: SwarmTier) -\u003e Self {\n        Self::with_enricher(working_dir, tier, Arc::new(NoopKgEnricher))\n    }\n\n    pub fn with_enricher(\n        working_dir: impl AsRef\u003cPath\u003e,\n        tier: SwarmTier,\n        kg_enricher: Arc\u003cdyn KgEnricher\u003e,\n    ) -\u003e Self {\n        let wd = working_dir.as_ref().to_path_buf();\n        Self {\n            generator: WorkPacketGenerator::new(\u0026wd),\n            file_walker: FileWalker::new(\u0026wd),\n            tier,\n            max_context_tokens: max_context_tokens(tier),\n            working_dir: wd,\n            kg_enricher,\n        }\n    }\n\n    /// Retry pack with KG enrichment.\n    pub async fn pack_retry(\n        \u0026self,\n        bead_id: \u0026str,\n        objective: \u0026str,\n        escalation_state: \u0026EscalationState,\n        verifier_report: \u0026VerifierReport,\n    ) -\u003e WorkPacket {\n        let mut packet = self.generator.generate(\n            bead_id, objective,\n            escalation_state.current_tier,\n            escalation_state,\n            Some(verifier_report),\n        );\n\n        // --- KG ENRICHMENT (new) ---\n        let error_codes: Vec\u003cString\u003e = verifier_report.failure_signals\n            .iter()\n            .filter_map(|s| s.code.clone())\n            .collect();\n\n        let enrichment = self.kg_enricher.enrich(\n            \u0026packet.unique_error_categories(),\n            \u0026error_codes,\n            \u0026packet.files_touched,\n            \u0026packet.key_symbols,\n            self.tier,\n            packet.iteration,\n        ).await;\n\n        packet.relevant_heuristics = enrichment.heuristics;\n        packet.relevant_playbooks = enrichment.playbooks;\n\n        // Add discovered related symbols to key_symbols (deduplicated)\n        for sym in enrichment.related_symbols {\n            if !packet.key_symbols.iter().any(|s| s.name == sym.name \u0026\u0026 s.file == sym.file) {\n                packet.key_symbols.push(sym);\n            }\n        }\n        // --- END KG ENRICHMENT ---\n\n        self.trim_to_budget(\u0026mut packet);\n        packet\n    }\n}\n```\n\n### Key change: `pack_retry()` becomes `async`\n\nThis is the minimum-blast-radius change. `pack_initial()` can remain sync (no errors to enrich from). Callers of `pack_retry()` in the swarm orchestrator loop are already async (tokio runtime).\n\n---\n\n## 7. SurrealDB Enricher Implementation Sketch\n\n```rust\n// coordination/src/kg/surreal_enricher.rs\n\nuse surrealdb::Surreal;\nuse surrealdb::engine::remote::ws::Client;\n\npub struct SurrealKgEnricher {\n    db: Surreal\u003cClient\u003e,\n    /// Timeout per tier (set in constructor from tier budgets)\n    default_timeout: Duration,\n    /// LRU cache: (error_categories_hash, file_hash) → KgEnrichment\n    cache: Arc\u003cMutex\u003cLruCache\u003cu64, KgEnrichment\u003e\u003e\u003e,\n}\n\nimpl SurrealKgEnricher {\n    pub async fn connect(url: \u0026str, ns: \u0026str, db_name: \u0026str) -\u003e Result\u003cSelf, SurrealError\u003e {\n        let db = Surreal::new::\u003csurrealdb::engine::remote::ws::Ws\u003e(url).await?;\n        db.use_ns(ns).use_db(db_name).await?;\n        Ok(Self {\n            db,\n            default_timeout: Duration::from_millis(1000),\n            cache: Arc::new(Mutex::new(LruCache::new(NonZeroUsize::new(256).unwrap()))),\n        })\n    }\n}\n\n#[async_trait::async_trait]\nimpl KgEnricher for SurrealKgEnricher {\n    async fn enrich(\n        \u0026self,\n        error_categories: \u0026[ErrorCategory],\n        error_codes: \u0026[String],\n        affected_files: \u0026[String],\n        key_symbols: \u0026[KeySymbol],\n        tier: SwarmTier,\n        iteration: u32,\n    ) -\u003e KgEnrichment {\n        let timeout = match tier {\n            SwarmTier::Implementer =\u003e Duration::from_millis(500),\n            SwarmTier::Integrator | SwarmTier::Adversary =\u003e Duration::from_millis(1000),\n            SwarmTier::Cloud =\u003e Duration::from_millis(2000),\n        };\n\n        let budget = kg_token_budget(tier, iteration);\n\n        // Check cache first\n        let cache_key = hash_query_inputs(error_categories, affected_files, key_symbols);\n        if let Some(cached) = self.cache.lock().unwrap().get(\u0026cache_key) {\n            return cached.clone();\n        }\n\n        // Run queries with timeout\n        let result = tokio::time::timeout(timeout, async {\n            let start = std::time::Instant::now();\n\n            // Run fix pattern + error heuristic queries in parallel\n            let (fix_patterns, error_heuristics) = tokio::join!(\n                self.query_fix_patterns(error_categories, key_symbols, affected_files, tier),\n                self.query_error_heuristics(error_codes),\n            );\n\n            // Build enrichment\n            let mut enrichment = KgEnrichment {\n                heuristics: error_heuristics.unwrap_or_default(),\n                playbooks: fix_patterns.unwrap_or_default(),\n                related_symbols: vec![], // populated by symbol neighborhood if budget allows\n                query_time_ms: start.elapsed().as_millis() as u64,\n                kg_available: true,\n            };\n\n            // If budget allows, also query symbol neighborhood\n            let heuristic_budget = budget * 2 / 5; // 40% heuristics\n            let playbook_budget = budget * 3 / 5;  // 60% playbooks\n            // (Implementer ratio inverted: 25% heuristics, 75% playbooks)\n            let (h_budget, p_budget) = match tier {\n                SwarmTier::Implementer =\u003e (budget / 4, budget * 3 / 4),\n                SwarmTier::Adversary =\u003e (budget / 2, budget / 2),\n                _ =\u003e (heuristic_budget, playbook_budget),\n            };\n\n            // Trim heuristics and playbooks separately\n            while enrichment.heuristics.iter().map(|h| h.len()).sum::\u003cusize\u003e() / 4 \u003e h_budget\n                \u0026\u0026 !enrichment.heuristics.is_empty()\n            {\n                enrichment.heuristics.pop();\n            }\n            while enrichment.playbooks.iter().map(|p| p.len()).sum::\u003cusize\u003e() / 4 \u003e p_budget\n                \u0026\u0026 !enrichment.playbooks.is_empty()\n            {\n                enrichment.playbooks.pop();\n            }\n\n            enrichment\n        }).await;\n\n        match result {\n            Ok(enrichment) =\u003e {\n                // Cache the result\n                self.cache.lock().unwrap().put(cache_key, enrichment.clone());\n                enrichment\n            }\n            Err(_timeout) =\u003e {\n                tracing::warn!(\n                    tier = %tier,\n                    \"KG enrichment timed out after {:?}, proceeding without enrichment\",\n                    timeout,\n                );\n                KgEnrichment {\n                    kg_available: false,\n                    ..Default::default()\n                }\n            }\n        }\n    }\n}\n```\n\n---\n\n## 8. Graceful Degradation Strategy\n\n### Failure Modes \u0026 Responses\n\n| Failure Mode | Detection | Response | Impact |\n|---|---|---|---|\n| SurrealDB unreachable | TCP connect timeout | Return `KgEnrichment::default()` with `kg_available=false` | Zero — matches current behavior |\n| Query timeout | `tokio::time::timeout` expires | Return partial results or empty | Minimal — packet generated without enrichment |\n| Malformed query results | Serde deserialization error | Log warning, skip that query | Other queries still contribute |\n| Empty KG (no data yet) | Queries return 0 rows | Return empty enrichment | Expected during initial deployment |\n| Cache hit | LRU cache lookup | Return cached enrichment (fast path) | Positive — reduces query load |\n\n### Degradation Hierarchy\n\n```\n1. Full KG enrichment (heuristics + playbooks + related symbols)\n2. Partial enrichment (some queries succeeded, others timed out)\n3. Cache-only (KG unreachable but cached result exists for similar query)\n4. No enrichment (KG unavailable, no cache hit) — identical to current behavior\n```\n\n### Key Design Principle\n\nThe `KgEnricher::enrich()` method returns `KgEnrichment` directly (NOT `Result\u003cKgEnrichment, _\u003e`). This is intentional: the caller never needs to handle errors. All error handling is internal to the implementation. The `kg_available` field provides observability without forcing error handling on the caller.\n\n### Monitoring\n\n```rust\n// Add to KgEnrichment for operational visibility\npub struct KgEnrichment {\n    // ... existing fields ...\n    /// Number of queries that succeeded out of total attempted\n    pub queries_succeeded: u8,\n    pub queries_attempted: u8,\n}\n```\n\nLog structured events for dashboarding:\n```\ntracing::info!(\n    tier = %tier,\n    iteration = iteration,\n    heuristic_count = enrichment.heuristics.len(),\n    playbook_count = enrichment.playbooks.len(),\n    kg_available = enrichment.kg_available,\n    query_time_ms = enrichment.query_time_ms,\n    \"KG enrichment complete\"\n);\n```\n\n---\n\n## 9. Module Layout\n\n```\ncoordination/src/kg/\n├── mod.rs           // pub mod enricher; pub mod surreal_enricher;\n├── enricher.rs      // KgEnricher trait + KgEnrichment + NoopKgEnricher\n├── surreal_enricher.rs  // SurrealKgEnricher implementation\n├── queries.rs       // SurrealQL query string constants\n└── cache.rs         // LRU cache wrapper with hash helpers\n```\n\nAdd `mod kg;` to `coordination/src/lib.rs`.\n\n### Cargo.toml additions\n\n```toml\n[dependencies]\nsurrealdb = { version = \"2\", features = [\"kv-mem\"], optional = true }\nasync-trait = \"0.1\"\nlru = \"0.12\"\n\n[features]\ndefault = []\nkg = [\"surrealdb\"]\n```\n\nFeature-gated: `kg` feature enables SurrealDB enricher. Without it, only `NoopKgEnricher` is available. This keeps the build fast for developers who don't have SurrealDB running.\n\n---\n\n## 10. Query Pipeline Per Tier\n\n### Implementer (14B, 2K budget, 500ms timeout)\n\n```\nParallel:\n  ├── Query A: fix_patterns (LIMIT 3) → playbooks\n  └── Query E: error_heuristics → heuristics\nSequential (if time remains):\n  └── Query D: bridge docs for top symbol → append to heuristics\n```\n\n### Integrator (72B, 4K budget, 1000ms timeout)\n\n```\nParallel:\n  ├── Query A: fix_patterns (LIMIT 5) → playbooks\n  ├── Query E: error_heuristics → heuristics\n  └── Query B: symbol neighborhood → related_symbols\nSequential (if time remains):\n  └── Query C: semantic doc search (LIMIT 2) → append to heuristics\n```\n\n### Cloud (frontier, 8K budget, 2000ms timeout)\n\n```\nParallel:\n  ├── Query A: fix_patterns (LIMIT 8) → playbooks\n  ├── Query E: error_heuristics → heuristics\n  ├── Query B: symbol neighborhood → related_symbols\n  └── Query C: semantic doc search (LIMIT 4) → heuristics\nSequential:\n  └── Query D: bridge docs for all symbols → append to heuristics\n```\n\n---\n\n## 11. SurrealDB Schema Requirements\n\nThe following tables must exist in the KG for queries to work:\n\n```surql\n-- Fix patterns (populated by post-fix analysis)\nDEFINE TABLE fix_pattern SCHEMAFULL;\nDEFINE FIELD error_category ON fix_pattern TYPE string;\nDEFINE FIELD error_code ON fix_pattern TYPE option\u003cstring\u003e;\nDEFINE FIELD affected_symbol ON fix_pattern TYPE string;\nDEFINE FIELD file_path ON fix_pattern TYPE string;\nDEFINE FIELD fix_description ON fix_pattern TYPE string;\nDEFINE FIELD fix_diff_summary ON fix_pattern TYPE string;\nDEFINE FIELD success_count ON fix_pattern TYPE int DEFAULT 1;\nDEFINE FIELD last_applied ON fix_pattern TYPE datetime;\nDEFINE INDEX idx_fix_category ON fix_pattern FIELDS error_category;\n\n-- Code symbols (populated by codegraph-rust indexer)\nDEFINE TABLE symbol SCHEMAFULL;\nDEFINE FIELD name ON symbol TYPE string;\nDEFINE FIELD kind ON symbol TYPE string; -- struct, trait, fn, enum, impl\nDEFINE FIELD file_path ON symbol TYPE string;\nDEFINE FIELD line_number ON symbol TYPE option\u003cint\u003e;\nDEFINE INDEX idx_symbol_name ON symbol FIELDS name;\n\n-- Edges\nDEFINE TABLE calls SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE defines SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE implements SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE documented_by SCHEMAFULL TYPE RELATION FROM symbol TO doc_chunk;\n\n-- Documentation chunks (populated by CocoIndex pipeline)\nDEFINE TABLE doc_chunk SCHEMAFULL;\nDEFINE FIELD title ON doc_chunk TYPE string;\nDEFINE FIELD content ON doc_chunk TYPE string;\nDEFINE FIELD source_file ON doc_chunk TYPE string;\nDEFINE FIELD embedding ON doc_chunk TYPE array; -- 3584-dim float32\nDEFINE INDEX idx_doc_embedding ON doc_chunk FIELDS embedding MTREE DIMENSION 3584;\n\n-- Error heuristics (curated reference)\nDEFINE TABLE error_heuristic SCHEMAFULL;\nDEFINE FIELD error_code ON error_heuristic TYPE string;\nDEFINE FIELD category ON error_heuristic TYPE string;\nDEFINE FIELD heuristic_text ON error_heuristic TYPE string;\nDEFINE FIELD common_fixes ON error_heuristic TYPE array;\nDEFINE FIELD complexity_level ON error_heuristic TYPE int;\nDEFINE FIELD frequency ON error_heuristic TYPE int DEFAULT 0;\nDEFINE INDEX idx_heuristic_code ON error_heuristic FIELDS error_code;\n```\n\n---\n\n## 12. Open Questions for Implementation Phase\n\n1. **Embedding model**: Which model generates the `$query_embedding` for semantic doc search? Candidates: Nomic Embed V2 (3584-dim, matches schema), or a smaller model if latency is a concern. Recommendation: Use the same model that CocoIndex used to generate `doc_chunk.embedding`.\n\n2. **Fix pattern ingestion**: When does `fix_pattern` get populated? Recommendation: After a successful verifier pass, extract a diff summary + error categories from the just-closed iteration and INSERT into `fix_pattern`. This should be a post-success hook in the orchestrator loop.\n\n3. **SurrealDB deployment**: Where does SurrealDB run? Options: (a) on slurm-ctl as a systemd service (recommended — stable, NFS-accessible), (b) as a SLURM job on vasp-02 (co-located with 14B model). Depends on beefcake-swarm-3is.1 (SurrealDB deployment research).\n\n4. **Error heuristic seeding**: The `error_heuristic` table needs initial population. Recommendation: Seed from the Rust compiler error index (https://doc.rust-lang.org/error_codes/) with hand-curated common_fixes for the top 20 error codes.\n\n---\n\n## 13. Migration Path\n\n### Phase 3 (current): Design only\n- This document defines the interface\n- NoopKgEnricher ships as default\n\n### Phase 4 (implementation):\n1. Add `kg` module with trait + noop impl\n2. Make `pack_retry()` async\n3. Update callers in swarm-agents orchestrator\n4. Implement SurrealKgEnricher behind `kg` feature flag\n5. Add fix_pattern ingestion hook to orchestrator\n6. Seed error_heuristic table\n7. Integration test with in-memory SurrealDB (`kv-mem` feature)\n\n---\n\nEND OF DESIGN","status":"closed","priority":0,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:20.223004-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:31:47.888474-06:00","closed_at":"2026-02-13T08:31:47.888474-06:00","close_reason":"Design complete: KgEnricher trait, SurrealQL queries, token budgets, integration point (async pack_retry), graceful degradation strategy, schema requirements, and migration path all specified.","labels":["delegate:pal-consensus","design","epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.4.1","depends_on_id":"beefcake-swarm-3is.4","type":"parent-child","created_at":"2026-02-13T08:11:20.223784-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.4.2","title":"Implement KG-enriched WorkPacket generation","description":"Wire KG queries into ContextPacker::pack_retry(). Populate relevant_heuristics and relevant_playbooks. Optional (graceful degradation if KG unavailable). Add KgEnricher trait. Gate behind --knowledge-graph CLI flag. Respect tier token budgets (2K/4K/8K).","notes":"Files: coordination/src/context_packer/packer.rs, coordination/src/work_packet/types.rs. Must degrade gracefully — KG enrichment is additive, never blocking.","status":"open","priority":0,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:20.444056-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:20.444056-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.4.2","depends_on_id":"beefcake-swarm-3is.4","type":"parent-child","created_at":"2026-02-13T08:11:20.444825-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.4.2","depends_on_id":"beefcake-swarm-3is.4.1","type":"blocks","created_at":"2026-02-13T08:12:00.233616-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.4.2","depends_on_id":"beefcake-swarm-3is.1.4","type":"blocks","created_at":"2026-02-13T08:12:00.348129-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.4.2","depends_on_id":"beefcake-swarm-3is.2.2","type":"blocks","created_at":"2026-02-13T08:12:00.453619-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.4.3","title":"KG-aware Router task classification","description":"Enhance task_classifier.rs with KG structural signals: fan-in (callers count), coupling (importers count), trait impl complexity. Add kg_complexity_boost() method. Additive — keyword heuristic remains as fallback.","notes":"File: coordination/src/router/task_classifier.rs. Pure deterministic logic. KG signals are optional boost factors, never replace existing classification.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:20.685128-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:20.685128-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.4.3","depends_on_id":"beefcake-swarm-3is.4","type":"parent-child","created_at":"2026-02-13T08:11:20.686032-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.4.3","depends_on_id":"beefcake-swarm-3is.4.2","type":"blocks","created_at":"2026-02-13T08:12:00.70076-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.4.4","title":"KG-informed escalation signals","description":"New EscalationReason variants: HighFanIn, MultipleImplementors. New thresholds in EscalationConfig: fan_in_threshold (default 5), implementor_threshold (default 3). Pure deterministic logic, no LLM calls.","notes":"File: coordination/src/escalation/engine.rs. Additive new variants — existing escalation reasons unchanged.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:20.928312-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:20.928312-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.4.4","depends_on_id":"beefcake-swarm-3is.4","type":"parent-child","created_at":"2026-02-13T08:11:20.929189-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.4.4","depends_on_id":"beefcake-swarm-3is.4.2","type":"blocks","created_at":"2026-02-13T08:12:00.937163-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.5","title":"UKG: Self-Learning Loop","description":"Sub-epic 5: Error Pattern KB, fix pattern capture, tiered archival memory (Letta/MemGPT pattern). Agents learn from past fixes and avoid repeating failures.","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:19.653632-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.164401-06:00","labels":["epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.5","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:19.654561-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.5.1","title":"Research self-learning architectures for coding agents","description":"Research how SWE-agent, OpenHands, Goose, Cursor, Aider implement learning from past fixes. How do they store fix patterns, retrieve them, define success signals, prevent learning bad patterns? Also research DSPy feedback loops. Output: comparison matrix + recommended architecture.","notes":"PAL consensus + also try Cloud Council tool (test its functionality). This is the highest-value research task. Compare: episodic memory, RAG over past runs, structured pattern extraction, embedding-based retrieval.\n\n## COMPREHENSIVE RESEARCH: Self-Learning Architectures for Coding Agents\n\n### Research Date: 2026-02-13\n### Methodology: Web search + deep dives on 7 systems + SurrealDB graph patterns + episodic memory literature\n\n---\n\n## 1. COMPARISON MATRIX\n\n| System | What Gets Stored | Retrieval Method | Success Signal | Bad Pattern Prevention | Storage Backend |\n|--------|-----------------|------------------|----------------|----------------------|-----------------|\n| SWE-agent (SWE-Bench-CL) | Problem summaries, solutions, rationales, tool usage, success status | FAISS vector index, cosine similarity on embeddings (text-embedding-3-small), top-k retrieval | Task completion status, forward/backward knowledge transfer metrics, CL-Plasticity/Stability | Success status tracked per memory; low inter-task similarity limits spurious transfer | FAISS vector store |\n| OpenHands | Agent orchestration state, task decomposition patterns, CI/CD pipeline results | REST API with sandboxed runtimes, human feedback loop | Human verification at checkpoints, CI/CD pipeline pass | 90/10 automation/human split; human oversight on strategy | In-memory + external integrations |\n| Goose (Block) | Recipes (YAML workflow definitions) with goals, required extensions, structured inputs, sub-recipes | MCP-based tool discovery, recipe matching | Recipe execution success, tool chain completion | Open-source community review; recipe versioning | File-based YAML recipes, versioned in git |\n| Cursor Composer | Project rules (.mdc files), user rules, team rules, agent rules (AGENTS.md) | Codebase-wide semantic search, self-summarization for long contexts | RL training signal (test pass, linter pass), reinforcement learning at scale | RL training in real codebases filters low-quality patterns naturally | Rules files in .cursor/rules/, MoE model weights |\n| Aider | Git commit history with descriptive messages, conversation context | Git-integrated diff history, /run command for test feedback | Test execution results, compilation success, git commit success | Git-based rollback; human review of each commit | Git repository (commit history as memory) |\n| DSPy | Optimized prompts, few-shot examples, bootstrap demonstrations | BootstrapFewShot selection, MIPROv2 instruction generation, GEPA (Genetic-Pareto) evolution | Metric-driven evaluation (user-defined), feedback loops | Metric quality bounds bad learning; GEPA reflective optimization allows course correction | In-memory pipeline parameters, serialized optimized programs |\n| Letta/MemGPT | Core memory blocks (in-context), archival memory (explicit knowledge), recall memory (conversation history), skills (.md files) | Vector search, full-text search, hybrid search across messages/tools; graph traversal | Task completion, user satisfaction, memory relevance | Sleep-time agents for async memory refinement; memory blocks with explicit constraints/limits | Postgres (default), SQLite, vector indexes, git-versioned skills |\n\n---\n\n## 2. DETAILED SYSTEM ANALYSES\n\n### 2.1 SWE-agent / SWE-Bench-CL (Princeton)\n\n**Architecture:** Agent-Computer Interface (ACI) design. The SWE-Bench-CL paper introduces continual learning where agents vectorize task experiences.\n\n**Storage Details:**\n- After each task: problem summary, solution approach, rationale, tool usage patterns, and success/failure status are vectorized\n- Uses OpenAI text-embedding-3-small for embeddings\n- FAISS vector index for storage and retrieval\n\n**Retrieval:**\n- On new task: query memory with current problem statement\n- Cosine similarity ranking\n- Top-k memories prepended to agent's initial prompt\n- Prioritizes experiences from same task sequence\n\n**Key Insight:** Low inter-task structural similarity is a major challenge. Contextual sensitivity (\"prompt poisoning\") means retrieved memories can hurt as much as help if not well-filtered.\n\n**Metrics:** CL-Plasticity (ability to learn new), CL-Stability (retention of old), forward/backward knowledge transfer.\n\n### 2.2 OpenHands (All Hands AI)\n\n**Architecture:** Multi-agent orchestration platform. Agents operate in sandboxed Docker environments.\n\n**Learning Model:** Primarily relies on human-in-the-loop rather than automated self-learning. The platform targets 90% automation / 10% human effort, with humans providing strategy and verification at checkpoints.\n\n**Key Insight:** OpenHands' value is in orchestration (coordinating agent fleets for large refactors), not in autonomous self-learning. Learning happens implicitly through the human feedback loop and agent orchestration patterns.\n\n### 2.3 Goose (Block)\n\n**Architecture:** MCP-based extensible agent framework. Uses \"recipes\" for reusable workflows.\n\n**Storage:** Recipes are YAML workflow definitions containing:\n- Goals\n- Required MCP extensions\n- Structured inputs\n- Sub-recipes (composable workflows)\n- Shared via git repositories\n\n**Learning:** Recipes represent manually curated \"learned\" patterns. No automated self-learning from past fixes. Instead, teams share and version recipes.\n\n**Key Insight:** Recipe-based approach is deterministic and auditable but doesn't learn autonomously. Good model for encoding expert knowledge (human writes recipe once, agents reuse forever).\n\n### 2.4 Cursor Composer\n\n**Architecture:** Custom MoE model trained via reinforcement learning in real codebases.\n\n**Learning:**\n- Model trained in agentic setting with access to semantic search + test runners\n- RL training teaches practical behaviors: running tests, fixing linters, navigating large projects\n- Self-summarization for long context management\n- Rules system for persistent project/user/team preferences\n\n**Key Insight:** Cursor's \"learning\" is baked into model weights via RL, not retrievable patterns. The Rules system (.cursor/rules/*.mdc) is the closest to explicit pattern storage, but requires manual authoring.\n\n### 2.5 Aider\n\n**Architecture:** Terminal-based pair programming assistant with deep git integration.\n\n**Learning:** Git-centric implicit learning:\n- Auto-commits with descriptive messages create an audit trail\n- /run command feeds test output back for iterative fixing\n- Architect mode → Code mode → Ask mode workflow\n- No explicit pattern storage or retrieval\n\n**Key Insight:** Aider's \"memory\" IS the git history. Clean commit messages serve as a form of episodic memory. The /run feedback loop (run tests → see errors → fix → repeat) is a lightweight self-learning mechanism but doesn't persist across sessions.\n\n### 2.6 DSPy (Stanford)\n\n**Architecture:** Declarative framework for programming LMs. Key innovation: programs (not prompts) that self-optimize.\n\n**Storage:**\n- Optimized few-shot examples (bootstrapped from training data)\n- Optimized instruction strings\n- Pipeline parameters (serializable)\n\n**Optimizers:**\n- BootstrapFewShot: Finds most effective training examples\n- MIPROv2: Generates optimized prompt instructions\n- GEPA (2025): Genetic-Pareto reflective optimizer that evolves textual components\n\n**Feedback Loop:**\n1. Define metric (e.g., test pass rate)\n2. Optimizer generates candidate prompts/examples\n3. Evaluate against metric\n4. Select best performers\n5. Iterate (can achieve 25-65% improvement over standard few-shot)\n\n**Key Insight:** DSPy is the most rigorous self-learning framework. Its metric-driven approach prevents bad learning naturally (bad patterns score low). However, it optimizes prompts/examples, not stored fix patterns per se. **Most applicable to beefcake-swarm:** DSPy's feedback loop model could be adapted to optimize how work packets are constructed.\n\n### 2.7 Letta/MemGPT (UC Berkeley)\n\n**Architecture:** Most sophisticated memory system. Three-tier OS-inspired memory hierarchy:\n\n**Tier 1 - Core Memory (RAM equivalent):**\n- In-context memory blocks with labels, descriptions, values, character limits\n- Always visible to the agent\n- Agent can self-edit these blocks\n- Used for: current user info, agent persona, active task context\n\n**Tier 2 - Recall Memory (Conversation History):**\n- Complete interaction history\n- Searchable via vector, full-text, or hybrid search\n- Supports pagination for large histories\n\n**Tier 3 - Archival Memory (Disk equivalent):**\n- Explicitly stored processed knowledge\n- Agent autonomously decides what to archive\n- Vector-indexed for semantic retrieval\n- No size limit\n\n**Additional: Skills (.md files):**\n- Learned task patterns stored as markdown\n- Versioned in git\n- Shareable between agents\n- Captures repeated patterns (DB migrations, API changes)\n\n**Memory Management:**\n- Sleep-time agents: async processes that refine/consolidate memory\n- Recursive summarization for context compression\n- Eviction strategies for less relevant tokens\n- Git-based versioning of agent memories\n\n**Key Insight:** Letta's architecture is the most directly applicable to beefcake-swarm. The tiered memory model maps naturally to: Core Memory = current work packet context, Recall Memory = past fix attempts for this issue, Archival Memory = cross-issue pattern knowledge base.\n\n---\n\n## 3. SURREALDB-SPECIFIC PATTERNS FOR FIX STORAGE\n\n### 3.1 Recommended Schema\n\n```surql\n-- Error pattern node\nDEFINE TABLE error_pattern SCHEMAFULL;\nDEFINE FIELD error_code ON error_pattern TYPE string;\nDEFINE FIELD error_category ON error_pattern TYPE string;  -- borrow_checker, lifetime, trait_bounds, type_mismatch, async_send\nDEFINE FIELD error_signature ON error_pattern TYPE string;  -- normalized error message\nDEFINE FIELD embedding ON error_pattern TYPE array\u003cfloat\u003e;  -- vector embedding of error context\nDEFINE FIELD frequency ON error_pattern TYPE int DEFAULT 0;\nDEFINE FIELD first_seen ON error_pattern TYPE datetime;\nDEFINE FIELD last_seen ON error_pattern TYPE datetime;\n\n-- Fix strategy node\nDEFINE TABLE fix_strategy SCHEMAFULL;\nDEFINE FIELD description ON fix_strategy TYPE string;\nDEFINE FIELD diff_template ON fix_strategy TYPE string;  -- generalized diff pattern\nDEFINE FIELD model_tier ON fix_strategy TYPE string;  -- which model tier produced this fix\nDEFINE FIELD confidence ON fix_strategy TYPE float;  -- 0.0-1.0 based on success rate\nDEFINE FIELD embedding ON fix_strategy TYPE array\u003cfloat\u003e;\nDEFINE FIELD times_applied ON fix_strategy TYPE int DEFAULT 0;\nDEFINE FIELD times_succeeded ON fix_strategy TYPE int DEFAULT 0;\n\n-- File context node\nDEFINE TABLE file_context SCHEMAFULL;\nDEFINE FIELD file_path ON file_context TYPE string;\nDEFINE FIELD crate_name ON file_context TYPE string;\nDEFINE FIELD key_symbols ON file_context TYPE array\u003cstring\u003e;\n\n-- Relationships (graph edges)\n-- error_pattern -[FIXED_BY]-\u003e fix_strategy\nDEFINE TABLE fixed_by SCHEMAFULL;\nDEFINE FIELD success_rate ON fixed_by TYPE float;\nDEFINE FIELD last_applied ON fixed_by TYPE datetime;\n\n-- error_pattern -[OCCURS_IN]-\u003e file_context\nDEFINE TABLE occurs_in SCHEMAFULL;\nDEFINE FIELD frequency ON occurs_in TYPE int;\n\n-- fix_strategy -[MODIFIES]-\u003e file_context\nDEFINE TABLE modifies SCHEMAFULL;\nDEFINE FIELD change_type ON modifies TYPE string;  -- add, remove, modify\n\n-- fix_strategy -[ESCALATED_FROM]-\u003e fix_strategy\nDEFINE TABLE escalated_from SCHEMAFULL;\nDEFINE FIELD reason ON escalated_from TYPE string;\n\n-- error_pattern -[CO_OCCURS_WITH]-\u003e error_pattern\nDEFINE TABLE co_occurs_with SCHEMAFULL;\nDEFINE FIELD frequency ON co_occurs_with TYPE int;\n```\n\n### 3.2 Retrieval Strategy: Hybrid (Graph + Vector)\n\n**Primary Query Pattern (Concept-Based):**\n1. Embed the current error context\n2. Vector search on error_pattern.embedding for top-5 similar errors\n3. Graph traverse: error_pattern -[FIXED_BY]-\u003e fix_strategy (filter by confidence \u003e 0.7)\n4. Rank fix strategies by: (semantic_similarity * 0.4) + (confidence * 0.3) + (recency * 0.2) + (model_tier_match * 0.1)\n\n```surql\n-- Example hybrid query\nLET $query_embedding = \u003cembedding from current error\u003e;\nLET $similar_errors = (\n    SELECT *, embedding \u003c|5,40|\u003e $query_embedding AS similarity\n    FROM error_pattern\n    WHERE error_category = $current_category\n);\nLET $fix_candidates = (\n    SELECT \u003c-fixed_by\u003c-fix_strategy.* AS strategy, \n           \u003c-fixed_by.success_rate AS success_rate\n    FROM $similar_errors\n    WHERE \u003c-fixed_by.success_rate \u003e 0.7\n    ORDER BY success_rate DESC\n);\n```\n\n**Secondary Pattern (Co-occurrence Graph Walk):**\nWhen multiple errors appear together (common in Rust cascade failures):\n1. Find current error in graph\n2. Walk co_occurs_with edges to find related errors\n3. Find fixes that address the ROOT error (not the cascade)\n4. This prevents applying band-aid fixes to symptom errors\n\n### 3.3 Graph Traversal vs Vector Search vs Hybrid\n\n| Approach | Best For | Weakness |\n|----------|----------|----------|\n| Pure Vector | Novel errors never seen before | Misses structural relationships (A always follows B) |\n| Pure Graph | Known error cascades, dependency chains | Can't handle novel errors |\n| Hybrid (RECOMMENDED) | Both known and novel patterns | Slightly more complex queries |\n\n**Recommendation:** Start with hybrid. Vector search handles cold start and novel errors. Graph traversal improves as the KB grows and captures structural patterns the vector space misses.\n\n---\n\n## 4. COLD START / BOOTSTRAP STRATEGY\n\n### 4.1 Phase 1: Seed from Existing Data\n1. **Mine git history:** Parse past commits for error→fix pairs. Use `git log --all -p` + rustc error parsing to extract (error_signature, diff, success=true) tuples\n2. **Mine beads notes:** Extract error descriptions and resolution notes from closed issues\n3. **Import SWE-Bench patterns:** Use published SWE-Bench fix patterns as initial seed (Rust-specific subset)\n\n### 4.2 Phase 2: Active Learning\n1. Every verifier pass/fail generates a (error_context, fix_attempt, outcome) tuple\n2. On success: create/update error_pattern → fix_strategy edge with incremented success count\n3. On failure: create/update edge with failure count, potentially create escalated_from link\n\n### 4.3 Phase 3: Consolidation\n1. Run periodic \"sleep-time\" consolidation (borrowing from Letta):\n   - Merge similar error_pattern nodes (cosine similarity \u003e 0.95)\n   - Prune fix_strategy nodes with success_rate \u003c 0.1 and times_applied \u003e 5\n   - Generalize diff_templates from specific to abstract patterns\n\n### 4.4 Cold Start Mitigations\n- **Content-based fallback:** If KB has no relevant patterns, fall back to error_category-based heuristics (the router already classifies errors by type)\n- **Model-as-prior:** Use the LLM's pretrained knowledge as the initial \"memory\" until the KB builds up\n- **Transfer from documentation:** Seed KB with Rust compiler error explanations (rustc --explain EXXXX) mapped to common fix strategies\n\n---\n\n## 5. RETENTION / DECAY POLICIES\n\n### 5.1 Recommended Policy\n```\nConfidence decay: confidence *= 0.95 per week if not revalidated\nMinimum threshold: Remove patterns with confidence \u003c 0.1 after 30 days\nFrequency boost: Recently-used patterns get confidence += 0.05 per successful application\nMaximum age: Patterns not used in 90 days get flagged for review\nNever delete: Patterns with success_rate \u003e 0.9 and times_applied \u003e 10 (these are \"golden rules\")\n```\n\n### 5.2 Preventing Bad Pattern Learning\n1. **Minimum evidence threshold:** Require times_applied \u003e= 3 before a fix_strategy's confidence exceeds 0.5\n2. **Blind validation:** The validator agent (14B, no implementer context) independently verifies fixes - only validated fixes update confidence positively\n3. **Cascade detection:** If a fix introduces new errors, decrement confidence by 0.2 and create a co_occurs_with edge to the new error\n4. **Human veto:** Escalated-to-human fixes automatically get confidence capped at 0.3 until human approves\n5. **A/B testing:** Periodically apply KB-suggested fix vs. fresh LLM attempt on same error; only keep KB pattern if it outperforms\n\n---\n\n## 6. RECOMMENDED ARCHITECTURE FOR BEEFCAKE-SWARM\n\n### 6.1 Architecture: Hybrid Letta-Inspired + DSPy-Optimized + SurrealDB Graph\n\n**Layer 1: Working Memory (Work Packet)**\n- Current error context, file contexts, key symbols\n- Maps to Letta's \"Core Memory\"\n- Already implemented in work_packet/ module\n\n**Layer 2: Episodic Memory (SurrealDB Graph)**\n- Error patterns, fix strategies, relationships\n- Graph structure enables cascade detection and co-occurrence analysis\n- Vector embeddings enable novel error matching\n- Maps to Letta's \"Archival Memory\"\n\n**Layer 3: Strategic Memory (DSPy-optimized prompts)**\n- Optimized prompt templates per error category\n- Few-shot examples that have been validated as effective\n- Periodically re-optimized using DSPy's BootstrapFewShot against test suite metrics\n- Maps to Letta's \"Recall Memory\" but more structured\n\n**Feedback Loop (DSPy-inspired):**\n1. Error occurs → query Layer 2 (SurrealDB) for similar patterns\n2. If match found: inject fix strategy into work packet\n3. If no match: use Layer 3 optimized prompts for error category\n4. If still failing: escalate per existing escalation ladder\n5. After resolution: update Layer 2 with new evidence\n6. Periodically: re-optimize Layer 3 prompts using accumulated evidence\n\n### 6.2 Integration Points with Existing Codebase\n\n| Existing Module | Integration |\n|----------------|-------------|\n| verifier/ | Generates (error, outcome) tuples → feeds Layer 2 |\n| escalation/ | Queries Layer 2 before escalating; uses past fix data to skip unnecessary tiers |\n| router/ | Uses Layer 2 error_category statistics to improve routing accuracy |\n| work_packet/ | Enriched with Layer 2 fix suggestions before sending to model |\n| feedback/ | Closes the loop: compilation results update Layer 2 confidence scores |\n| ensemble/ | Multi-model voting results stored as evidence in Layer 2 |\n\n### 6.3 Implementation Priority\n1. SurrealDB schema + basic CRUD for error_pattern and fix_strategy\n2. Integration with verifier output parser (error → pattern extraction)\n3. Vector embedding pipeline (embed error contexts)\n4. Work packet enrichment (query KB before sending to model)\n5. Confidence update loop (success/failure feedback)\n6. DSPy prompt optimization layer (requires accumulated data)\n7. Sleep-time consolidation agent (periodic cleanup)\n\n---\n\n## 7. KEY TAKEAWAYS\n\n1. **No existing system does exactly what beefcake-swarm needs.** SWE-agent has the closest retrieval architecture (vector-based episodic memory), but lacks graph structure for error cascades. Letta has the best memory hierarchy, but isn't code-fix-specific. DSPy has the best feedback loop, but optimizes prompts not fix patterns.\n\n2. **The hybrid approach is novel and valuable.** Combining graph structure (for Rust error cascades, which are inherently relational) with vector search (for novel errors) with DSPy-style metric optimization (for prompt quality) creates a system more capable than any single existing approach.\n\n3. **SurrealDB is a strong fit** because it natively supports document + graph + vector in one database, eliminating the need to sync between a vector DB and a graph DB.\n\n4. **The cold start problem is solvable** by mining existing git history and beads notes, plus using the Rust compiler's own error explanations as seed data.\n\n5. **Blind validation (existing validator agent) is a critical anti-bad-learning mechanism** that most other systems lack. The beefcake-swarm architecture already has this with the independent 14B validator.\n\n---\n\nSources:\n- SWE-Bench-CL: https://arxiv.org/html/2507.00014v1\n- SWE-agent: https://github.com/princeton-nlp/SWE-agent\n- OpenHands: https://github.com/OpenHands/OpenHands\n- Goose: https://github.com/block/goose\n- Cursor: https://dasroot.net/posts/2026/02/cursor-ai-deep-dive-technical-architecture-advanced-features-best-practices/\n- Aider: https://github.com/Aider-AI/aider\n- DSPy: https://github.com/stanfordnlp/dspy / https://arxiv.org/abs/2310.03714\n- Letta/MemGPT: https://docs.letta.com/concepts/memgpt/ / https://www.letta.com/blog/letta-code / https://www.letta.com/blog/letta-v1-agent\n- SurrealDB KG-RAG: https://surrealdb.com/blog/knowledge-graph-rag-two-query-patterns-for-smarter-ai-agents\n- Episodic Memory in AI Agents: https://arxiv.org/html/2601.11653 / https://openreview.net/pdf?id=U51WxL382H\n- Agent Memory Systems: https://www.digitalapplied.com/blog/ai-agent-memory-systems-complete-guide","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:21.159109-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:33:18.944008-06:00","closed_at":"2026-02-13T08:33:18.944008-06:00","close_reason":"Comprehensive research completed. Analyzed 7 systems (SWE-agent, OpenHands, Goose, Cursor, Aider, DSPy, Letta/MemGPT). Produced comparison matrix, SurrealDB schema design, hybrid retrieval strategy, cold start plan, retention policies, and recommended 3-layer architecture (Work Packet + SurrealDB Graph + DSPy Prompts). Key finding: no existing system does exactly what beefcake-swarm needs - the hybrid graph+vector+metric-optimization approach is novel.","labels":["delegate:pal-consensus","epic:ukg","research"],"dependencies":[{"issue_id":"beefcake-swarm-3is.5.1","depends_on_id":"beefcake-swarm-3is.5","type":"parent-child","created_at":"2026-02-13T08:11:21.160211-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.5.2","title":"Implement Error Pattern KB in SurrealDB","description":"Build Error Pattern KB as SurrealDB graph nodes. Schema: fix_pattern node (error_signature, original_code, fix_diff, strategy, model_tier, iterations, success_count). Edges: fix_pattern-\u003efixed-\u003efunction. Query: top-3 matching past fixes by error category + file + symbols, ranked by success rate + recency. Relates to beefcake-swarm-3r9 (may supersede or complement depending on 1.5 investigation).","notes":"Storage decision depends on 1.5 investigation. If SurrealDB wins, this supersedes 3r9. If not, this becomes a SurrealDB mirror of the RocksDB store for cross-issue queries.","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:21.384524-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:21.384524-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.5.2","depends_on_id":"beefcake-swarm-3is.5","type":"parent-child","created_at":"2026-02-13T08:11:21.385456-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.2","depends_on_id":"beefcake-swarm-3is.5.1","type":"blocks","created_at":"2026-02-13T08:12:01.182534-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.2","depends_on_id":"beefcake-swarm-3is.1.4","type":"blocks","created_at":"2026-02-13T08:12:01.28039-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.2","depends_on_id":"beefcake-swarm-3is.1.5","type":"blocks","created_at":"2026-02-13T08:12:01.377201-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.5.3","title":"Wire learning feedback into orchestrator loop","description":"After successful verification, capture: diff, error categories fixed, model tier, iteration count. Store as fix pattern via KgClient. Non-blocking, fire-and-forget. Also track per-model success rates by error category for routing optimization.","notes":"File: crates/swarm-agents/src/main.rs. Must be non-blocking — never slow down the main loop. Use tokio::spawn for async storage.","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:21.602965-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:21.602965-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.5.3","depends_on_id":"beefcake-swarm-3is.5","type":"parent-child","created_at":"2026-02-13T08:11:21.60382-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.3","depends_on_id":"beefcake-swarm-3is.5.2","type":"blocks","created_at":"2026-02-13T08:12:01.589416-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.3","depends_on_id":"beefcake-swarm-3is.4.2","type":"blocks","created_at":"2026-02-13T08:12:01.687371-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.5.4","title":"Tiered archival memory (Letta/MemGPT pattern)","description":"Per-issue memory in SurrealDB: working memory = WorkPacket, archival = prior diffs/reports/decisions per bead_id. On pack_retry, recall failed approaches. Promotion rules: successful fixes → shared KB, failures → scoped to bead_id. Relates to beefcake-swarm-b30 (may supersede or complement depending on 1.5 investigation).","notes":"Storage decision depends on 1.5 investigation. Key pattern: working memory (current context) vs archival memory (past attempts). Prevents retrying the same failed approach.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:21.842369-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:21.842369-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.5.4","depends_on_id":"beefcake-swarm-3is.5","type":"parent-child","created_at":"2026-02-13T08:11:21.843653-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.4","depends_on_id":"beefcake-swarm-3is.5.2","type":"blocks","created_at":"2026-02-13T08:12:01.9143-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.5.4","depends_on_id":"beefcake-swarm-3is.1.5","type":"blocks","created_at":"2026-02-13T08:12:02.015741-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.6","title":"UKG: Benchmarking (pgvector vs SurrealDB)","description":"Sub-epic 6: Design and run benchmarks comparing pgvector and SurrealDB on latency, quality, throughput. Produce migration decision.","status":"in_progress","priority":2,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:19.863588-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.166863-06:00","labels":["benchmark","epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.6","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:19.864354-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.6.1","title":"Design benchmark framework (pgvector vs SurrealDB)","description":"Design benchmark comparing pgvector and SurrealDB: vector search latency (p50/p95/p99), hybrid search quality (graph+vector vs vector-only), write throughput, query flexibility. Define 10 benchmark queries from real agent workloads.","notes":"PAL chat with gpt-5.2 (Manager role — task decomposition). Queries should cover: similar error lookup, symbol dependency traversal, hybrid doc+code search, write-heavy pattern storage.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:22.062597-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:22.062597-06:00","labels":["benchmark","delegate:gpt-5.2","design","epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.6.1","depends_on_id":"beefcake-swarm-3is.6","type":"parent-child","created_at":"2026-02-13T08:11:22.063533-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.6.1","depends_on_id":"beefcake-swarm-3is.3.2","type":"blocks","created_at":"2026-02-13T08:12:02.247737-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.6.1","depends_on_id":"beefcake-swarm-3is.2.2","type":"blocks","created_at":"2026-02-13T08:12:02.345617-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.6.2","title":"Run benchmarks and produce comparison report","description":"Implement benchmark harness in Rust. Run against both stores with identical data. Produce comparison report with latency histograms, quality scores, operational assessment.","notes":"Use criterion.rs for microbenchmarks. Run on ai-proxy LXC where both stores live. Report format: markdown with embedded charts (plotters crate).","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:22.282273-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:22.282273-06:00","labels":["benchmark","delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.6.2","depends_on_id":"beefcake-swarm-3is.6","type":"parent-child","created_at":"2026-02-13T08:11:22.283136-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.6.2","depends_on_id":"beefcake-swarm-3is.6.1","type":"blocks","created_at":"2026-02-13T08:12:02.563757-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.6.3","title":"Migration decision (pgvector → SurrealDB)","description":"Based on benchmark results + 1.5 investigation, make go/no-go on full pgvector→SurrealDB migration. Frontier consensus + human sign-off required.","notes":"PAL consensus. This is a blocking decision for long-term architecture. Three outcomes: (1) full migration, (2) keep both with role separation, (3) stay with pgvector.","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:22.500512-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:22.500512-06:00","labels":["delegate:pal-consensus","design","epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.6.3","depends_on_id":"beefcake-swarm-3is.6","type":"parent-child","created_at":"2026-02-13T08:11:22.501405-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.6.3","depends_on_id":"beefcake-swarm-3is.6.2","type":"blocks","created_at":"2026-02-13T08:12:02.779652-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.6.3","depends_on_id":"beefcake-swarm-3is.1.5","type":"blocks","created_at":"2026-02-13T08:12:02.875491-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.7","title":"UKG: MCP Tool Exposure","description":"Sub-epic 7: Expose KG as MCP tools: search_knowledge_graph (hybrid vector+graph), query_code_graph (structural), store_knowledge (agent write-back).","status":"in_progress","priority":1,"issue_type":"epic","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:09:20.084551-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:24:45.169343-06:00","labels":["epic:ukg"],"dependencies":[{"issue_id":"beefcake-swarm-3is.7","depends_on_id":"beefcake-swarm-3is","type":"parent-child","created_at":"2026-02-13T08:09:20.085333-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.7.1","title":"Add search_knowledge_graph MCP tool","description":"New MCP tool in coordination/src/main.rs. Accepts query string, returns semantically similar code + related doc chunks + graph-traversed dependencies. Hybrid search: embed query via nomic on vasp-02:8080, then vector + graph traversal. Gate behind --knowledge-graph flag.","notes":"Follow ask_rust_architect pattern at coordination/src/main.rs line 563. Return format: ranked list of (code_symbol, relevance_score, related_docs, dependency_chain).","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:22.724167-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:22.724167-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.7.1","depends_on_id":"beefcake-swarm-3is.7","type":"parent-child","created_at":"2026-02-13T08:11:22.725051-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.7.1","depends_on_id":"beefcake-swarm-3is.1.4","type":"blocks","created_at":"2026-02-13T08:12:03.104917-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.7.1","depends_on_id":"beefcake-swarm-3is.2.2","type":"blocks","created_at":"2026-02-13T08:12:03.202286-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.7.2","title":"Add query_code_graph MCP tool (structural queries)","description":"Pure structural queries: 'what calls X?', 'what implements Y?', 'what imports Z?'. Deterministic, no embeddings needed. Useful for Planner agent (beefcake-swarm-j4l).","notes":"SurrealQL graph traversal only. No vector search. Fast path for structural questions. Expose as separate tool for clarity.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:22.946001-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:22.946001-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.7.2","depends_on_id":"beefcake-swarm-3is.7","type":"parent-child","created_at":"2026-02-13T08:11:22.946844-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.7.2","depends_on_id":"beefcake-swarm-3is.7.1","type":"blocks","created_at":"2026-02-13T08:12:03.425022-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3is.7.3","title":"Add store_knowledge MCP tool (agent write-back)","description":"Agents write knowledge back to KG: design decisions, architecture notes, 'this function is tricky because...'. Schema: agent_note with confidence field (decays over time). Lower weight than code-derived facts.","notes":"Confidence decay: new notes start at 1.0, decay by 0.1 per week. Agent-contributed knowledge is always lower-ranked than code-derived facts in search results.","status":"open","priority":3,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T08:11:23.193209-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T08:11:23.193209-06:00","labels":["delegate:claude-subagent","epic:ukg","implementation"],"dependencies":[{"issue_id":"beefcake-swarm-3is.7.3","depends_on_id":"beefcake-swarm-3is.7","type":"parent-child","created_at":"2026-02-13T08:11:23.194068-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-3is.7.3","depends_on_id":"beefcake-swarm-3is.7.1","type":"blocks","created_at":"2026-02-13T08:12:03.704176-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-3kk","title":"Configure rig-core client timeouts","description":"The Implementer and Validator create rig-core CompletionsClient instances without configuring timeouts. If the inference server hangs (GPU OOM, SLURM preemption, network partition), the HTTP request blocks indefinitely and the orchestrator loop stalls.\n\nFix: Check rig-core's client builder for timeout configuration:\n1. Set connect_timeout (5s) and request_timeout (5min for 72B reasoning, 2min for 14B fast)\n2. Add these as fields in SwarmConfig per endpoint\n3. If rig-core doesn't support timeouts directly, wrap the implement/validate calls with tokio::time::timeout()\n\nAlso consider: retry logic for transient HTTP failures (502, 503 from inference server starting up).\n\nFiles: crates/swarm-agents/src/implementer.rs, crates/swarm-agents/src/validator.rs, crates/swarm-agents/src/config.rs\nFound by: G3-Pro deep review","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:34.003768-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:14:34.003768-06:00"}
{"id":"beefcake-swarm-3qg","title":"Fix redundant verifier run in orchestrator loop","description":"The orchestrator loop in main.rs runs the verifier TWICE per retry iteration: once at the end of iteration N (line 253) to check implementer output, then again at the start of iteration N+1 (line 217) to build retry context for pack_retry(). Rust compilation is expensive (~seconds per run); doubling it wastes time and cluster resources.\n\nFix: Carry the VerifierReport forward from the end of each loop iteration into the next via Option\u003cVerifierReport\u003e. Only run the verifier once per iteration. The report from the failed iteration already contains everything pack_retry() needs.\n\nFiles: crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:11:26.495938-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:11:26.495938-06:00"}
{"id":"beefcake-swarm-3r9","title":"Build Error Pattern Knowledge Base for cross-issue learning","description":"UNANIMOUS CONSENSUS (4/4 models)\n\nThe swarm currently has NO memory across issues. EscalationState tracks error history for one issue only. The same borrow checker error pattern might be solved 50 times without the swarm ever learning the fix.\n\nImplement a persistent Error Pattern KB:\n1. After each successful fix, store: (error_signature, original_code_snippet, fix_diff, strategy_description, model_tier, iterations_to_resolve)\n2. Before each implementation attempt, query the KB for similar past errors (match on ErrorCategory + key tokens from message + file structure)\n3. Include top-3 matching successful strategies in WorkPacket.relevant_playbooks (field already exists but is always empty)\n\nStorage: Use the existing RocksDB infrastructure in coordination/state/. Add a new column family for fix patterns.\n\nError signature should include: ErrorCategory, rustc error code, key tokens from the error message (e.g. 'cannot borrow', 'lifetime mismatch'), affected symbol types.\n\nThis is essentially few-shot learning from the swarm's own history. G3-Pro calls it 'Compiler RAG'. Opus calls it 'the missing flywheel'.\n\nAlso track per-model success rates by error category to inform routing decisions (e.g. if 14B has \u003c40% success on Lifetime errors, escalate immediately).\n\nFiles: coordination/src/state/ (new KB module), coordination/src/context_packer/packer.rs, coordination/src/work_packet/generator.rs","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:31.480361-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:31.480361-06:00"}
{"id":"beefcake-swarm-3rl","title":"Revisit AdalFlow/TextGrad prompt optimization after 50+ completed issues","description":"All 4 models agreed: prompt optimization (AdalFlow, TextGrad) is premature before we have data. Revisit after the swarm has completed 50+ issues with logged prompts and outcomes. At that point: (1) Build scoring function from validator pass rate + iteration count, (2) Run A/B tests on prompt variations, (3) Evaluate whether systematic prompt tuning beats manual iteration. Depends on SWE-bench eval harness being built first. Backlog until data exists.","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:50:02.027259-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:50:02.027259-06:00","dependencies":[{"issue_id":"beefcake-swarm-3rl","depends_on_id":"beefcake-swarm-yhx","type":"blocks","created_at":"2026-02-11T16:50:22.577043-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-481","title":"Evaluate DSRs (dspy-rs) for structured LLM pipeline optimization","description":"DSRs (https://dsrs.herumbshandilya.com/) is a Rust-native rewrite of the DSPy framework for programming robust LM-powered applications. It provides typed Signatures, composable Modules, Predictors, and Optimizers (COPRO, MIPROv2, GEPA) for automatically improving LLM prompts and pipelines.\n\nCritically, DSRs already uses rig-core as its LM backend — the same crate swarm-agents uses for Implementer and Validator. This means integration could be relatively seamless.\n\nKey opportunities for beefcake-swarm:\n\n1. **Signature-based prompting**: Replace the hand-rolled format_work_packet() prompt builder with typed DSRs Signatures. This gives structured input/output contracts that the framework can optimize automatically.\n\n2. **Prompt optimization**: Use DSRs optimizers to automatically tune the implementer and validator prompts. Instead of manually iterating on prompt wording, COPRO/MIPROv2 can search for better prompts given a set of examples and a metric (e.g. verifier pass rate).\n\n3. **Evaluation framework**: DSRs' evaluate module could replace or augment the validator's pass/fail heuristic (starts_with PASS) with a more robust evaluation pipeline.\n\n4. **Pipeline composition**: The swarm loop (pack -\u003e implement -\u003e verify -\u003e validate) maps naturally to DSRs' Module composition pattern, potentially simplifying the orchestrator.\n\nInvestigation steps:\n1. Add dspy-rs = \"0.7\" to swarm-agents/Cargo.toml\n2. Define Signatures for the implementer task (input: WorkPacket fields, output: code changes) and validator task (input: diff, output: pass/fail + feedback)\n3. Wrap Implementer and Validator as DSRs Modules\n4. Evaluate whether DSRs Predictors can replace the raw rig-core prompt() calls\n5. Prototype an optimizer run on a set of known-good/bad code changes to auto-tune prompts\n\nRisks:\n- DSRs is in beta (v0.7.3) — API may have breaking changes\n- Optimizer training requires a dataset of examples with ground truth\n- May add significant dependency weight (arrow, parquet deps)\n- Unclear if DSRs supports local llama.cpp endpoints natively (needs OpenAI-compatible API adapter)\n\nReference: https://github.com/krypticmouse/DSRs, https://crates.io/crates/dspy-rs, https://docs.rs/dspy-rs\n\nDepends on: agent traits extraction (needed to wrap agents as DSRs Modules)","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:15:30.482492-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:15:30.482492-06:00","dependencies":[{"issue_id":"beefcake-swarm-481","depends_on_id":"beefcake-swarm-7ny","type":"blocks","created_at":"2026-02-11T16:15:39.14492-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-4j8","title":"Use git status -z for robust porcelain parsing","description":"FileWalker::modified_files() and WorktreeBridge merge checks parse 'git status --porcelain' output by slicing at [3..]. While porcelain v1 format is stable for normal cases, renamed files produce 'R  old -\u003e new' format, and filenames containing spaces or special characters can cause incorrect parsing.\n\nFix: Switch to 'git status -z --porcelain' which uses NUL byte separators instead of newlines, handles renames as two separate NUL-separated entries, and correctly handles filenames with spaces, quotes, and unicode characters. Parse by splitting on \\0 instead of \\n.\n\nFiles: coordination/src/context_packer/file_walker.rs, coordination/src/work_packet/generator.rs\nFound by: G3-Pro deep review","status":"closed","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:36.271461-06:00","created_by":"TheFermiSea","updated_at":"2026-02-12T11:14:25.714746-06:00","closed_at":"2026-02-12T11:14:25.714746-06:00","close_reason":"Fixed in PR #1 review commit a26ba46"}
{"id":"beefcake-swarm-4p5","title":"Iteration delta memory (structured context evolution)","description":"UNIQUE INSIGHT from GPT-5.2-Codex and Opus\n\nReplace flat 'previous_attempts' strings with structured iteration deltas that capture WHAT CHANGED between iterations, not just what happened.\n\nAdd IterationDelta struct:\n- fixed_errors: Vec\u003cErrorCategory\u003e — what improved\n- new_errors: Vec\u003cErrorCategory\u003e — what regressed  \n- files_modified: Vec\u003cString\u003e — what was touched\n- hypothesis: Option\u003cString\u003e — what the model claimed it was doing (extracted from response)\n- result_summary: String — 'borrow error fixed, but introduced lifetime error in return type'\n- strategy_used: String — 'added Arc wrapper' or 'changed lifetime annotation'\n\nInclude only the last 2-3 deltas in context (not full history). The model needs to see 'you fixed X but broke Y' not 'here is everything that ever went wrong'.\n\nAlso add provenance tags to FileContext (source: compiler_error, diff, dependency, import) and decay: reduce weight of previously included context not re-referenced by new errors.\n\nFiles: coordination/src/work_packet/types.rs (new IterationDelta type), coordination/src/escalation/state.rs, coordination/src/context_packer/packer.rs","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:35:37.668508-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:35:37.668508-06:00"}
{"id":"beefcake-swarm-5bk","title":"Integrate tree-sitter-rust for AST-aware context packing","description":"Replace regex/line-based symbol extraction in ContextPacker with tree-sitter-rust AST parsing. Currently build_file_contexts() takes first 30 lines of each file (packer.rs:94-136) which misses critical context. With tree-sitter: extract fn signatures, impl blocks, trait definitions, struct fields at AST level. On verifier failures, use rustc error spans to pack ±80 lines around each span + referenced symbol definitions. This dramatically improves context quality and reduces iterations. Deps: tree-sitter, tree-sitter-rust crates. Files: coordination/src/context_packer/packer.rs, coordination/src/context_packer/file_walker.rs, new file coordination/src/context_packer/ast_index.rs. All 4 consulted models (G3-Pro, GPT-5.2-Codex, GPT-5.2, Claude Opus 4.5) flagged this as highest priority. Opus rated it P0.","status":"open","priority":0,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:48:44.470879-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:48:44.470879-06:00"}
{"id":"beefcake-swarm-5iz","title":"Persist EscalationState across crashes","description":"EscalationState is purely in-memory. If the orchestrator crashes at iteration 5 of 6, the next run starts fresh at iteration 1, losing all error history, escalation records, and iteration counts. This causes:\n- Infinite loops if the crash is triggered by a specific payload\n- Lost 'learning' from previous error patterns\n- Incorrect escalation decisions (tier budgets reset)\n\nFix: After each call to escalation.record_iteration(), serialize the EscalationState to a JSON file inside the worktree directory (e.g. \u003cworktree\u003e/.beefcake-state.json). On loop startup, check if the state file exists in the worktree and deserialize it to resume.\n\nImplementation:\n1. Add save_to_file(\u0026self, path: \u0026Path) -\u003e Result\u003c()\u003e to EscalationState\n2. Add load_from_file(path: \u0026Path) -\u003e Result\u003cOption\u003cSelf\u003e\u003e to EscalationState\n3. In main.rs, after creating worktree, check for existing state file\n4. After each record_iteration(), save state\n\nFiles: coordination/src/escalation/state.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:12:29.339283-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:12:29.339283-06:00"}
{"id":"beefcake-swarm-5uq","title":"Add Adversarial Breaker agent for pre-merge red-teaming","description":"STRONG CONSENSUS (3/4 models)\n\nBefore merging, a dedicated agent should actively try to BREAK the implementation by generating edge-case tests, weird inputs, concurrency interleavings, and invalid states.\n\nThe SwarmTier::Adversary already exists in escalation/state.rs but isn't integrated into the loop.\n\nImplementation:\n1. After Verifier passes (all green), invoke the Adversary agent\n2. Give it ONLY the diff and public API signatures (no implementation context — truly adversarial)\n3. It generates: proptest harnesses, edge-case unit tests, boundary condition checks\n4. Run these generated tests through the Verifier\n5. If any Adversary test fails, reject the implementation and feed the failing test back to the Implementer\n\nThis catches 'compiles but is wrong' — the biggest failure mode that compilation gates miss.\n\nG3-Pro: 'Active Sabotage / Mutation Testing'. Opus: 'Adversary actively tries to break the code'.\n\nFiles: crates/swarm-agents/src/ (new adversary module), crates/swarm-agents/src/main.rs","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:43.60542-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:43.60542-06:00","dependencies":[{"issue_id":"beefcake-swarm-5uq","depends_on_id":"beefcake-swarm-j4l","type":"blocks","created_at":"2026-02-11T16:35:50.750484-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-687","title":"Add retry with backoff for transient git failures","description":"Git operations can fail transiently with index.lock errors when multiple processes access the same repo (e.g. worktree operations while the main repo is active, or concurrent prune/gc). Currently all git commands in WorktreeBridge and BeadsBridge fail immediately on any error.\n\nFix: Create a retry_git_command() helper that wraps Command execution with exponential backoff (3 retries, 100ms/500ms/2s delays). Apply specifically to git worktree add, git merge, and git commit operations which are most susceptible to lock contention. Only retry on stderr containing 'index.lock' or 'Unable to create'.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:11:42.798648-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:11:42.798648-06:00"}
{"id":"beefcake-swarm-7cg","title":"Manager re-invokes workers after tests pass, wasting turns","description":"Job 1606: workers successfully wrote test_sanitize_id_edge_cases and all 30 tests passed, but the manager kept re-invoking workers (6 invocations, each burning 25 turns) to fix formatting and re-verify. Root cause: manager doesn't recognize when the core task is done. Needs: (1) verifier should signal pass/fail to manager, (2) manager should stop iterating when verifier passes, (3) consider adding a 'task complete' signal from worker back to manager.","status":"closed","priority":2,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-15T22:32:48.906666-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T22:38:22.333107-06:00","closed_at":"2026-02-15T22:38:22.333107-06:00","close_reason":"Verifier all_green is now authoritative success signal, reviewer loop removed"}
{"id":"beefcake-swarm-7g0","title":"Deploy beefcake-swarm to ai-proxy LXC","description":"Clone repo to ai-proxy (ssh root@100.105.113.58), build workspace, configure endpoints. Verify cargo build --workspace succeeds on the LXC. Set up systemd service or SLURM job for orchestrator.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:27.161098-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:27.161098-06:00","dependencies":[{"issue_id":"beefcake-swarm-7g0","depends_on_id":"beefcake-swarm-7jt","type":"blocks","created_at":"2026-02-11T15:32:56.697608-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-7go","title":"Detect and clean zombie branches on startup","description":"If the orchestrator crashes mid-loop, /tmp may clean the worktree directory but the swarm/\u003cissue_id\u003e branch persists in the repo. On next run, git worktree add -b fails with 'A branch named swarm/... already exists' and the agent gets permanently stuck on that issue.\n\nFix: On startup (before the main loop), scan for orphaned swarm/* branches that have no corresponding worktree directory. For each orphan: attempt git branch -D to delete it. Also check git worktree list --porcelain for stale entries and run git worktree prune. This requires the remove_worktree() method from the worktree cleanup issue.\n\nAdd a WorktreeBridge::cleanup_stale() method that:\n1. Runs git worktree prune\n2. Lists all branches matching swarm/*\n3. For each, checks if worktree_path(id) exists\n4. If not, force-deletes the branch\n\nCall cleanup_stale() in main() before entering the issue-picking loop.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs, crates/swarm-agents/src/main.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:12:21.043384-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:12:21.043384-06:00","dependencies":[{"issue_id":"beefcake-swarm-7go","depends_on_id":"beefcake-swarm-rb2","type":"blocks","created_at":"2026-02-11T16:15:38.551142-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-7jt","title":"Add coordination crate integration tests for standalone build","description":"The coordination crate was copied from beefcake2. Verify all existing tests pass in the new standalone context. Fix any path assumptions or missing dependencies. Run cargo test -p coordination.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:30.152182-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:30.152182-06:00"}
{"id":"beefcake-swarm-7ny","title":"Extract agent traits for dependency injection and testability","description":"Implementer and Validator are concrete structs that make real HTTP calls to inference endpoints. The orchestrator loop in main.rs instantiates them directly, making it impossible to unit test the loop logic without a running inference server.\n\nFix: Define traits in a new module (e.g. crates/swarm-agents/src/agents.rs):\n\npub trait ImplementerAgent {\n    async fn implement(\u0026self, task_description: \u0026str) -\u003e Result\u003cString\u003e;\n}\n\npub trait ValidatorAgent {\n    async fn validate(\u0026self, diff: \u0026str) -\u003e Result\u003cValidationResult\u003e;\n}\n\nHave the existing Implementer and Validator implement these traits. Refactor main.rs to accept trait objects (Box\u003cdyn ImplementerAgent\u003e) or use generics. This enables mock implementations for testing.\n\nConsider using mockall or manual mock structs for integration tests of the orchestrator loop.\n\nFiles: crates/swarm-agents/src/implementer.rs, crates/swarm-agents/src/validator.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:27.012071-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:27.012071-06:00"}
{"id":"beefcake-swarm-8nm","title":"Build context packer for agent context windows","description":"Agents need a repo packer to build context windows. Options: tree-sitter AST extraction, repomap (Aider-style), or custom Rust walker. Must respect .gitignore, count tokens, fit in model context. Can leverage indexing/index_flow_v2.py for semantic search.","status":"closed","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:13.558286-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:02:59.325665-06:00","closed_at":"2026-02-11T16:02:59.325665-06:00","close_reason":"Closed"}
{"id":"beefcake-swarm-94s","title":"Evaluate llama-cpp-rs native bindings vs HTTP overhead","description":"Claude Opus 4.5 suggested evaluating llama-cpp-rs for native Rust bindings to llama.cpp, eliminating HTTP overhead. Currently we call llama.cpp via OpenAI-compatible HTTP API through rig-core. Evaluate: (1) Does llama-cpp-rs support grammar/GBNF constraints? (2) Latency improvement vs HTTP? (3) Can it coexist with SLURM job management? (4) Does it complicate the SLURM lifecycle (inference runs on compute nodes, orchestrator on controller)? Note: HTTP may actually be correct for our cluster architecture where inference and orchestration run on different nodes.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:50:05.882818-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:50:05.882818-06:00"}
{"id":"beefcake-swarm-9w5","title":"Increase SLURM time limit for swarm orchestrator (1h→4h)","status":"closed","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-15T18:45:30.922805-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T19:47:28.612628-06:00","closed_at":"2026-02-15T19:47:28.612628-06:00","close_reason":"Fixed in dogfood round 2: 1h→4h"}
{"id":"beefcake-swarm-a3y","title":"Improve token counting accuracy (bytes vs chars)","description":"WorkPacket::estimated_tokens() uses json.len() / 4 where String::len() returns byte count, not character count. For pure ASCII Rust code this is accurate, but multi-byte UTF-8 characters (unicode identifiers, emoji in comments, non-ASCII strings) cause byte count to overestimate character count, which then underestimates token count.\n\nAdditionally, the 4-chars-per-token heuristic is rough. JSON serialization adds overhead (field names, braces, escaping) that inflates the count vs what the LLM tokenizer actually sees.\n\nFix (incremental):\n1. Short term: Switch to json.chars().count() / 4 for slightly better accuracy\n2. Medium term: Use tiktoken-rs or a BPE tokenizer for the specific model family\n3. Add a safety margin (e.g. budget * 0.9) to avoid edge-case context overflow\n\nFiles: coordination/src/work_packet/types.rs\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:30.728451-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:30.728451-06:00"}
{"id":"beefcake-swarm-a58","title":"Fix write_file tool: handle JSON-escaped content from local models","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-15T18:45:30.831688-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T19:47:28.383859-06:00","closed_at":"2026-02-15T19:47:28.383859-06:00","close_reason":"Fixed in dogfood round 2"}
{"id":"beefcake-swarm-a8m","title":"Add merge conflict test for WorktreeBridge","description":"WorktreeBridge tests cover create and list but not the merge conflict scenario, which is the most operationally dangerous path. When the main branch moves forward with a conflicting change while the worktree branch also makes changes, merge_and_remove() will fail — and the cleanup behavior needs to be verified.\n\nFix: Add test case to worktree_bridge.rs::tests:\n1. Create a worktree\n2. Commit a change to a file on the main branch\n3. Commit a conflicting change to the same file in the worktree\n4. Call merge_and_remove() — assert it returns Err\n5. Verify the worktree still exists (not accidentally deleted)\n6. Call remove_worktree() — assert it cleans up (depends on worktree cleanup method)\n\nAlso test: merge_and_remove() with uncommitted changes (should fail with descriptive error).\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:37.969298-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:37.969298-06:00","dependencies":[{"issue_id":"beefcake-swarm-a8m","depends_on_id":"beefcake-swarm-rb2","type":"blocks","created_at":"2026-02-11T16:15:38.919132-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-ago","title":"Set up CI with GitHub Actions","description":"Add .github/workflows/ci.yml with cargo check, cargo test, cargo clippy, cargo fmt --check. Cache cargo registry and target dir. Consider separate jobs for coordination and swarm-agents.","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:34.906223-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:34.906223-06:00"}
{"id":"beefcake-swarm-aq1","title":"Design rig-core Manager-Worker migration plan","status":"closed","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-13T12:21:56.345729-06:00","created_by":"TheFermiSea","updated_at":"2026-02-13T12:26:38.661731-06:00","closed_at":"2026-02-13T12:26:38.661731-06:00","close_reason":"Migration plan and tool skeleton delivered"}
{"id":"beefcake-swarm-axn","title":"AST-aware context packing via tree-sitter","description":"UNANIMOUS CONSENSUS (4/4 models)\n\nThe current ContextPacker uses 'first 30 lines of each file' as context — this is spatial, not semantic. All 4 reviewing models flagged this as a critical weakness causing hallucinated APIs and wasted context budget.\n\nReplace with AST-aware context extraction:\n1. Integrate tree-sitter-rust for Rust AST parsing\n2. Extract semantic units: full function bodies at error spans, struct/enum definitions, trait bounds, impl blocks\n3. Score context by relevance: error spans \u003e call sites \u003e modified files \u003e imports \u003e headers\n4. Include symbol-to-file map so the LLM knows 'where to edit' without searching\n\nSpecific improvements to build_file_contexts():\n- Instead of first 30 lines, extract the function/impl containing the error span\n- Include trait definitions referenced by trait bound errors\n- Include callers/callees of modified functions\n- Score each FileContext with a priority (Error \u003e Modified \u003e Dependency \u003e Header)\n\nConsider: headless rust-analyzer integration as a higher-fidelity alternative to tree-sitter. Start with tree-sitter for speed, upgrade to RA later.\n\nDependencies: tree-sitter = '0.24', tree-sitter-rust\nFiles: coordination/src/context_packer/packer.rs, coordination/Cargo.toml","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:26.120968-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:26.120968-06:00"}
{"id":"beefcake-swarm-ayy","title":"Add GBNF grammar constraints for structured LLM output","description":"When JSON adherence is flaky from llama-server, use GBNF grammar constraints. Add grammar parameter support to implementer and validator agents. Define grammars for common output formats (code blocks, pass/fail verdicts, structured feedback).","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:23.009266-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:23.009266-06:00"}
{"id":"beefcake-swarm-b30","title":"Implement archival memory with recall for retries (Letta/MemGPT pattern)","description":"Steal Letta/MemGPT pattern: build RocksDB-backed tiered memory system. Working memory = current WorkPacket. Archival memory = queryable store of prior diffs, verifier reports, repeated compiler errors, decisions, design notes per bead_id. On pack_retry, recall relevant prior approaches that failed (FailedApproachIndex). Add memory promotion rules: successful fix patterns get promoted, failed approaches get stored with error context. Use existing RocksDB in coordination/src/state/. Add memory schema, summarizer hook, and recall policy. Files: coordination/src/state/ (new memory module), coordination/src/context_packer/packer.rs (recall integration). Medium effort, high payoff for multi-iteration issues.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:39.77938-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:39.77938-06:00"}
{"id":"beefcake-swarm-bbv","title":"Death spiral circuit breaker (revert on error increase)","description":"UNIQUE INSIGHT from G3-Pro, validated by GPT-5.2\n\nA common failure mode: the 72B model tries to 'fix' a compile error by refactoring the entire file, introducing 10 new errors. The current loop keeps trying to fix the growing pile of errors, wasting all iterations.\n\nImplement a circuit breaker in the orchestrator loop:\n1. After each Verifier run, compare error count to previous iteration\n2. If error_count(N) \u003e error_count(N-1) * 1.5 (50% increase), trigger circuit breaker\n3. Circuit breaker action: git revert to previous state, then retry with STRICT constraints:\n   - 'Change ONLY the line reported in the error'\n   - 'Do not refactor surrounding code'\n   - max_patch_loc reduced to 20\n4. If the strict retry also fails, escalate immediately (don't burn remaining budget)\n\nAlso enforce: if diff size exceeds max_patch_loc, reject the patch BEFORE running verifier. Don't waste compile time on oversized patches.\n\nGPT-5.2 adds: 'treat green-to-red regressions as negative progress' and 'use edit distance between error sets rather than just category comparison'.\n\nFiles: crates/swarm-agents/src/main.rs, coordination/src/escalation/state.rs","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:35:26.803399-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:35:26.803399-06:00"}
{"id":"beefcake-swarm-cp8","title":"Add cd and sed to exec_tool command allowlist","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-15T18:45:30.732277-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T19:47:28.3815-06:00","closed_at":"2026-02-15T19:47:28.3815-06:00","close_reason":"Fixed in dogfood round 2"}
{"id":"beefcake-swarm-d15","title":"Add OpenTelemetry tracing spans to orchestrator loop","description":"No observability in the orchestrator loop means blind debugging when iterations fail. Add OpenTelemetry spans around: issue selection, worktree creation, context packing, implementer call, patch application, verifier pipeline, validator call, merge. Track metrics: pass rate per tier, iterations to green, tokens consumed, wallclock per phase, escalation frequency. Use tracing-opentelemetry + opentelemetry-otlp crates. Export to stdout/file initially, OTLP endpoint later. Files: crates/swarm-agents/Cargo.toml, crates/swarm-agents/src/main.rs. Claude Opus 4.5 rated this P0.","status":"open","priority":0,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:48:53.017261-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:48:53.017261-06:00"}
{"id":"beefcake-swarm-d1a","title":"Evaluate Kalosm for Rust-native LLM orchestration","description":"Kalosm is a Rust-native AI orchestration framework. G3-Pro flagged it as worth investigating. Evaluate: (1) Does it offer capabilities beyond rig-core? (2) Local embedding model support for retrieval/RAG, (3) Tool-calling patterns, (4) Integration with llama.cpp. Compare with current rig-core setup. If it offers embeddings without Python, could replace CocoIndex dependency. Low priority — only pursue if rig-core proves limiting.","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:58.112912-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:58.112912-06:00"}
{"id":"beefcake-swarm-djc","title":"Migrate coordination crate from anyhow to thiserror","description":"The coordination crate is a library but uses anyhow::Result in some internal functions. Library crates should provide structured error types via thiserror so consumers can match on specific error variants, while anyhow is appropriate for application binaries (swarm-agents).\n\nFix: Define error enums with thiserror for each coordination module:\n- VerifierError (GateTimeout, CommandFailed, ParseError)\n- EscalationError (BudgetExhausted, InvalidTransition)\n- ContextPackerError (FileWalkFailed, TokenBudgetExceeded)\n- WorkPacketError (GitCommandFailed, SymbolExtractionFailed)\n\nKeep anyhow in swarm-agents binary for ergonomic error propagation.\n\nFiles: coordination/src/verifier/, coordination/src/escalation/, coordination/src/context_packer/, coordination/src/work_packet/\nFound by: G3-Pro deep review","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:26.440288-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:14:26.440288-06:00"}
{"id":"beefcake-swarm-dxt","title":"Atomic issue claiming to prevent race conditions","description":"The orchestrator has a check-then-act race: it calls list_open(), sorts by priority, picks the first issue, then calls update_status(in_progress). If two orchestrator instances run simultaneously (or two swarm loops), both will pick the same P1 issue, leading to duplicate work, git conflicts, and wasted inference credits.\n\nFix: Either implement an atomic claim_next_available() in BeadsBridge that combines list+claim in a single operation, or handle the case where update_status returns 'already claimed' and retry with the next issue. Consider adding a locking mechanism (file lock or beads-level CAS).\n\nFiles: crates/swarm-agents/src/beads_bridge.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:11:38.686478-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:11:38.686478-06:00"}
{"id":"beefcake-swarm-e3c","title":"Create monitoring dashboard for swarm operations","description":"Extend infrastructure/gpu-dashboard.py or create new tool to monitor: active agent tasks, inference endpoint health, beads issue throughput, escalation frequency. Could use beads_viewer (bv) robot mode for metrics.","status":"open","priority":3,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:38.541669-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:38.541669-06:00"}
{"id":"beefcake-swarm-e74","title":"Validate write_file paths to prevent stray files in worktree root","description":"Job 1606: model wrote lib.rs to worktree root instead of crates/swarm-agents/src/lib.rs. Add validation in WriteFileTool::call() to warn or reject writes to the worktree root directory (paths without any '/' separator), since legitimate writes are always to subdirectories.","status":"closed","priority":3,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-15T22:32:49.146464-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T22:38:22.444213-06:00","closed_at":"2026-02-15T22:38:22.444213-06:00","close_reason":"Added tracing::warn for write_file paths with no directory component"}
{"id":"beefcake-swarm-ez1","title":"Add tracing spans with #[instrument] to key methods","description":"Current logging is flat — all log lines are at the same level with no hierarchical grouping. It's hard to correlate which logs belong to which iteration, issue, or git operation when reviewing output.\n\nFix: Add #[tracing::instrument] attributes to key methods:\n- ContextPacker::pack_initial(bead_id, objective) — span includes bead_id\n- ContextPacker::pack_retry(bead_id, ...) — span includes bead_id and iteration\n- WorktreeBridge::create(issue_id) — span includes issue_id\n- WorktreeBridge::merge_and_remove(issue_id) — span includes issue_id\n- Verifier::run_pipeline() — span includes working_dir\n- Implementer::implement() — span includes model name\n\nAdd skip directives for large parameters (e.g. task_description content). This creates nested spans visible in structured log output (JSON) or tracing-subscriber's hierarchical formatter.\n\nFiles: coordination/src/context_packer/packer.rs, crates/swarm-agents/src/worktree_bridge.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:29.189788-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:14:29.189788-06:00"}
{"id":"beefcake-swarm-fxo","title":"Integrate DSRs as prompt policy optimization layer","description":"STRONG CONSENSUS (3/4 models, builds on existing beefcake-swarm-481)\n\nUse DSRs (dspy-rs) not as a per-agent prompt hack, but as a POLICY LAYER that optimizes how the swarm communicates with models.\n\nArchitecture:\n1. DSRs sits BETWEEN ContextPacker and LLM call\n2. Define typed Signatures for each agent role (Implementer, Validator, Planner, Adversary)\n3. Define reward signals from deterministic gates:\n   - +1.0 for all-green first try\n   - +0.2 for green in \u003c3 iterations with small patch\n   - -1.0 for regression introduced\n   - -0.5 for timeout/hang\n   - -0.3 for validator risk flag\n4. Treat prompt as: Template (stable) + Policy Parameters (learned)\n   - Learned params: plan-vs-code ratio, tests-first toggle, max patch aggressiveness, context ordering\n5. A/B test prompt variants using the existing ensemble infrastructure\n6. Store prompt versions with success rate metrics in RocksDB\n\nOffline optimization loop: replay past issues through DSRs optimizer to find better prompt configurations. Deploy optimized prompts, measure on new issues, iterate.\n\nDepends on: agent traits extraction (beefcake-swarm-7ny), DSRs evaluation (beefcake-swarm-481)\nFiles: crates/swarm-agents/src/ (prompt policy module), coordination/src/ensemble/","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:35:15.423884-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:35:15.423884-06:00","dependencies":[{"issue_id":"beefcake-swarm-fxo","depends_on_id":"beefcake-swarm-7ny","type":"blocks","created_at":"2026-02-11T16:35:50.515608-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-fxo","depends_on_id":"beefcake-swarm-481","type":"blocks","created_at":"2026-02-11T16:35:50.633539-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-gbh","title":"Add unit test for WorktreeBridge::sanitize_id edge cases","description":"Add tests for: unicode input, very long IDs (\u003e255 chars), IDs that are all dots, IDs with only special characters. File: crates/swarm-agents/src/worktree_bridge.rs","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-15T13:31:39.863288-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T13:31:46.713952-06:00"}
{"id":"beefcake-swarm-i42","title":"Add explicit planning step for complex issues (RA.Aid pattern)","description":"Currently the orchestrator goes straight from issue to implementation (main.rs:229). For complex issues, a planning phase reduces iteration count. Steal RA.Aid Research-Plan-Act pattern: (1) Add complexity_score to issue classification based on error categories and file count, (2) For complex issues (\u003e3 files, trait/lifetime errors, escalated tasks): inject planning step where 72B model generates a concrete plan, (3) 14B Implementer executes the plan, (4) Store plan as artifact in RocksDB, include in WorkPacket on retry. Add plan field to WorkPacket struct. Low-medium effort, medium-high payoff. All 4 models recommended this pattern.","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:23.350566-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:23.350566-06:00"}
{"id":"beefcake-swarm-j4l","title":"Add Planner agent with structured change contracts","description":"UNANIMOUS CONSENSUS (4/4 models: G3-Pro, GPT-5.2-Codex, GPT-5.2, Opus 4.5)\n\nThe current loop goes straight to code generation without formalizing intent. Add a Planner agent (can use the fast 14B model) that runs BEFORE the Implementer and produces a structured 'change contract':\n\n- Acceptance criteria (what must be true when done)\n- Invariants / behavioral promises (what must NOT change)\n- Files expected to change (scoping)\n- Risk classification (API break risk, concurrency risk, security risk)\n- Test plan: which tests to add/modify, what failure proves correctness\n\nThe contract becomes:\n1. A constraint for the Implementer (bounded search space)\n2. A checklist for the Validator (check against plan, not just 'does it look right')\n3. An audit trail (what was intended vs what happened)\n4. Partial rollback points (if step 3/5 fails, retry from step 3)\n\nImplementation: Add a PlannerAgent trait and struct in swarm-agents, produce a ChangeContract type in coordination/. The contract feeds into WorkPacket.constraints and WorkPacket.decisions. The Validator receives the contract alongside the diff.\n\nG3-Pro calls this 'Legislator-Executor'. GPT-5.2-Codex calls it '3-stage contract + selection'. Both identify it as the highest-impact architectural change.\n\nFiles: crates/swarm-agents/src/ (new planner module), coordination/src/ (new contract types), crates/swarm-agents/src/main.rs","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:20.665759-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:20.665759-06:00"}
{"id":"beefcake-swarm-jse","title":"Sandbox verification pipeline (treat agent code as untrusted)","description":"STRONG CONSENSUS (3/4 models)\n\nAgent-generated code can do ANYTHING — including filesystem deletion, network exfiltration, command execution in build.rs, or infinite loops in tests. On an HPC cluster with shared NFS, this is catastrophic.\n\nRun all verification gates in a sandbox:\n1. Use bubblewrap (bwrap) or firejail on Linux to sandbox cargo commands\n2. Restrictions: no network access, restricted filesystem (only worktree + target dirs), CPU/memory/time caps\n3. Per-test timeouts (not just per-gate — the existing gate_timeout_secs config isn't enforced)\n4. Kill process trees on timeout (not just the parent process)\n5. Detect 'test passed but took 10x longer than baseline' as suspicious\n\nAlso add: pre-check for dangerous patterns before running (new unsafe blocks, std::process::Command, std::fs::remove_dir_all, network calls).\n\nNote: VerifierConfig already has gate_timeout_secs (line 28 in pipeline.rs) but it's not enforced — gates use blocking Command::output() with no timeout.\n\nFiles: coordination/src/verifier/pipeline.rs, coordination/src/verifier/ (new sandbox module)","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:52.565254-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:52.565254-06:00"}
{"id":"beefcake-swarm-lzb","title":"Implement escalation ladder: Implementer → Integrator → Cloud → Human","description":"Wire the coordination crate's escalation engine into swarm-agents. Implementer tries 14B first, escalates to 72B Integrator, then Cloud Council (external API), finally Human. Use existing EscalationEngine and SwarmTier types from coordination/src/escalation/.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:26.789193-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:26.789193-06:00","dependencies":[{"issue_id":"beefcake-swarm-lzb","depends_on_id":"beefcake-swarm-tup","type":"blocks","created_at":"2026-02-11T15:32:56.607478-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-mu1","title":"Implement graceful shutdown with worktree cleanup","description":"The orchestrator has no signal handler. If killed via SIGINT (Ctrl+C) or SIGTERM (deployment, systemd stop) while a worktree is active, the worktree remains on disk and registered in git, causing disk space leaks and 'already checked out' errors on restart.\n\nFix: Use tokio::signal::ctrl_c() and tokio::signal::unix::signal(SignalKind::terminate()) to catch shutdown signals. When received:\n1. Set a shutdown flag (AtomicBool or tokio::sync::watch)\n2. Check the flag at the top of each loop iteration\n3. On shutdown: log the in-progress issue ID, call worktree_bridge.remove_worktree(), update beads status back to 'open', then exit cleanly\n\nConsider wrapping the active worktree path in an Arc\u003cMutex\u003cOption\u003cString\u003e\u003e\u003e so the signal handler knows what to clean up. Requires the remove_worktree() method from the worktree cleanup issue.\n\nFiles: crates/swarm-agents/src/main.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:12:25.932582-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:12:25.932582-06:00","dependencies":[{"issue_id":"beefcake-swarm-mu1","depends_on_id":"beefcake-swarm-rb2","type":"blocks","created_at":"2026-02-11T16:15:38.683284-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-nu2","title":"Adapt flywheel for SLURM/NFS patterns","description":"Review forked agentic_coding_flywheel_setup submodule. Extract useful prompts, task decomposition strategies, tool configurations. Discard Docker/cloud assumptions, adapt for SLURM scheduler and NFS shared storage.","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:23.624475-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:23.624475-06:00"}
{"id":"beefcake-swarm-ost","title":"Priority-scored context trimming in ContextPacker","description":"ContextPacker::trim_to_budget() drops file_contexts from the end via pop(). This happens to work correctly because WorkPacketGenerator inserts error contexts first (highest priority) and file header contexts last (lowest priority). But this ordering guarantee is implicit and fragile — any change to insertion order in a different module breaks trimming priority silently.\n\nFix: Add a priority field to FileContext (or use an enum: Error \u003e Modified \u003e Header \u003e Reference). Before trimming, sort file_contexts by priority (lowest priority last). Then pop() is guaranteed to drop the least important context first regardless of insertion order.\n\nAlternative: Instead of modifying FileContext, partition into priority buckets and trim from the lowest bucket first.\n\nFiles: coordination/src/context_packer/packer.rs, coordination/src/work_packet/types.rs\nFound by: G3-Pro deep review","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:13:32.774693-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:13:32.774693-06:00"}
{"id":"beefcake-swarm-ozx","title":"Evaluate TensorZero as Rust inference gateway (spike)","description":"TensorZero is a Rust-native inference gateway with structured output enforcement, model routing, and observability. G3-Pro said INTEGRATE IMMEDIATELY — it could replace SLURM endpoint discovery and manual health checks. Evaluate: (1) Does it support llama.cpp OpenAI-compatible API? (2) Does it handle health checks, failover, backoff? (3) Can it enforce JSON schema outputs? (4) Does it add observability we lack? If compatible, prototype replacing coordination/src/slurm/ endpoint management. If not, steal the structured output + routing patterns only. Compare with current rig-core setup. Time-box: 2-3 days for spike.","status":"open","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:05.750864-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:05.750864-06:00"}
{"id":"beefcake-swarm-pbk","title":"Structure validator feedback as actionable deltas (TextGrad pattern)","description":"Currently validator feedback (main.rs:287 result.feedback) goes into logs but does not systematically feed back into the next iteration prompt. Steal the TextGrad pattern: structure validator critiques as specific code locations + suggested fixes, not just prose. Modify pack_retry to include validator feedback as a first-class FailureSignal with category ValidatorRejection. Add structured fields: file, line_range, issue_type, suggested_fix. This creates tighter feedback loops and fewer iterations. Files: coordination/src/verifier/report.rs (add ValidatorFeedback type), coordination/src/context_packer/packer.rs (include in pack_retry), crates/swarm-agents/src/validator.rs (structured output), crates/swarm-agents/src/main.rs (wire feedback). Low effort, medium payoff.","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:09.801827-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:09.801827-06:00"}
{"id":"beefcake-swarm-pid","title":"Expand verifier with Miri, proptest, cargo-deny gates","description":"STRONG CONSENSUS (3/4 models)\n\nThe verifier only runs fmt/clippy/check/test. Add risk-based verification gates:\n\nHigh-value additions:\n1. cargo miri test — UB detection for unsafe-heavy code (mandatory for any unsafe blocks)\n2. cargo deny check — security advisories, banned crates, license compliance\n3. cargo semver-checks — prevent accidental breaking API changes\n4. cargo udeps — detect unused dependencies introduced by agents\n5. nextest — faster test execution with better isolation + flaky detection\n6. Feature matrix: test with --no-default-features and key feature flags\n7. Doc tests / rustdoc lints for public API changes\n\nMake gates ADAPTIVE: select which extra gates to run based on diff risk profile:\n- Touches unsafe → require Miri\n- Changes Cargo.toml → require deny + udeps\n- Changes public API → require semver-checks\n- All changes → nextest with flaky detection\n\nFiles: coordination/src/verifier/pipeline.rs, coordination/src/verifier/ (new gate modules)","status":"open","priority":2,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:35:02.17173-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:35:02.17173-06:00"}
{"id":"beefcake-swarm-r93","title":"Wire beads_bridge to orchestrator main loop","description":"Connect beads_bridge.rs to the main orchestrator loop. List open issues, pick highest priority, update status to in_progress, close on completion. Use br CLI --json output for machine-readable parsing.","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:20.297648-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:02:59.372575-06:00","closed_at":"2026-02-11T16:02:59.372575-06:00","close_reason":"Closed"}
{"id":"beefcake-swarm-rb2","title":"Add worktree cleanup method for merge failure recovery","description":"WorktreeBridge::merge_and_remove() bails on merge conflict (line 140), leaving the worktree on disk and the swarm/\u003cid\u003e branch in the repo. The orchestrator exits with an error (line 306), creating zombie worktrees that block future attempts on the same issue (create() checks for existing worktree on line 74).\n\nFix: Add a remove_worktree(issue_id) method that force-removes the worktree and force-deletes the branch (git worktree remove --force + git branch -D). Use this in the orchestrator failure path so cleanup happens even when merge fails.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:11:30.568197-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:11:30.568197-06:00"}
{"id":"beefcake-swarm-rib","title":"Fix git object permissions in worktree commit step","description":"Job 1606: orchestrator exits with 'insufficient permission for adding an object to repository database .git/objects'. Worktree links to main repo .git/objects which gets owned by root after git pull as root. Fix: (1) SLURM script should chown .git/objects after pull, (2) orchestrator should pull as brian not root, (3) consider git config safe.directory.","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-15T22:32:48.656554-06:00","created_by":"TheFermiSea","updated_at":"2026-02-15T22:38:22.228378-06:00","closed_at":"2026-02-15T22:38:22.228378-06:00","close_reason":"git add now handles permissions gracefully + SLURM script chowns .git before run"}
{"id":"beefcake-swarm-rzr","title":"Integrate Gastown for workspace isolation","description":"Wire up gastown CLI calls from swarm-agents orchestrator. Create worktrees per agent task on NFS (/cluster/shared/wt/\u003cissue_id\u003e). Prevent file conflicts for parallel agents. Commands: gastown create, gastown merge.","status":"closed","priority":1,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:16.884368-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:02:59.350146-06:00","closed_at":"2026-02-11T16:02:59.350146-06:00","close_reason":"Closed"}
{"id":"beefcake-swarm-s2f","title":"Manager prompt must instruct stop-on-success after verifier passes","description":"Job 1608: manager (Opus 4.6-thinking) wrote tests at depth 5, tests passed at depth 9, but continued spawning workers for 45+ more minutes. The manager prompt needs explicit instructions: once cargo test passes and verifier is green, IMMEDIATELY return the result summary. Do not continue iterating or spawning additional workers.","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-16T08:53:41.14823-06:00","created_by":"TheFermiSea","updated_at":"2026-02-16T08:55:24.961831-06:00","closed_at":"2026-02-16T08:55:24.961831-06:00","close_reason":"Added CRITICAL: Stop When Done section to both cloud and local manager prompts"}
{"id":"beefcake-swarm-tup","title":"Implement 2-agent loop MVP (orchestrator → implementer → verifier → validator)","description":"Core loop: orchestrator picks beads issue, creates gastown worktree, runs implementer (72B), deterministic verifier (fmt/clippy/test), blind validator (14B). If pass: merge+close. If fail: update notes, retry.","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:10.010034-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:10.010034-06:00","dependencies":[{"issue_id":"beefcake-swarm-tup","depends_on_id":"beefcake-swarm-r93","type":"blocks","created_at":"2026-02-11T15:32:56.348232-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-tup","depends_on_id":"beefcake-swarm-rzr","type":"blocks","created_at":"2026-02-11T15:32:56.434885-06:00","created_by":"TheFermiSea"},{"issue_id":"beefcake-swarm-tup","depends_on_id":"beefcake-swarm-8nm","type":"blocks","created_at":"2026-02-11T15:32:56.520579-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-ty0","title":"Use structured error fields in tracing macros","description":"Error logging uses string interpolation: error!('Failed to create worktree: {e}'). This embeds the error message in the format string, making it impossible for log aggregators (Loki, Datadog) to parse the error type separately from the message.\n\nFix: Use structured fields in tracing macros throughout main.rs and worktree_bridge.rs:\n- error!(error = %e, issue_id = %id, 'Failed to create worktree') — Display format\n- error!(error = ?e, issue_id = %id, 'Failed to create worktree') — Debug format (more detail)\n- warn!(iteration, feedback = %result.feedback, 'Validator FAILED')\n\nThis allows querying logs by error type, issue_id, or iteration number independently.\n\nFiles: crates/swarm-agents/src/main.rs, crates/swarm-agents/src/worktree_bridge.rs\nFound by: G3-Pro deep review","status":"open","priority":3,"issue_type":"task","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:14:31.658991-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:14:31.658991-06:00","dependencies":[{"issue_id":"beefcake-swarm-ty0","depends_on_id":"beefcake-swarm-ez1","type":"blocks","created_at":"2026-02-11T16:15:39.033017-06:00","created_by":"TheFermiSea"}]}
{"id":"beefcake-swarm-vl2","title":"WorktreeBridge::create should clean up stale branches before creating","description":"Job 1607 failed in 16 seconds because branch 'swarm/bd-h1n' already existed from a previous failed run (job 1606). WorktreeBridge::create calls 'git worktree add -b swarm/\u003cid\u003e' which fails if the branch exists. Fix: before creating, check if the branch exists and delete it (git branch -D), and also prune stale worktrees (git worktree prune). File: crates/swarm-agents/src/worktree_bridge.rs, create() method.","status":"open","priority":2,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-16T07:53:55.449831-06:00","created_by":"TheFermiSea","updated_at":"2026-02-16T07:53:55.449831-06:00"}
{"id":"beefcake-swarm-wpe","title":"Implement DSR prompt optimization (deferred)","description":"Dynamic System Reasoning for optimizing agent prompts based on success/failure feedback. Deferred from initial implementation. Track in backlog for future iteration.","status":"open","priority":4,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T15:32:41.835271-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T15:32:41.835271-06:00"}
{"id":"beefcake-swarm-xv9","title":"Handle detached HEAD in WorkPacketGenerator","description":"WorkPacketGenerator::git_branch() uses 'git rev-parse --abbrev-ref HEAD' which returns the literal string 'HEAD' when in detached HEAD state (common in CI, after checkout of a specific commit, or in freshly created worktrees before the first commit).\n\nThis means the WorkPacket.branch field becomes 'HEAD' which is misleading in the prompt sent to the LLM and may confuse the agent about which branch it's working on.\n\nFix: If git_branch() returns 'HEAD', fall back to:\n1. Check for CI env vars (CI_COMMIT_REF_NAME, GITHUB_HEAD_REF, BRANCH_NAME)\n2. Try 'git name-rev --name-only HEAD'\n3. Use 'detached@\u003cshort-sha\u003e' as a last resort\n\nFiles: coordination/src/work_packet/generator.rs\nFound by: G3-Pro deep review","status":"open","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:12:23.225461-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:12:23.225461-06:00"}
{"id":"beefcake-swarm-y2d","title":"Sanitize issue IDs to prevent path traversal","description":"WorktreeBridge::worktree_path() does base_dir.join(issue_id) with no validation. If issue_id contains path separators (e.g. ../../etc or ../../../tmp/evil), it could write worktrees outside the base directory.\n\nFix: Add validate_issue_id() that rejects any ID not matching [a-zA-Z0-9_-]. Call it in create(), merge_and_remove(), and worktree_path(). Beads IDs are alphanumeric-with-hyphens so this won't break real usage.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs\nFound by: G3-Pro deep review","status":"closed","priority":1,"issue_type":"bug","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:11:34.634992-06:00","created_by":"TheFermiSea","updated_at":"2026-02-12T11:14:25.71102-06:00","closed_at":"2026-02-12T11:14:25.71102-06:00","close_reason":"Fixed in PR #1 review commit a26ba46"}
{"id":"beefcake-swarm-yhx","title":"Build local SWE-bench-style evaluation harness","description":"Create a corpus of synthetic bead issues (or use real resolved ones) and measure swarm end-to-end: pass rate, iterations to green, tokens consumed, wallclock per phase, escalation frequency. This is how we decide between better retrieval vs bigger model vs more planning. Required before any prompt optimization (AdalFlow/TextGrad). Structure: (1) Issue corpus in .beads/ or test fixtures, (2) Runner that executes swarm loop per issue, (3) Metrics collector + summary report. GPT-5.2 specifically recommended this. Revisit AdalFlow/TextGrad after 50+ completed issues with logged data.","status":"open","priority":2,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:49:54.486409-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:49:54.486409-06:00"}
{"id":"beefcake-swarm-zg6","title":"Multi-proposal speculative execution for hard tasks","description":"UNANIMOUS CONSENSUS (4/4 models)\n\nFor hard/risky tasks, a single implementer often gets stuck in local minima. All 4 models propose spawning multiple implementations in parallel.\n\nDesign:\n1. Classify issue difficulty (simple vs hard) based on: error category history, file count, risk classification from Planner\n2. For hard tasks, spawn 2-3 Implementers in parallel with different strategies:\n   - Different temperature settings (0.2 conservative, 0.8 creative)\n   - Different system prompts ('minimal fix' vs 'refactor for clarity')\n   - Different model tiers (14B fast attempt + 72B deep attempt)\n3. Each produces a patch in its own worktree\n4. Run Verifier on all patches in parallel (CPU-bound, manageable)\n5. If multiple pass, Validator ranks and picks the best (most concise, least risk)\n6. If none pass, combine insights from all attempts for the retry context\n\nThis trades compute for success rate. G3-Pro: 'Parallel Speculative Decoding'. GPT-5.2: 'Selection via competition — turn merges into a market'.\n\nFiles: crates/swarm-agents/src/main.rs, crates/swarm-agents/src/worktree_bridge.rs (multi-worktree support)","status":"open","priority":1,"issue_type":"feature","owner":"squires.b@gmail.com","created_at":"2026-02-11T16:34:36.824715-06:00","created_by":"TheFermiSea","updated_at":"2026-02-11T16:34:36.824715-06:00","dependencies":[{"issue_id":"beefcake-swarm-zg6","depends_on_id":"beefcake-swarm-j4l","type":"blocks","created_at":"2026-02-11T16:35:50.86899-06:00","created_by":"TheFermiSea"}]}
