{"id": "beefcake-swarm-0l5", "title": "Make WorkPacket constraints configurable", "description": "WorkPacketGenerator has hardcoded default constraints (line 46: 'No new dependencies without explicit approval', 'Don't break existing public API'). These are baked into the binary and cannot be overridden per-task.\n\nThis is problematic for tasks that intentionally require adding dependencies, changing public API, or have different LOC limits.\n\nFix: Move default constraints into SwarmConfig (or a new PackerConfig). Allow per-issue constraint overrides via beads issue labels or metadata. The ContextPacker should accept optional extra constraints that merge with (or replace) the defaults.\n\nConsider: constraints could come from beads issue fields, a .beefcake.toml project config, or CLI flags.\n\nFiles: coordination/src/work_packet/generator.rs, crates/swarm-agents/src/config.rs, coordination/src/context_packer/packer.rs\nFound by: G3-Pro deep review", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-10m", "title": "Upgrade ContextPacker to span-aware error-driven retrieval (Potpie pattern)", "description": "All 4 models identified that current context packing (first 30 lines per file) misses critical information. Steal Potpie pattern: on verifier failures, use rustc error spans to retrieve relevant code regions. Specifically: (1) Parse rustc JSON error spans (file + line range), (2) Pack \u00b180 lines around each error span, (3) Include trait definitions and impl blocks referenced in trait bound errors, (4) Include callers/callees of symbols in error messages. Depends on tree-sitter integration for symbol graph. Files: coordination/src/context_packer/packer.rs (pack_retry enhancement), coordination/src/verifier/report.rs (span extraction from rustc JSON). Depends on tree-sitter issue.", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:49:14Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:59Z", "dependencies": [{"issue_id": "beefcake-swarm-10m", "depends_on_id": "beefcake-swarm-5bk", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-19l", "title": "Verifier scope should derive package set from git-changed files", "notes": "R3 audit (2026-02-17): add to Round 3 active set. Orchestrator currently hard-codes packages=[\"swarm-agents\"], which can miss breakage in other changed crates and produce false greens.", "status": "open", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:34Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:25:01Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-1nt", "title": "Add ast-grep scan as warning gate in verifier pipeline", "description": "We now have 7 ast-grep rules in rules/. Add an optional 'sg scan' step to the verifier pipeline that runs after clippy but before tests. Errors from sg scan should be reported as warnings (non-blocking) in the verifier report, giving the LLM feedback about quality issues without failing the build. Files: coordination/src/verifier/pipeline.rs, coordination/src/verifier/report.rs", "status": "open", "priority": 2, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:09:48Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:09:48Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-1qs", "title": "Wire NotebookLM into orchestrator loop", "description": "4 integration points: pre-task enrichment, pre-escalation check, post-success capture, render knowledge fields in format_task_prompt", "status": "open", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:40:05Z", "created_by": "claude-code", "updated_at": "2026-02-17T09:40:05Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-2jk", "title": "Extract shared SourceFileProvider to deduplicate file reading", "description": "Both ContextPacker::build_file_contexts() and WorkPacketGenerator::extract_symbols_from_files() independently read files from disk and iterate lines. This duplicates I/O when both are called in sequence (e.g. pack_initial calls the generator then builds its own contexts).\n\nFix: Extract a SourceFileProvider struct that:\n1. Caches file content in a HashMap<PathBuf, String> on first read\n2. Provides methods: get_content(path) -> &str, get_header(path, lines: usize) -> &str, get_lines_around(path, line, context) -> &str\n3. Is shared between WorkPacketGenerator and ContextPacker via reference\n\nThis eliminates redundant disk reads and provides a single point for file access patterns.\n\nFiles: coordination/src/work_packet/generator.rs, coordination/src/context_packer/packer.rs\nFound by: G3-Pro deep review", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:24Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-393", "title": "Enforce gate_timeout_secs in verifier pipeline (currently unused)", "notes": "SWARM-READY: Wire existing gate_timeout_secs config into verifier pipeline. Small, well-defined change.", "status": "open", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:34Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T20:54:27Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3hz", "title": "Tag swarm-ready issues with complexity estimate for better task selection", "description": "Dogfooding showed additive tasks (add tests, add methods) succeed while modification tasks fail. The orchestrator should prefer simpler issues. Add a 'swarm_complexity' label to beads issues: 'additive' (new code only), 'modify_small' (<50 lines changed), 'modify_large' (>50 lines). The issue picker should sort by: priority first, then swarm_complexity (additive first). Files: crates/swarm-agents/src/orchestrator.rs, crates/swarm-agents/src/beads_bridge.rs", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:09:48Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:09:48Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.4.3", "title": "KG-aware Router task classification", "description": "Enhance task_classifier.rs with KG structural signals: fan-in (callers count), coupling (importers count), trait impl complexity. Add kg_complexity_boost() method. Additive \u2014 keyword heuristic remains as fallback.", "notes": "File: coordination/src/router/task_classifier.rs. Pure deterministic logic. KG signals are optional boost factors, never replace existing classification.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.4.3", "depends_on_id": "beefcake-swarm-3is.4", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.4.3", "depends_on_id": "beefcake-swarm-3is.4.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.4.4", "title": "KG-informed escalation signals", "description": "New EscalationReason variants: HighFanIn, MultipleImplementors. New thresholds in EscalationConfig: fan_in_threshold (default 5), implementor_threshold (default 3). Pure deterministic logic, no LLM calls.", "notes": "File: coordination/src/escalation/engine.rs. Additive new variants \u2014 existing escalation reasons unchanged.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.4.4", "depends_on_id": "beefcake-swarm-3is.4", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.4.4", "depends_on_id": "beefcake-swarm-3is.4.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.5.2", "title": "Implement Error Pattern KB in SurrealDB", "description": "Build Error Pattern KB as SurrealDB graph nodes. Schema: fix_pattern node (error_signature, original_code, fix_diff, strategy, model_tier, iterations, success_count). Edges: fix_pattern->fixed->function. Query: top-3 matching past fixes by error category + file + symbols, ranked by success rate + recency. Relates to beefcake-swarm-3r9 (may supersede or complement depending on 1.5 investigation).", "notes": "Storage decision depends on 1.5 investigation. If SurrealDB wins, this supersedes 3r9. If not, this becomes a SurrealDB mirror of the RocksDB store for cross-issue queries.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.5.2", "depends_on_id": "beefcake-swarm-3is.1.4", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.2", "depends_on_id": "beefcake-swarm-3is.1.5", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.2", "depends_on_id": "beefcake-swarm-3is.5", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.2", "depends_on_id": "beefcake-swarm-3is.5.1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 3, "dependent_count": 2, "comment_count": 0}
{"id": "beefcake-swarm-3is.5.3", "title": "Wire learning feedback into orchestrator loop", "description": "After successful verification, capture: diff, error categories fixed, model tier, iteration count. Store as fix pattern via KgClient. Non-blocking, fire-and-forget. Also track per-model success rates by error category for routing optimization.", "notes": "File: crates/swarm-agents/src/main.rs. Must be non-blocking \u2014 never slow down the main loop. Use tokio::spawn for async storage.", "status": "open", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:22Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.5.3", "depends_on_id": "beefcake-swarm-3is.4.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.3", "depends_on_id": "beefcake-swarm-3is.5", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.3", "depends_on_id": "beefcake-swarm-3is.5.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 2, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.5.4", "title": "Tiered archival memory (Letta/MemGPT pattern)", "description": "Per-issue memory in SurrealDB: working memory = WorkPacket, archival = prior diffs/reports/decisions per bead_id. On pack_retry, recall failed approaches. Promotion rules: successful fixes \u2192 shared KB, failures \u2192 scoped to bead_id. Relates to beefcake-swarm-b30 (may supersede or complement depending on 1.5 investigation).", "notes": "Storage decision depends on 1.5 investigation. Key pattern: working memory (current context) vs archival memory (past attempts). Prevents retrying the same failed approach.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:22Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.5.4", "depends_on_id": "beefcake-swarm-3is.1.5", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.4", "depends_on_id": "beefcake-swarm-3is.5", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.5.4", "depends_on_id": "beefcake-swarm-3is.5.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 2, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.6.1", "title": "Design benchmark framework (pgvector vs SurrealDB)", "description": "Design benchmark comparing pgvector and SurrealDB: vector search latency (p50/p95/p99), hybrid search quality (graph+vector vs vector-only), write throughput, query flexibility. Define 10 benchmark queries from real agent workloads.", "notes": "PAL chat with gpt-5.2 (Manager role \u2014 task decomposition). Queries should cover: similar error lookup, symbol dependency traversal, hybrid doc+code search, write-heavy pattern storage.", "status": "open", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:22Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "labels": ["benchmark", "delegate:gpt-5.2", "design", "epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.6.1", "depends_on_id": "beefcake-swarm-3is.2.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.6.1", "depends_on_id": "beefcake-swarm-3is.3.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.6.1", "depends_on_id": "beefcake-swarm-3is.6", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 2, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.6.2", "title": "Run benchmarks and produce comparison report", "description": "Implement benchmark harness in Rust. Run against both stores with identical data. Produce comparison report with latency histograms, quality scores, operational assessment.", "notes": "Use criterion.rs for microbenchmarks. Run on ai-proxy LXC where both stores live. Report format: markdown with embedded charts (plotters crate).", "status": "open", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:22Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:46Z", "labels": ["benchmark", "delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.6.2", "depends_on_id": "beefcake-swarm-3is.6", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.6.2", "depends_on_id": "beefcake-swarm-3is.6.1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.6.3", "title": "Migration decision (pgvector \u2192 SurrealDB)", "description": "Based on benchmark results + 1.5 investigation, make go/no-go on full pgvector\u2192SurrealDB migration. Frontier consensus + human sign-off required.", "notes": "PAL consensus. This is a blocking decision for long-term architecture. Three outcomes: (1) full migration, (2) keep both with role separation, (3) stay with pgvector.", "status": "open", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:46Z", "labels": ["delegate:pal-consensus", "design", "epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.6.3", "depends_on_id": "beefcake-swarm-3is.1.5", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.6.3", "depends_on_id": "beefcake-swarm-3is.6", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.6.3", "depends_on_id": "beefcake-swarm-3is.6.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 2, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.7.1", "title": "Add search_knowledge_graph MCP tool", "description": "New MCP tool in coordination/src/main.rs. Accepts query string, returns semantically similar code + related doc chunks + graph-traversed dependencies. Hybrid search: embed query via nomic on vasp-02:8080, then vector + graph traversal. Gate behind --knowledge-graph flag.", "notes": "Follow ask_rust_architect pattern at coordination/src/main.rs line 563. Return format: ranked list of (code_symbol, relevance_score, related_docs, dependency_chain).", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:46Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.7.1", "depends_on_id": "beefcake-swarm-3is.1.4", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.7.1", "depends_on_id": "beefcake-swarm-3is.2.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.7.1", "depends_on_id": "beefcake-swarm-3is.7", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 2, "dependent_count": 2, "comment_count": 0}
{"id": "beefcake-swarm-3is.7.2", "title": "Add query_code_graph MCP tool (structural queries)", "description": "Pure structural queries: 'what calls X?', 'what implements Y?', 'what imports Z?'. Deterministic, no embeddings needed. Useful for Planner agent (beefcake-swarm-j4l).", "notes": "SurrealQL graph traversal only. No vector search. Fast path for structural questions. Expose as separate tool for clarity.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:46Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.7.2", "depends_on_id": "beefcake-swarm-3is.7", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.7.2", "depends_on_id": "beefcake-swarm-3is.7.1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.7.3", "title": "Add store_knowledge MCP tool (agent write-back)", "description": "Agents write knowledge back to KG: design decisions, architecture notes, 'this function is tricky because...'. Schema: agent_note with confidence field (decays over time). Lower weight than code-derived facts.", "notes": "Confidence decay: new notes start at 1.0, decay by 0.1 per week. Agent-contributed knowledge is always lower-ranked than code-derived facts in search results.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:46Z", "labels": ["delegate:claude-subagent", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.7.3", "depends_on_id": "beefcake-swarm-3is.7", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.7.3", "depends_on_id": "beefcake-swarm-3is.7.1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-4p5", "title": "Iteration delta memory (structured context evolution)", "description": "UNIQUE INSIGHT from GPT-5.2-Codex and Opus\n\nReplace flat 'previous_attempts' strings with structured iteration deltas that capture WHAT CHANGED between iterations, not just what happened.\n\nAdd IterationDelta struct:\n- fixed_errors: Vec<ErrorCategory> \u2014 what improved\n- new_errors: Vec<ErrorCategory> \u2014 what regressed  \n- files_modified: Vec<String> \u2014 what was touched\n- hypothesis: Option<String> \u2014 what the model claimed it was doing (extracted from response)\n- result_summary: String \u2014 'borrow error fixed, but introduced lifetime error in return type'\n- strategy_used: String \u2014 'added Arc wrapper' or 'changed lifetime annotation'\n\nInclude only the last 2-3 deltas in context (not full history). The model needs to see 'you fixed X but broke Y' not 'here is everything that ever went wrong'.\n\nAlso add provenance tags to FileContext (source: compiler_error, diff, dependency, import) and decay: reduce weight of previously included context not re-referenced by new errors.\n\nFiles: coordination/src/work_packet/types.rs (new IterationDelta type), coordination/src/escalation/state.rs, coordination/src/context_packer/packer.rs", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:35:38Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-4qz", "title": "Add search_code (rg wrapper), file_exists, cargo_metadata tools for workers", "notes": "Workers currently can only read files by exact path. Adding rg wrapper and file_exists would help workers understand codebase structure before editing.", "status": "open", "priority": 2, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:43Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:54Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-54h", "title": "Integrate DSRs (dspy-rs) for typed signatures and prompt optimization", "description": "Replace hand-crafted prompt strings with DSRs typed signatures for worker agents.\n\nDSRs (https://dsrs.herumbshandilya.com/) is a Rust-native rewrite of DSPy that provides:\n- Typed Signatures: define structured I/O contracts for LLM tasks\n- Constraint assertions: #[assert(\"this.len() > 0\")] for output validation\n- COPRO optimizer: automatically tune prompts using past successful runs as training data\n- Native Rust: no Python sidecar needed, integrates directly into swarm-agents crate\n\nIMPLEMENTATION PLAN:\n\nPhase 1: Define signatures for each agent role\n- FixCompileError: file_contents + error_message \u2192 fixed_contents + explanation\n- ImplementFeature: issue_description + file_context \u2192 modified_files + explanation\n- ReviewCode: git_diff \u2192 verdict (PASS/FAIL) + feedback\n\nPhase 2: Replace Predict<T> for agent calls\n- Swap raw .prompt() calls with DSRs Predict<FixCompileError>::new()\n- Add constraint assertions for scope discipline (#[assert] on output to enforce file identity)\n- Use BamlType for structured error categories in routing\n\nPhase 3: Prompt optimization with COPRO\n- Log all (input, output, verifier_result) triples from swarm runs\n- After N successful runs, train COPRO to optimize prompts per-model\n- Separate optimizers for Qwen3-Coder-Next vs OR1-Behemoth vs strand-14B\n\nDependencies: dspy-rs = \"0.7\" in swarm-agents Cargo.toml\nWorks with existing OpenAI-compatible endpoints (vasp-02:8080, vasp-01:8081)", "notes": "Premature \u2014 requires dataset of successful swarm runs for COPRO optimizer training. Revisit after 50+ completed issues.", "status": "open", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T19:22:50Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:08Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-5bk", "title": "Integrate tree-sitter-rust for AST-aware context packing", "description": "Replace regex/line-based symbol extraction in ContextPacker with tree-sitter-rust AST parsing. Currently build_file_contexts() takes first 30 lines of each file (packer.rs:94-136) which misses critical context. With tree-sitter: extract fn signatures, impl blocks, trait definitions, struct fields at AST level. On verifier failures, use rustc error spans to pack \u00b180 lines around each span + referenced symbol definitions. This dramatically improves context quality and reduces iterations. Deps: tree-sitter, tree-sitter-rust crates. Files: coordination/src/context_packer/packer.rs, coordination/src/context_packer/file_walker.rs, new file coordination/src/context_packer/ast_index.rs. All 4 consulted models (G3-Pro, GPT-5.2-Codex, GPT-5.2, Claude Opus 4.5) flagged this as highest priority. Opus rated it P0.", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:48:44Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:59Z", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-5iz", "title": "Persist EscalationState across crashes", "description": "Persist EscalationState across crashes in coordination/src/escalation/state.rs.\n\nEscalationState is purely in-memory. If orchestrator crashes mid-issue, all error history and escalation progress is lost.\n\nADD to EscalationState:\n1. pub fn save_to_file(&self, path: &Path) -> Result<()> \u2014 serialize to JSON file\n2. pub fn load_from_file(path: &Path) -> Result<Option<Self>> \u2014 deserialize if file exists\n\nDerive Serialize/Deserialize on EscalationState (and SwarmTier if not already derived).\n\nThen in crates/swarm-agents/src/orchestrator.rs process_issue():\n- After creating worktree, check for <wt_path>/.beefcake-state.json and load if present\n- After each escalation.record_iteration(), call escalation.save_to_file(<wt_path>/.beefcake-state.json)\n\nACCEPTANCE: cargo test -p coordination passes. Add a roundtrip test: create state, save, load, verify fields match. cargo fmt and cargo clippy clean.", "notes": "Nice to have but not blocking dogfooding. Workaround: restart picks up the issue again from beads.", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:12:29Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:09Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-5uq", "title": "Add Adversarial Breaker agent for pre-merge red-teaming", "description": "STRONG CONSENSUS (3/4 models)\n\nBefore merging, a dedicated agent should actively try to BREAK the implementation by generating edge-case tests, weird inputs, concurrency interleavings, and invalid states.\n\nThe SwarmTier::Adversary already exists in escalation/state.rs but isn't integrated into the loop.\n\nImplementation:\n1. After Verifier passes (all green), invoke the Adversary agent\n2. Give it ONLY the diff and public API signatures (no implementation context \u2014 truly adversarial)\n3. It generates: proptest harnesses, edge-case unit tests, boundary condition checks\n4. Run these generated tests through the Verifier\n5. If any Adversary test fails, reject the implementation and feed the failing test back to the Implementer\n\nThis catches 'compiles but is wrong' \u2014 the biggest failure mode that compilation gates miss.\n\nG3-Pro: 'Active Sabotage / Mutation Testing'. Opus: 'Adversary actively tries to break the code'.\n\nFiles: crates/swarm-agents/src/ (new adversary module), crates/swarm-agents/src/main.rs", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:34:44Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:00Z", "dependencies": [{"issue_id": "beefcake-swarm-5uq", "depends_on_id": "beefcake-swarm-j4l", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-687", "title": "Add retry with backoff for transient git failures", "description": "Add retry_git_command() helper to crates/swarm-agents/src/worktree_bridge.rs for transient git failures.\n\nGit operations fail transiently with 'index.lock' errors under concurrent access. Currently all git commands fail immediately.\n\nADD helper function:\nfn retry_git_command(cmd: &mut Command, max_retries: u32) -> Result<Output> {\n    let delays = [100, 500, 2000]; // ms\n    for attempt in 0..=max_retries {\n        let output = cmd.output()?;\n        if output.status.success() { return Ok(output); }\n        let stderr = String::from_utf8_lossy(&output.stderr);\n        if attempt < max_retries && (stderr.contains(\"index.lock\") || stderr.contains(\"Unable to create\")) {\n            std::thread::sleep(Duration::from_millis(delays[attempt as usize]));\n            continue;\n        }\n        return Ok(output); // return the failure for caller to handle\n    }\n}\n\nApply to: create() (git worktree add), merge_and_remove() (git merge), and git_commit_changes() in orchestrator.rs.\n\nACCEPTANCE: cargo test -p swarm-agents passes. cargo fmt and cargo clippy clean.", "notes": "Previously P4 after job 1632 destroyed files. With edit_file tool now available, adding a helper function is an additive task. Re-evaluate after dogfood round 3.", "status": "open", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:11:43Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:10:29Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-6a1", "title": "Add scope constraints to prevent workers from modifying existing code beyond task scope", "description": "Job 1621: Worker was asked to add cleanup_worktree() method but also:\n- Changed .status.success() to .status().success() in EVERY existing method (wrong API)\n- Added .expect() after .context()? everywhere (nonsensical)\n- Introduced a typo (from_utf8_lossii)\n- Broke closing braces\n- Deleted comments\n\nThe worker went far beyond scope \u2014 it touched every method in the file when it only needed to add one new method and wire it into one spot in orchestrator.rs.\n\nFIX OPTIONS:\n1. Add file-diff size limits: if worker modifies >N lines outside the target area, reject the commit\n2. Add explicit scope constraints in the work packet: \"ONLY modify these functions/methods, do NOT touch existing code\"\n3. Add a pre-commit check that compares the diff against the task description and rejects over-broad changes\n4. Use git diff --stat to detect when changes are disproportionate to the task\n\nMost practical: Option 2 \u2014 add scope constraints to the work packet prompt that explicitly tell the worker what files/methods it may modify and what it must not touch.", "notes": "Partially mitigated by edit_file tool (beefcake-swarm-xom). Workers now specify exact blocks to change instead of rewriting entire files. Remaining value: prompt-level scope constraints in work packets.", "status": "open", "priority": 2, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T19:15:09Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:08Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-7go", "title": "Detect and clean zombie branches on startup", "description": "If the orchestrator crashes mid-loop, /tmp may clean the worktree directory but the swarm/<issue_id> branch persists in the repo. On next run, git worktree add -b fails with 'A branch named swarm/... already exists' and the agent gets permanently stuck on that issue.\n\nFix: On startup (before the main loop), scan for orphaned swarm/* branches that have no corresponding worktree directory. For each orphan: attempt git branch -D to delete it. Also check git worktree list --porcelain for stale entries and run git worktree prune. This requires the remove_worktree() method from the worktree cleanup issue.\n\nAdd a WorktreeBridge::cleanup_stale() method that:\n1. Runs git worktree prune\n2. Lists all branches matching swarm/*\n3. For each, checks if worktree_path(id) exists\n4. If not, force-deletes the branch\n\nCall cleanup_stale() in main() before entering the issue-picking loop.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs, crates/swarm-agents/src/main.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review", "notes": "SWARM-READY: Add cleanup_stale() method to WorktreeBridge + call in main.rs. Additive task similar to vl2 (stale branch cleanup).", "status": "open", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:12:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:29:00Z", "dependencies": [{"issue_id": "beefcake-swarm-7go", "depends_on_id": "beefcake-swarm-rb2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-7jt", "title": "Add coordination crate integration tests for standalone build", "description": "The coordination crate was copied from beefcake2. Verify all existing tests pass in the new standalone context. Fix any path assumptions or missing dependencies. Run cargo test -p coordination.", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:30Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-9fs", "title": "Short-circuit manager timeout when worker completes (rig agent opacity)", "description": "The manager agent has a 45-minute timeout. When a worker finishes in 8 minutes (job 1629), the manager still runs until timeout because rig's agent-as-tool pattern doesn't expose intermediate completion signals. Options: (1) Reduce AGENT_TIMEOUT to 15min since most real work happens in first 10min, (2) Monitor worktree for git changes and short-circuit when changes + verifier pass detected, (3) Add a side-channel (file flag) that the verifier sets when green. Files: crates/swarm-agents/src/orchestrator.rs", "notes": "MANUAL: Requires async signal handling and understanding of rig agent internals. Too complex for current swarm.", "status": "open", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:09:48Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:13:42Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-a3y", "title": "Improve token counting accuracy (bytes vs chars)", "description": "Fix token counting in coordination/src/work_packet/types.rs.\n\nWorkPacket::estimated_tokens() uses json.len() / 4 where String::len() returns byte count not character count. Multi-byte UTF-8 characters cause overestimation.\n\nFIX in estimated_tokens() method:\n1. Change json.len() / 4 to json.chars().count() / 4\n2. Apply a safety margin: multiply result by 1.1 (10% buffer) to avoid context overflow\n\nACCEPTANCE: cargo test -p coordination passes. Add a test with a WorkPacket containing unicode strings and verify estimated_tokens() returns reasonable values. cargo fmt and cargo clippy clean.", "notes": "SWARM-READY: Change json.len()/4 to json.chars().count()/4 plus add a test. Single-line fix.", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:13:42Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-a8m", "title": "Add merge conflict test for WorktreeBridge", "description": "WorktreeBridge tests cover create and list but not the merge conflict scenario, which is the most operationally dangerous path. When the main branch moves forward with a conflicting change while the worktree branch also makes changes, merge_and_remove() will fail \u2014 and the cleanup behavior needs to be verified.\n\nFix: Add test case to worktree_bridge.rs::tests:\n1. Create a worktree\n2. Commit a change to a file on the main branch\n3. Commit a conflicting change to the same file in the worktree\n4. Call merge_and_remove() \u2014 assert it returns Err\n5. Verify the worktree still exists (not accidentally deleted)\n6. Call remove_worktree() \u2014 assert it cleans up (depends on worktree cleanup method)\n\nAlso test: merge_and_remove() with uncommitted changes (should fail with descriptive error).\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review", "notes": "SWARM-READY: Pure additive test. Add test functions to worktree_bridge.rs::tests. Similar to gbh/vl2 which succeeded.", "status": "open", "priority": 0, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:38Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T20:53:57Z", "dependencies": [{"issue_id": "beefcake-swarm-a8m", "depends_on_id": "beefcake-swarm-rb2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-bbv", "title": "Death spiral circuit breaker (revert on error increase)", "description": "Add death spiral circuit breaker to crates/swarm-agents/src/orchestrator.rs.\n\nIn process_issue() main loop, after verifier runs (line ~456), compare error count to previous iteration. If errors increased by 50%+, revert changes and retry with strict constraints.\n\nADD after the verifier report check:\n1. Compare report.failure_signals.len() with previous iteration count (track in a local variable)\n2. If current_errors > prev_errors * 1.5 AND prev_errors > 0:\n   a. Log warning: 'Circuit breaker: error count increased from {prev} to {current}, reverting'\n   b. Run: git checkout -- . (in worktree) to revert all changes\n   c. Set a flag to add 'STRICT: change ONLY the lines reported in the error' to next iteration prompt\n3. If circuit breaker fires twice consecutively, force escalation (set escalation tier to next level)\n\nFiles: crates/swarm-agents/src/orchestrator.rs (process_issue function)\nACCEPTANCE: cargo test -p swarm-agents passes. cargo fmt and cargo clippy clean.", "notes": "CAUTION: Modifies orchestrator.rs process_issue() loop. With edit_file, might work if well-scoped. Test after simpler tasks succeed with edit_file.\nR3 audit (2026-02-17): demoted to P2. Important, but after verifier-scope correctness fixes (hf5/19l) and write_file blast-radius guard (jtj); circuit-breaker changes are loop-sensitive and higher implementation risk.", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:35:27Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:25:00Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-djc", "title": "Migrate coordination crate from anyhow to thiserror", "description": "The coordination crate is a library but uses anyhow::Result in some internal functions. Library crates should provide structured error types via thiserror so consumers can match on specific error variants, while anyhow is appropriate for application binaries (swarm-agents).\n\nFix: Define error enums with thiserror for each coordination module:\n- VerifierError (GateTimeout, CommandFailed, ParseError)\n- EscalationError (BudgetExhausted, InvalidTransition)\n- ContextPackerError (FileWalkFailed, TokenBudgetExceeded)\n- WorkPacketError (GitCommandFailed, SymbolExtractionFailed)\n\nKeep anyhow in swarm-agents binary for ergonomic error propagation.\n\nFiles: coordination/src/verifier/, coordination/src/escalation/, coordination/src/context_packer/, coordination/src/work_packet/\nFound by: G3-Pro deep review", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:26Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-dxt", "title": "Atomic issue claiming to prevent race conditions", "description": "The orchestrator has a check-then-act race: it calls list_open(), sorts by priority, picks the first issue, then calls update_status(in_progress). If two orchestrator instances run simultaneously (or two swarm loops), both will pick the same P1 issue, leading to duplicate work, git conflicts, and wasted inference credits.\n\nFix: Either implement an atomic claim_next_available() in BeadsBridge that combines list+claim in a single operation, or handle the case where update_status returns 'already claimed' and retry with the next issue. Consider adding a locking mechanism (file lock or beads-level CAS).\n\nFiles: crates/swarm-agents/src/beads_bridge.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review", "notes": "Previously P4 after job 1631 destroyed orchestrator.rs. With edit_file tool now available, this task may be feasible. Worker would use edit_file for targeted changes instead of rewriting the whole file. Re-evaluate after confirming edit_file works in dogfood round 3.", "status": "open", "priority": 3, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:11:39Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:10:29Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ez1", "title": "Add tracing spans with #[instrument] to key methods", "description": "Current logging is flat \u2014 all log lines are at the same level with no hierarchical grouping. It's hard to correlate which logs belong to which iteration, issue, or git operation when reviewing output.\n\nFix: Add #[tracing::instrument] attributes to key methods:\n- ContextPacker::pack_initial(bead_id, objective) \u2014 span includes bead_id\n- ContextPacker::pack_retry(bead_id, ...) \u2014 span includes bead_id and iteration\n- WorktreeBridge::create(issue_id) \u2014 span includes issue_id\n- WorktreeBridge::merge_and_remove(issue_id) \u2014 span includes issue_id\n- Verifier::run_pipeline() \u2014 span includes working_dir\n- Implementer::implement() \u2014 span includes model name\n\nAdd skip directives for large parameters (e.g. task_description content). This creates nested spans visible in structured log output (JSON) or tracing-subscriber's hierarchical formatter.\n\nFiles: coordination/src/context_packer/packer.rs, crates/swarm-agents/src/worktree_bridge.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:29Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-hf5", "title": "Align RunVerifierTool scope with orchestrator acceptance verifier", "notes": "R3 audit (2026-02-17): treat as Tier 1. run_verifier currently uses VerifierConfig::default() (workspace scope) while orchestrator acceptance scopes to package swarm-agents; this mismatch can cause false manager loops/timeouts and inconsistent pass criteria.", "status": "open", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:34Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:25:01Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ibu", "title": "Add OpenTelemetry spans per attempt/tool/gate for decision-grade observability", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:43Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T20:20:43Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-iye", "title": "Replace git reset --hard in prompts with constrained recovery tools", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:43Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T20:20:43Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-j4l", "title": "Add Planner agent with structured change contracts", "description": "UNANIMOUS CONSENSUS (4/4 models: G3-Pro, GPT-5.2-Codex, GPT-5.2, Opus 4.5)\n\nThe current loop goes straight to code generation without formalizing intent. Add a Planner agent (can use the fast 14B model) that runs BEFORE the Implementer and produces a structured 'change contract':\n\n- Acceptance criteria (what must be true when done)\n- Invariants / behavioral promises (what must NOT change)\n- Files expected to change (scoping)\n- Risk classification (API break risk, concurrency risk, security risk)\n- Test plan: which tests to add/modify, what failure proves correctness\n\nThe contract becomes:\n1. A constraint for the Implementer (bounded search space)\n2. A checklist for the Validator (check against plan, not just 'does it look right')\n3. An audit trail (what was intended vs what happened)\n4. Partial rollback points (if step 3/5 fails, retry from step 3)\n\nImplementation: Add a PlannerAgent trait and struct in swarm-agents, produce a ChangeContract type in coordination/. The contract feeds into WorkPacket.constraints and WorkPacket.decisions. The Validator receives the contract alongside the diff.\n\nG3-Pro calls this 'Legislator-Executor'. GPT-5.2-Codex calls it '3-stage contract + selection'. Both identify it as the highest-impact architectural change.\n\nFiles: crates/swarm-agents/src/ (new planner module), coordination/src/ (new contract types), crates/swarm-agents/src/main.rs", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:34:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:00Z", "dependency_count": 0, "dependent_count": 2, "comment_count": 0}
{"id": "beefcake-swarm-jse", "title": "Sandbox verification pipeline (treat agent code as untrusted)", "description": "STRONG CONSENSUS (3/4 models)\n\nAgent-generated code can do ANYTHING \u2014 including filesystem deletion, network exfiltration, command execution in build.rs, or infinite loops in tests. On an HPC cluster with shared NFS, this is catastrophic.\n\nRun all verification gates in a sandbox:\n1. Use bubblewrap (bwrap) or firejail on Linux to sandbox cargo commands\n2. Restrictions: no network access, restricted filesystem (only worktree + target dirs), CPU/memory/time caps\n3. Per-test timeouts (not just per-gate \u2014 the existing gate_timeout_secs config isn't enforced)\n4. Kill process trees on timeout (not just the parent process)\n5. Detect 'test passed but took 10x longer than baseline' as suspicious\n\nAlso add: pre-check for dangerous patterns before running (new unsafe blocks, std::process::Command, std::fs::remove_dir_all, network calls).\n\nNote: VerifierConfig already has gate_timeout_secs (line 28 in pipeline.rs) but it's not enforced \u2014 gates use blocking Command::output() with no timeout.\n\nFiles: coordination/src/verifier/pipeline.rs, coordination/src/verifier/ (new sandbox module)", "status": "open", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:34:53Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:59Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-jtj", "title": "Add blast-radius guard to write_file (reject >50% shrink of existing files)", "description": "Even with edit_file available, workers may still use write_file on existing files. At 5-15 tok/s, local models lose context and truncate, destroying files. Add a guard in WriteFileTool::call() that reads the existing file size, and if the new content is <50% of the original, returns an error telling the worker to use edit_file instead. Files: crates/swarm-agents/src/tools/fs_tools.rs", "notes": "SWARM-READY: Add size check guard in WriteFileTool::call(). Single function edit via edit_file. Well-scoped.", "status": "open", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:09:48Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T20:53:27Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-mu1", "title": "Implement graceful shutdown with worktree cleanup", "description": "The orchestrator has no signal handler. If killed via SIGINT (Ctrl+C) or SIGTERM (deployment, systemd stop) while a worktree is active, the worktree remains on disk and registered in git, causing disk space leaks and 'already checked out' errors on restart.\n\nFix: Use tokio::signal::ctrl_c() and tokio::signal::unix::signal(SignalKind::terminate()) to catch shutdown signals. When received:\n1. Set a shutdown flag (AtomicBool or tokio::sync::watch)\n2. Check the flag at the top of each loop iteration\n3. On shutdown: log the in-progress issue ID, call worktree_bridge.remove_worktree(), update beads status back to 'open', then exit cleanly\n\nConsider wrapping the active worktree path in an Arc<Mutex<Option<String>>> so the signal handler knows what to clean up. Requires the remove_worktree() method from the worktree cleanup issue.\n\nFiles: crates/swarm-agents/src/main.rs\nDepends on: worktree cleanup method (beefcake-swarm-rb2)\nFound by: G3-Pro deep review", "notes": "Dependency rb2 is closed. Worktree leaks on crash cause 'already checked out' on restart.", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:12:26Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:54Z", "dependencies": [{"issue_id": "beefcake-swarm-mu1", "depends_on_id": "beefcake-swarm-rb2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ost", "title": "Priority-scored context trimming in ContextPacker", "description": "ContextPacker::trim_to_budget() drops file_contexts from the end via pop(). This happens to work correctly because WorkPacketGenerator inserts error contexts first (highest priority) and file header contexts last (lowest priority). But this ordering guarantee is implicit and fragile \u2014 any change to insertion order in a different module breaks trimming priority silently.\n\nFix: Add a priority field to FileContext (or use an enum: Error > Modified > Header > Reference). Before trimming, sort file_contexts by priority (lowest priority last). Then pop() is guaranteed to drop the least important context first regardless of insertion order.\n\nAlternative: Instead of modifying FileContext, partition into priority buckets and trim from the lowest bucket first.\n\nFiles: coordination/src/context_packer/packer.rs, coordination/src/work_packet/types.rs\nFound by: G3-Pro deep review", "status": "open", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:33Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:16Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-pbk", "title": "Structure validator feedback as actionable deltas (TextGrad pattern)", "description": "Currently validator feedback (main.rs:287 result.feedback) goes into logs but does not systematically feed back into the next iteration prompt. Steal the TextGrad pattern: structure validator critiques as specific code locations + suggested fixes, not just prose. Modify pack_retry to include validator feedback as a first-class FailureSignal with category ValidatorRejection. Add structured fields: file, line_range, issue_type, suggested_fix. This creates tighter feedback loops and fewer iterations. Files: coordination/src/verifier/report.rs (add ValidatorFeedback type), coordination/src/context_packer/packer.rs (include in pack_retry), crates/swarm-agents/src/validator.rs (structured output), crates/swarm-agents/src/main.rs (wire feedback). Low effort, medium payoff.", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:49:10Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:00Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-pid", "title": "Expand verifier with Miri, proptest, cargo-deny gates", "description": "STRONG CONSENSUS (3/4 models)\n\nThe verifier only runs fmt/clippy/check/test. Add risk-based verification gates:\n\nHigh-value additions:\n1. cargo miri test \u2014 UB detection for unsafe-heavy code (mandatory for any unsafe blocks)\n2. cargo deny check \u2014 security advisories, banned crates, license compliance\n3. cargo semver-checks \u2014 prevent accidental breaking API changes\n4. cargo udeps \u2014 detect unused dependencies introduced by agents\n5. nextest \u2014 faster test execution with better isolation + flaky detection\n6. Feature matrix: test with --no-default-features and key feature flags\n7. Doc tests / rustdoc lints for public API changes\n\nMake gates ADAPTIVE: select which extra gates to run based on diff risk profile:\n- Touches unsafe \u2192 require Miri\n- Changes Cargo.toml \u2192 require deny + udeps\n- Changes public API \u2192 require semver-checks\n- All changes \u2192 nextest with flaky detection\n\nFiles: coordination/src/verifier/pipeline.rs, coordination/src/verifier/ (new gate modules)", "status": "open", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:35:02Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:59Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ty0", "title": "Use structured error fields in tracing macros", "description": "Error logging uses string interpolation: error!('Failed to create worktree: {e}'). This embeds the error message in the format string, making it impossible for log aggregators (Loki, Datadog) to parse the error type separately from the message.\n\nFix: Use structured fields in tracing macros throughout main.rs and worktree_bridge.rs:\n- error!(error = %e, issue_id = %id, 'Failed to create worktree') \u2014 Display format\n- error!(error = ?e, issue_id = %id, 'Failed to create worktree') \u2014 Debug format (more detail)\n- warn!(iteration, feedback = %result.feedback, 'Validator FAILED')\n\nThis allows querying logs by error type, issue_id, or iteration number independently.\n\nFiles: crates/swarm-agents/src/main.rs, crates/swarm-agents/src/worktree_bridge.rs\nFound by: G3-Pro deep review", "status": "open", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:32Z", "created_by": "TheFermiSea", "updated_at": "2026-02-11T16:14:32Z", "dependencies": [{"issue_id": "beefcake-swarm-ty0", "depends_on_id": "beefcake-swarm-ez1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-xv9", "title": "Handle detached HEAD in WorkPacketGenerator", "description": "WorkPacketGenerator::git_branch() uses 'git rev-parse --abbrev-ref HEAD' which returns the literal string 'HEAD' when in detached HEAD state (common in CI, after checkout of a specific commit, or in freshly created worktrees before the first commit).\n\nThis means the WorkPacket.branch field becomes 'HEAD' which is misleading in the prompt sent to the LLM and may confuse the agent about which branch it's working on.\n\nFix: If git_branch() returns 'HEAD', fall back to:\n1. Check for CI env vars (CI_COMMIT_REF_NAME, GITHUB_HEAD_REF, BRANCH_NAME)\n2. Try 'git name-rev --name-only HEAD'\n3. Use 'detached@<short-sha>' as a last resort\n\nFiles: coordination/src/work_packet/generator.rs\nFound by: G3-Pro deep review", "notes": "Only affects CI and unusual git states. Worktree branches are always named (swarm/<id>) so this doesn't fire in normal swarm operation.", "status": "open", "priority": 2, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:12:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:08:09Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ylh", "title": "Orchestrator needs task-completeness check beyond verifier gates", "description": "Job 1628 showed a false positive: the swarm auto-fixed a clippy lint without writing any new code, and the orchestrator accepted because 4/4 gates passed. The orchestrator should verify that the diff actually addresses the issue (e.g., check that new files/functions were added for test issues, or that the changed files are relevant to the issue description).", "notes": "MANUAL: Requires understanding of how auto-fix creates false positives. Complex judgment call on what constitutes task completion.", "status": "open", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T23:46:50Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:13:42Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-zg6", "title": "Multi-proposal speculative execution for hard tasks", "description": "UNANIMOUS CONSENSUS (4/4 models)\n\nFor hard/risky tasks, a single implementer often gets stuck in local minima. All 4 models propose spawning multiple implementations in parallel.\n\nDesign:\n1. Classify issue difficulty (simple vs hard) based on: error category history, file count, risk classification from Planner\n2. For hard tasks, spawn 2-3 Implementers in parallel with different strategies:\n   - Different temperature settings (0.2 conservative, 0.8 creative)\n   - Different system prompts ('minimal fix' vs 'refactor for clarity')\n   - Different model tiers (14B fast attempt + 72B deep attempt)\n3. Each produces a patch in its own worktree\n4. Run Verifier on all patches in parallel (CPU-bound, manageable)\n5. If multiple pass, Validator ranks and picks the best (most concise, least risk)\n6. If none pass, combine insights from all attempts for the retry context\n\nThis trades compute for success rate. G3-Pro: 'Parallel Speculative Decoding'. GPT-5.2: 'Selection via competition \u2014 turn merges into a market'.\n\nFiles: crates/swarm-agents/src/main.rs, crates/swarm-agents/src/worktree_bridge.rs (multi-worktree support)", "status": "open", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:34:37Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:03:00Z", "dependencies": [{"issue_id": "beefcake-swarm-zg6", "depends_on_id": "beefcake-swarm-j4l", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is", "title": "Unified Knowledge Graph Integration", "description": "Root epic: Bridge documentation and code for the agent swarm via a Unified Knowledge Graph backed by SurrealDB (RocksDB storage engine). Covers infrastructure, code graph ingestion, document ingestion, WorkPacket integration, self-learning loop, benchmarking, and MCP tool exposure.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:01Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:32Z", "labels": ["epic:ukg"], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.1", "title": "UKG: Infrastructure & Deployment", "description": "Sub-epic 1: Deploy SurrealDB on ai-proxy LXC, design schema, create Rust client, investigate SurrealDB vs direct RocksDB for existing stores.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:32Z", "labels": ["epic:ukg", "infrastructure"], "dependencies": [{"issue_id": "beefcake-swarm-3is.1", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.1.3", "title": "Design & deploy SurrealDB schema", "description": "Create SurrealQL schema for code graph (function, struct, trait, module, file + calls/defines/imports/implements edges), doc graph (document, chunk + contains edges), and bridge edges (documents, mentions). Vector indexes for 3584-dim nomic embeddings (MTREE cosine). Deploy to prod namespace.", "notes": "Use PAL chat with g3-pro (Librarian role). Output: schema.surql file. Consider: SCHEMAFULL tables, DEFINE INDEX for vector search, DEFINE FIELD with type constraints.", "status": "in_progress", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:18Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:44Z", "labels": ["delegate:g3-pro", "design", "epic:ukg", "implementation"], "dependencies": [{"issue_id": "beefcake-swarm-3is.1.3", "depends_on_id": "beefcake-swarm-3is.1", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.1.3", "depends_on_id": "beefcake-swarm-3is.1.2", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 2, "comment_count": 0}
{"id": "beefcake-swarm-3is.2", "title": "UKG: Code Graph Ingestion (codegraph-rust)", "description": "Sub-epic 2: Research and deploy codegraph-rust for AST-level code graph ingestion into SurrealDB. Share tree-sitter parsing with context_packer.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:32Z", "labels": ["epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.2", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.3", "title": "UKG: Document/Semantic Ingestion (CocoIndex \u2192 SurrealDB)", "description": "Sub-epic 3: Extend CocoIndex pipeline with SurrealDB target for dual-write (pgvector + SurrealDB). Build bridge edges linking doc chunks to code symbols.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:33Z", "labels": ["epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.3", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.4", "title": "UKG: WorkPacket & Coordination Integration", "description": "Sub-epic 4: Wire knowledge graph into WorkPacket enrichment, Router task classification, and Escalation signals. THE critical integration layer.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:33Z", "labels": ["epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.4", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.5", "title": "UKG: Self-Learning Loop", "description": "Sub-epic 5: Error Pattern KB, fix pattern capture, tiered archival memory (Letta/MemGPT pattern). Agents learn from past fixes and avoid repeating failures.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:33Z", "labels": ["epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.5", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.6", "title": "UKG: Benchmarking (pgvector vs SurrealDB)", "description": "Sub-epic 6: Design and run benchmarks comparing pgvector and SurrealDB on latency, quality, throughput. Produce migration decision.", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:33Z", "labels": ["benchmark", "epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.6", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.7", "title": "UKG: MCP Tool Exposure", "description": "Sub-epic 7: Expose KG as MCP tools: search_knowledge_graph (hybrid vector+graph), query_code_graph (structural), store_knowledge (agent write-back).", "status": "in_progress", "priority": 4, "issue_type": "epic", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:09:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:33Z", "labels": ["epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.7", "depends_on_id": "beefcake-swarm-3is", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-04v", "title": "Add auth retry logic to NotebookBridge::run_command", "description": "Detect auth errors in nlm stderr and retry once with backoff, giving the CLI internal CSRF refresh mechanism a second attempt. Prevents 20-minute session timeout from causing permanent failures.", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T20:21:44Z", "created_by": "claude-code", "updated_at": "2026-02-17T20:36:48Z", "closed_at": "2026-02-17T20:36:48Z", "close_reason": "Auth retry logic added to NotebookBridge::run_command \u2014 retries once on auth errors with 2s delay", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-06v", "title": "SECURITY: exec_tool allowlist bypass via sh -c shell metacharacters", "status": "closed", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:33Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T20:22:59Z", "closed_at": "2026-02-16T20:22:59Z", "close_reason": "Fixed: reject shell metacharacters + direct execution without sh -c", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-0ko", "title": "Add GBNF grammar-constrained structured output for implementer patches", "description": "The apply_implementer_changes stub (main.rs:91-110) is the #1 blocker identified by all 4 consulted models. llama.cpp supports GBNF grammars and automatic JSON schema conversion via the json_schema body field in /v1/chat/completions. Define an ImplementerOutput JSON schema (list of file edits with paths, search/replace blocks or unified hunks, optional reasoning). Pass schema via json_schema field to force structured output. Implement real patch application by parsing the validated JSON response. Fallback: search-replace blocks (Aider pattern). Files: crates/swarm-agents/src/main.rs (apply_implementer_changes), crates/swarm-agents/src/implementer.rs, new file crates/swarm-agents/src/patch_applicator.rs. Reference: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md", "status": "closed", "priority": 0, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:48:49Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:22Z", "closed_at": "2026-02-16T13:01:22Z", "close_reason": "Stale \u2014 references apply_implementer_changes stub in main.rs which no longer exists. Architecture changed to agent-as-tool pattern where workers write files directly via write_file tool. GBNF constrained output is not applicable to the current rig-based approach.", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-0lh", "title": "Study Goose/Aider/OpenHands patch engines for apply_implementer_changes", "description": "Research patch application patterns from existing tools: (1) Aider: search-replace blocks with <<<< SEARCH / ==== REPLACE / >>>> markers, (2) OpenHands/SWE-agent: unified diff with patch command, (3) Goose by Block: closest existing product to our swarm (study their architecture and patch engine). Document pros/cons of each approach for our structured output pipeline. Determine which pattern best complements GBNF-constrained JSON output. G3-Pro identified Goose as most architecturally similar. GPT-5.2-Codex recommended OpenHands diff-structured outputs. Deliverable: design doc + recommendation.", "status": "closed", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:49:51Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:39Z", "closed_at": "2026-02-16T13:01:39Z", "close_reason": "Blocked by beefcake-swarm-0ko which was closed as stale. Patch engine study is irrelevant \u2014 current arch uses rig write_file tool, not patch application.", "dependencies": [{"issue_id": "beefcake-swarm-0lh", "depends_on_id": "beefcake-swarm-0ko", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-0wn", "title": "Phase 1: Wire coordination/ harness into swarm-agents orchestrator", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T10:10:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T12:40:56Z", "closed_at": "2026-02-15T12:40:56Z", "close_reason": "Phase 1 complete: PR #2 squash-merged to main", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-1ep", "title": "Enforce max_turns on manager agent (rig doesn't enforce on outer agent)", "description": "Job 1608: rig's default_max_turns(25) is only enforced on nested tool-agents (workers get MaxTurnError at 25). The outer manager agent exceeds its limit (logs show depth 27/25, 30/25) and keeps spawning workers indefinitely. Fix: wrap manager.prompt() with a tokio timeout + manual turn enforcement. The manager at 50 turns \u00d7 workers at 50 turns = 2500 potential LLM calls per iteration without enforcement.", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T08:53:41Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T08:55:25Z", "closed_at": "2026-02-16T08:55:25Z", "close_reason": "Added 10-min tokio::time::timeout around manager.prompt() to hard-cap runaway agents", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-2fd", "title": "Add integration test for orchestrator loop", "description": "There is no test that validates the full orchestrator loop data flow: ContextPacker -> format_work_packet -> Implementer -> apply_changes -> Verifier -> Validator. Each component is tested in isolation but the wiring between them is only exercised by running the real binary against a live inference server.\n\nFix: Create a test in crates/swarm-agents/tests/ that:\n1. Uses mock Implementer (returns a fixed code change)\n2. Uses mock Validator (returns PASS)\n3. Sets up a real temp git repo with a known-broken Rust file\n4. Runs the loop for 1 iteration\n5. Verifies: pack_initial called, implementer received formatted prompt, verifier ran, validator received diff, issue closed\n\nThis depends on extracting agent traits (beefcake-swarm-AGENT_TRAITS_ID) so mocks can be injected.\n\nFiles: crates/swarm-agents/tests/ (new), crates/swarm-agents/src/main.rs\nDepends on: agent traits extraction\nFound by: G3-Pro deep review", "status": "closed", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:35Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:23Z", "closed_at": "2026-02-16T13:02:23Z", "close_reason": "Dep 7ny closed as stale. The concept (integration test for orchestrator loop) is valid but needs to be reimagined against current rig agent-as-tool architecture. Reopen with updated description when needed.", "dependencies": [{"issue_id": "beefcake-swarm-2fd", "depends_on_id": "beefcake-swarm-7ny", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-2w0", "title": "Increase worker agent depth limits (10\u219225)", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T18:45:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T19:47:28Z", "closed_at": "2026-02-15T19:47:28Z", "close_reason": "Fixed in dogfood round 2", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3is.1.1", "title": "Research SurrealDB deployment for HPC/LXC", "description": "Research SurrealDB deployment best practices via PAL consensus. Compare embedded (RocksDB backend) vs networked (TiKV), auth/TLS setup, systemd config, resource requirements for ai-proxy LXC. Include SurrealDB 2.x architecture (compute/storage separation). Output: deployment decision document.", "notes": "PAL consensus prompt: Compare SurrealDB embedded (RocksDB) vs networked (TiKV) for a single-node LXC deployment. Consider: memory footprint, persistence guarantees, auth setup, systemd integration. Expected output: config template + systemd unit.\n\n## RESEARCH OUTPUT: SurrealDB Deployment for HPC/LXC\n\n### Executive Decision\n**RECOMMENDATION: Embedded RocksDB mode on ai-proxy LXC**\n\nTiKV is unnecessary for a single-node deployment. RocksDB embedded mode provides the best performance with zero network overhead, simpler operations, and lower resource consumption. TiKV only makes sense for multi-node HA deployments.\n\n---\n\n### 1. Architecture Overview\n\nSurrealDB separates compute (query layer) and storage layers:\n- **Query Layer**: Parser \u2192 Executor \u2192 Iterator \u2192 Document Processor (stateless)\n- **Storage Layer**: RocksDB (default), SurrealKV (experimental), TiKV (distributed)\n\nFor single-node: compute + storage run in the same process. No external dependencies.\n\n### 2. Storage Engine Comparison (Single-Node Context)\n\n| Aspect | RocksDB (Embedded) | TiKV (Single-Node) | SurrealKV |\n|--------|-------------------|--------------------|-----------| \n| Maturity | Industry standard (Meta) | Production-ready but overkill | Experimental, Rust-native |\n| Dependencies | C++ lib (compiles with SurrealDB) | Separate PD + TiKV services | None (pure Rust) |\n| Performance | ~508k reads/s, ~155k writes/s | Same (TiKV uses RocksDB internally) + network overhead | Not yet benchmarked |\n| Operational complexity | Minimal \u2014 single binary | High \u2014 3 separate daemons | Minimal |\n| HA/Replication | None (single-node) | Multi-raft (overkill for 1 node) | None |\n| Features | Standard KV | Distributed transactions | Time-travel queries, versioned data |\n\n**Decision: RocksDB.** TiKV adds PD + TiKV daemon overhead with zero benefit on single node. SurrealKV is still experimental.\n\n### 3. Resource Requirements\n\n**Minimum (ai-proxy LXC):**\n- CPU: 1+ vCPU (SurrealDB Cloud free tier runs on 0.25 vCPU)\n- RAM: 512 MB minimum, 2-4 GB recommended for knowledge graph + vector indexes\n- Disk: SSD/NVMe preferred (RocksDB LSM-tree optimized for SSDs)\n- The HNSW vector index uses a bounded memory cache (default 256 MiB, configurable)\n\n**Production recommended (for reference):**\n- 4+ cores, 8+ GB RAM, NVMe/SSD, no swap\n\nFor our UKG use case on ai-proxy, 2-4 GB RAM should be sufficient. RocksDB dynamically calculates block cache size based on available memory (`SURREAL_ROCKSDB_BLOCK_CACHE_SIZE`).\n\n### 4. Installation\n\n```bash\n# Install SurrealDB binary\ncurl --proto '=https' --tlsv1.2 -sSf https://install.surrealdb.com | sh\n\n# Verify\nsurreal version\n```\n\n### 5. Start Command (Production Config)\n\n```bash\nsurreal start \\\n  --log info \\\n  --user root \\\n  --pass '<STRONG_PASSWORD>' \\\n  --bind 0.0.0.0:8000 \\\n  rocksdb:///var/lib/surrealdb/data\n```\n\nKey flags:\n- `--bind 0.0.0.0:8000` \u2014 Listen on all interfaces (required for 10.0.0.0/24 access from compute nodes)\n- `--log info` \u2014 Production log level (debug/trace impact performance)\n- `--query-timeout <duration>` \u2014 Optional max query execution time\n- `--transaction-timeout <duration>` \u2014 Optional max transaction time\n- `--web-crt` / `--web-key` \u2014 TLS certificate/key paths (optional for internal network)\n- `--allow-net` / `--deny-net` \u2014 Control outbound network capabilities\n- `--allow-funcs` / `--deny-funcs` \u2014 Restrict available SurrealQL functions\n\n### 6. Systemd Unit File\n\n```ini\n[Unit]\nDescription=SurrealDB Server\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=surrealdb\nGroup=surrealdb\nExecStart=/usr/local/bin/surreal start \\\n  --log info \\\n  --bind 0.0.0.0:8000 \\\n  rocksdb:///var/lib/surrealdb/data\nRestart=always\nRestartSec=5\nLimitNOFILE=65536\nEnvironment=SURREAL_ROCKSDB_BLOCK_CACHE_SIZE=536870912\nEnvironment=SURREAL_SYNC_DATA=true\n\n[Install]\nWantedBy=multi-user.target\n```\n\nNotes:\n- Root user credentials persist after first start (no need in subsequent starts)\n- `LimitNOFILE=65536` \u2014 RocksDB opens many file descriptors\n- `SURREAL_SYNC_DATA=true` \u2014 Ensures write durability (default)\n- `SURREAL_ROCKSDB_BLOCK_CACHE_SIZE=536870912` \u2014 512 MB block cache (tune based on available RAM)\n\n### 7. Environment Variables (Key Tuning)\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `SURREAL_BIND` | 127.0.0.1:8000 | Bind address |\n| `SURREAL_LOG` | info | Log level |\n| `SURREAL_ROCKSDB_BLOCK_CACHE_SIZE` | Auto (based on RAM) | Read cache size |\n| `SURREAL_ROCKSDB_WRITE_BUFFER_SIZE` | Default | Write buffer memory |\n| `SURREAL_ROCKSDB_MAX_WRITE_BUFFER_NUMBER` | Default | Max concurrent write buffers |\n| `SURREAL_SYNC_DATA` | true | Fsync writes to disk |\n| `SURREAL_QUERY_TIMEOUT` | None | Max query execution time |\n\n### 8. Authentication & Security\n\n- **Auth enabled by default** since SurrealDB 2.0 (good)\n- First start with `--user`/`--pass` creates OWNER-role root user, persisted in storage\n- For internal HPC network: TLS optional but recommended if traversing untrusted segments\n- Restrict capabilities with `--deny-net`, `--deny-funcs` to minimize attack surface\n\n### 9. Network Access from Compute Nodes\n\nai-proxy LXC (100.105.113.58) is accessible from compute nodes on 10.0.0.0/24.\n- Bind to `0.0.0.0:8000` (or specific interface)\n- Compute nodes connect via: `http://100.105.113.58:8000` or the 10.x.x.x address\n- SurrealDB exposes HTTP/WebSocket APIs on the bind port\n- Rust SDK connects via: `Surreal::new::<Ws>(\"100.105.113.58:8000\")`\n\n### 10. Graph + Vector Capabilities (UKG-Relevant)\n\n**Graph:**\n- `RELATE` statement creates typed edges between records (vertex \u2192 edge \u2192 vertex)\n- Graph traversal via `->` syntax: `SELECT * FROM crate:tokio->depends_on->crate`\n- Edges are first-class records with metadata fields\n- No JOINs needed \u2014 record links provide direct traversal\n\n**Vector Search:**\n- MTREE index: `DEFINE INDEX idx ON table FIELDS embedding MTREE DIMENSION 768 DIST COSINE TYPE F32`\n- HNSW index: Faster ANN with tunable M and EFC parameters, 256 MiB default cache\n- kNN queries: `WHERE embedding <|K|> $query_vector`\n- Similarity functions: `vector::similarity::cosine()`, `vector::similarity::euclidean()`, etc.\n\n**Combined Graph+Vector (GraphRAG):**\n- Can traverse graph edges AND do vector similarity in the same query\n- Ideal for knowledge graph enrichment: find related code entities via graph, rank by vector similarity\n\n### 11. Deployment Steps\n\n```bash\n# 1. On ai-proxy LXC:\ncurl --proto '=https' --tlsv1.2 -sSf https://install.surrealdb.com | sh\n\n# 2. Create service user + data dir\nuseradd -r -s /bin/false surrealdb\nmkdir -p /var/lib/surrealdb/data\nchown surrealdb:surrealdb /var/lib/surrealdb/data\n\n# 3. Install systemd unit (see above)\n# 4. Enable and start\nsystemctl enable surrealdb\nsystemctl start surrealdb\n\n# 5. Initialize root credentials\nsurreal start --user root --pass '<PASSWORD>' --bind 0.0.0.0:8000 rocksdb:///var/lib/surrealdb/data\n# (first run only, then ctrl-c and let systemd manage)\n\n# 6. Verify from compute node\ncurl http://100.105.113.58:8000/health\n```\n\n### Sources\n- SurrealDB Architecture: https://surrealdb.com/docs/surrealdb/introduction/architecture\n- Storage & Deployment: https://surrealdb.com/learn/fundamentals/performance/deployment-storage\n- CLI Start Command: https://surrealdb.com/docs/surrealdb/cli/start\n- Environment Variables: https://surrealdb.com/docs/surrealdb/cli/env\n- Performance Best Practices: https://surrealdb.com/docs/surrealdb/reference-guide/performance-best-practices\n- Vector Search: https://surrealdb.com/docs/surrealdb/models/vector\n- SurrealDB Scalability: https://surrealdb.com/blog/surrealdb-scalability", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:18Z", "created_by": "TheFermiSea", "updated_at": "2026-02-13T08:31:16Z", "closed_at": "2026-02-13T08:31:16Z", "close_reason": "Research complete. Recommendation: Embedded RocksDB mode on ai-proxy LXC. Full deployment decision document with systemd config, tuning params, and graph+vector capabilities written to issue notes.", "labels": ["delegate:pal-consensus", "epic:ukg", "infrastructure", "research"], "dependencies": [{"issue_id": "beefcake-swarm-3is.1.1", "depends_on_id": "beefcake-swarm-3is.1", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.1.2", "title": "Deploy SurrealDB on ai-proxy LXC", "description": "Install SurrealDB on ai-proxy (100.105.113.58). Systemd service, network binding for 10.0.0.0/24 + Tailscale. Namespaces: beefcake/knowledge_graph (prod), beefcake/benchmark. Verify connectivity from vasp nodes.", "notes": "Human task \u2014 SSH required. Share creds via /cluster/shared/ai/surreal.env. Bind to 0.0.0.0:8000 with auth enabled.", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:18Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:44Z", "closed_at": "2026-02-13T08:45:50Z", "close_reason": "SurrealDB v2.6.1 deployed on ai-proxy LXC (100.105.113.58). RocksDB backend, systemd enabled, namespaces created (beefcake/knowledge_graph, beefcake/benchmark), credentials at /cluster/shared/ai/surreal.env, health verified externally.", "labels": ["delegate:human", "epic:ukg", "infrastructure"], "dependencies": [{"issue_id": "beefcake-swarm-3is.1.2", "depends_on_id": "beefcake-swarm-3is.1", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-3is.1.2", "depends_on_id": "beefcake-swarm-3is.1.1", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 2, "comment_count": 0}
{"id": "beefcake-swarm-3is.1.5", "title": "Investigate SurrealDB vs direct RocksDB for Error KB and Archival Memory", "description": "Investigate whether SurrealDB (which uses RocksDB as its default storage backend) should replace the direct RocksDB approach proposed in beefcake-swarm-3r9 (Error Pattern KB) and beefcake-swarm-b30 (Archival Memory). Compare: query flexibility (SurrealQL graph+vector vs manual KV), latency overhead (SurrealDB query layer vs direct RocksDB), operational simplicity (one DB vs two). Reference SurrealDB architecture.", "notes": "PAL consensus. Key insight: SurrealDB uses RocksDB under the hood \u2014 so the question is whether the query layer overhead is worth the graph/vector capabilities. If yes, 3r9 and b30 migrate to SurrealDB. If no, keep direct RocksDB for latency-sensitive operations (per-issue state) and use SurrealDB for cross-issue knowledge only.\n\n## RESEARCH OUTPUT: SurrealDB vs Direct RocksDB for Error KB & Archival Memory\n\n### Executive Decision\n**RECOMMENDATION: Hybrid Architecture \u2014 SurrealDB for Knowledge Graph, keep direct RocksDB for hot-path state**\n\nThe key insight is that SurrealDB *uses* RocksDB under the hood. The question is whether the SurrealQL query layer overhead is justified by the graph+vector capabilities. The answer is **yes for cross-issue knowledge, no for per-issue hot-path state**.\n\n---\n\n### 1. Current State Analysis\n\n**Existing Direct RocksDB (coordination/src/state/store.rs):**\n- 6 column families: sessions, tasks, results, voting, context, events\n- Simple KV access pattern: `put(cf, key, bincode_value)` / `get(cf, key)` / `list_keys(cf, prefix)`\n- Bincode serialization for speed\n- ~400 LOC, clean abstraction\n- Sub-millisecond operations (direct RocksDB = no query parsing, no optimization, no transaction overhead)\n\n**Proposed Error KB (beefcake-swarm-3r9) needs:**\n- Store: `(error_signature, code_snippet, fix_diff, strategy, model_tier, iterations)`\n- Query: Find similar past errors by ErrorCategory + key tokens + file structure\n- Aggregate: Per-model success rates by error category\n- Feed top-3 matching strategies into WorkPacket.relevant_playbooks\n\n**Proposed Archival Memory (beefcake-swarm-b30) needs:**\n- Store: Prior diffs, verifier reports, repeated errors, decisions, design notes per bead_id\n- Query: Semantic recall of relevant prior approaches that failed\n- Promote: Successful fix patterns \u2192 higher recall priority\n- Summarize: Compress old memories to save context window tokens\n\n---\n\n### 2. Latency Analysis\n\n| Operation | Direct RocksDB | SurrealDB (Embedded) | SurrealDB (Networked) |\n|-----------|---------------|---------------------|----------------------|\n| Point read | ~1-10 \u03bcs | ~50-500 \u03bcs (parse+execute+KV) | ~1-5 ms (+ network) |\n| Point write | ~1-10 \u03bcs | ~100-1000 \u03bcs | ~2-10 ms |\n| Prefix scan | ~10-100 \u03bcs | ~200-2000 \u03bcs | ~5-20 ms |\n| Graph traversal | N/A (manual) | ~500 \u03bcs-5 ms | ~5-50 ms |\n| Vector similarity | N/A (manual) | ~1-50 ms (HNSW) | ~5-100 ms |\n| Complex join | N/A (manual) | ~1-10 ms | ~10-100 ms |\n\n**Key finding:** SurrealDB query layer adds ~10-100x overhead for simple KV operations due to SurrealQL parsing, query planning, and transaction management. For the hot-path (ensemble coordination during compilation loops), this matters. For knowledge retrieval (once per issue start or escalation), it doesn't.\n\n**Note:** One GitHub issue (#4767) reported embedded SurrealDB taking 17s vs 5s standalone for 69K record queries, suggesting the embedded Rust SDK may have performance anomalies. Worth benchmarking in our specific environment.\n\n---\n\n### 3. Feature Comparison for Error KB\n\n| Requirement | Direct RocksDB | SurrealDB |\n|-------------|---------------|-----------|\n| Store error patterns | \u2705 Column family + bincode | \u2705 `CREATE error_pattern SET ...` |\n| Exact match (ErrorCategory) | \u2705 Prefix scan on compound key | \u2705 `WHERE category = 'BorrowChecker'` |\n| Fuzzy match (key tokens) | \u274c Must implement manually | \u2705 Full-text search + vector similarity |\n| Graph: \"errors \u2192 fixes \u2192 crates \u2192 files\" | \u274c Manual adjacency lists in KV | \u2705 `RELATE error:e1->fixed_by->fix:f1` + traversal |\n| Aggregation (success rates) | \u274c Manual iteration + counting | \u2705 `SELECT model_tier, math::mean(success) ... GROUP BY model_tier, category` |\n| Top-K similar patterns | \u274c Must build custom index | \u2705 `WHERE embedding <|3|> $query_vec` |\n| Temporal queries | \u26a0\ufe0f Manual timestamp keys | \u2705 SurrealKV time-travel (experimental) |\n\n**Direct RocksDB wins on:** simplicity for simple KV, zero overhead, existing code reuse.\n**SurrealDB wins on:** every cross-issue query the Error KB needs.\n\n---\n\n### 4. Feature Comparison for Archival Memory\n\n| Requirement | Direct RocksDB | SurrealDB |\n|-------------|---------------|-----------|\n| Store per-bead memories | \u2705 CF + `mem:{bead_id}:{type}:{ts}` | \u2705 `CREATE memory SET bead_id = ...` |\n| Semantic recall | \u274c Must build vector index | \u2705 MTREE/HNSW + cosine similarity |\n| Failed approach index | \u26a0\ufe0f Manual compound keys | \u2705 `SELECT * FROM memory WHERE bead_id = $id AND outcome = 'failed'` |\n| Memory promotion | \u26a0\ufe0f Manual update + re-index | \u2705 `UPDATE memory SET priority += 1 WHERE ...` |\n| Summarization storage | \u2705 Simple KV | \u2705 `UPDATE memory SET summary = ...` |\n| Cross-bead pattern matching | \u274c Full scan required | \u2705 Graph traversal + vector search |\n| Letta-style tiered memory | \u26a0\ufe0f Build from scratch | \u2705 Natural fit: working=WorkPacket, archival=SurrealDB |\n\n---\n\n### 5. Operational Comparison\n\n| Aspect | Direct RocksDB (keep existing) | SurrealDB (add new) |\n|--------|-------------------------------|---------------------|\n| Dependencies | Already in Cargo.toml | +surrealdb crate (~slow compile due to C++ RocksDB dep) |\n| Data location | Local file (same process) | Network (ai-proxy LXC) OR embedded |\n| Backup | RocksDB checkpoint | SurrealDB export + RocksDB backup |\n| Monitoring | Manual | SurrealDB health endpoint |\n| Schema evolution | Manual migration code | SurrealQL `DEFINE TABLE/FIELD` |\n| Multi-agent access | \u26a0\ufe0f Single process only (RwLock) | \u2705 Network API, concurrent connections |\n| Failure isolation | Crash takes down coordination | Separate process, can restart independently |\n\n**Critical finding on multi-agent access:** The current RocksDB store uses `RwLock<DB>` \u2014 only one process can open it. When the swarm scales to multiple agents on different nodes, they can't share a direct RocksDB instance. SurrealDB's network API solves this inherently.\n\n---\n\n### 6. Architecture Recommendation: Hybrid\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  coordination process       \u2502     \u2502  ai-proxy LXC            \u2502\n\u2502                             \u2502     \u2502                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Direct RocksDB  \u2502        \u2502     \u2502  \u2502 SurrealDB          \u2502  \u2502\n\u2502  \u2502 (hot-path state)\u2502        \u2502     \u2502  \u2502 (RocksDB backend)  \u2502  \u2502\n\u2502  \u2502                 \u2502        \u2502     \u2502  \u2502                    \u2502  \u2502\n\u2502  \u2502 \u2022 sessions      \u2502        \u2502     \u2502  \u2502 \u2022 error_pattern KB \u2502  \u2502\n\u2502  \u2502 \u2022 tasks         \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2502 \u2022 archival_memory  \u2502  \u2502\n\u2502  \u2502 \u2022 results       \u2502 query  \u2502     \u2502  \u2502 \u2022 fix_strategies   \u2502  \u2502\n\u2502  \u2502 \u2022 voting        \u2502 on     \u2502     \u2502  \u2502 \u2022 model_stats      \u2502  \u2502\n\u2502  \u2502 \u2022 context       \u2502 issue  \u2502     \u2502  \u2502 \u2022 crate_graph      \u2502  \u2502\n\u2502  \u2502 \u2022 events        \u2502 start  \u2502     \u2502  \u2502 \u2022 code_embeddings  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                             \u2502     \u2502    :8000 HTTP/WS API     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Keep in direct RocksDB (hot path, per-issue):**\n- Ensemble sessions, tasks, results, voting, context, events\n- These are written/read hundreds of times per compilation cycle\n- Microsecond latency matters here\n- Single-process access is fine (one coordination MCP server)\n\n**Move to SurrealDB (knowledge path, cross-issue):**\n- Error Pattern KB (beefcake-swarm-3r9) \u2192 `error_pattern` table + graph edges\n- Archival Memory (beefcake-swarm-b30) \u2192 `memory` table + vector index\n- Model performance stats \u2192 `model_stats` table with aggregations\n- Code structure graph (future UKG) \u2192 full graph model\n\n**Why not embedded SurrealDB?**\n1. Multi-agent access: future swarm agents on different nodes need shared KB\n2. Failure isolation: KB crash shouldn't kill coordination loop\n3. ai-proxy LXC has the resources and is already the gateway node\n4. Operational clarity: one SurrealDB instance serves the whole cluster\n\n---\n\n### 7. SurrealDB Schema Design for Error KB\n\n```sql\n-- Error Pattern KB\nDEFINE TABLE error_pattern SCHEMAFULL;\nDEFINE FIELD category ON error_pattern TYPE string;  -- ErrorCategory enum\nDEFINE FIELD rustc_code ON error_pattern TYPE option<string>;  -- E0505, E0106, etc.\nDEFINE FIELD key_tokens ON error_pattern TYPE array<string>;  -- ['cannot borrow', 'mutable']\nDEFINE FIELD original_code ON error_pattern TYPE string;\nDEFINE FIELD fix_diff ON error_pattern TYPE string;\nDEFINE FIELD strategy ON error_pattern TYPE string;\nDEFINE FIELD model_tier ON error_pattern TYPE string;  -- strand/hydra/behemoth/council\nDEFINE FIELD iterations ON error_pattern TYPE int;\nDEFINE FIELD success ON error_pattern TYPE bool;\nDEFINE FIELD embedding ON error_pattern TYPE option<array<float>>;  -- for vector similarity\nDEFINE FIELD created_at ON error_pattern TYPE datetime DEFAULT time::now();\n\n-- Indexes\nDEFINE INDEX idx_category ON error_pattern FIELDS category;\nDEFINE INDEX idx_rustc_code ON error_pattern FIELDS rustc_code;\nDEFINE INDEX idx_embedding ON error_pattern FIELDS embedding MTREE DIMENSION 384 DIST COSINE TYPE F32;\n\n-- Archival Memory (Letta/MemGPT pattern)\nDEFINE TABLE memory SCHEMAFULL;\nDEFINE FIELD bead_id ON memory TYPE string;\nDEFINE FIELD memory_type ON memory TYPE string;  -- diff, verifier_report, error, decision, note\nDEFINE FIELD content ON memory TYPE string;\nDEFINE FIELD summary ON memory TYPE option<string>;\nDEFINE FIELD outcome ON memory TYPE option<string>;  -- success, failed, partial\nDEFINE FIELD priority ON memory TYPE int DEFAULT 0;\nDEFINE FIELD embedding ON memory TYPE option<array<float>>;\nDEFINE FIELD created_at ON memory TYPE datetime DEFAULT time::now();\n\nDEFINE INDEX idx_bead ON memory FIELDS bead_id;\nDEFINE INDEX idx_type ON memory FIELDS memory_type;\nDEFINE INDEX idx_mem_embedding ON memory FIELDS embedding MTREE DIMENSION 384 DIST COSINE TYPE F32;\n\n-- Graph edges: error \u2192 fixed_by \u2192 fix_strategy\nDEFINE TABLE fixed_by SCHEMAFULL;\nDEFINE FIELD strategy_used ON fixed_by TYPE string;\nDEFINE FIELD confidence ON fixed_by TYPE float;\n\n-- Model performance tracking\nDEFINE TABLE model_stats SCHEMAFULL;\nDEFINE FIELD model_id ON model_stats TYPE string;\nDEFINE FIELD category ON model_stats TYPE string;\nDEFINE FIELD attempts ON model_stats TYPE int DEFAULT 0;\nDEFINE FIELD successes ON model_stats TYPE int DEFAULT 0;\nDEFINE FIELD avg_iterations ON model_stats TYPE float DEFAULT 0.0;\nDEFINE FIELD updated_at ON model_stats TYPE datetime DEFAULT time::now();\n\nDEFINE INDEX idx_model_cat ON model_stats FIELDS model_id, category UNIQUE;\n```\n\n### 8. Example Queries\n\n```sql\n-- Find similar error patterns (top 3 by vector similarity)\nSELECT *, vector::similarity::cosine(embedding, $query_embedding) AS sim\nFROM error_pattern\nWHERE category = 'BorrowChecker'\n  AND embedding <|3|> $query_embedding;\n\n-- Model success rate by category (for routing decisions)\nSELECT model_id, category, \n       (successes * 1.0 / attempts) AS success_rate,\n       avg_iterations\nFROM model_stats\nWHERE attempts > 5\nORDER BY success_rate DESC;\n\n-- Recall failed approaches for a bead (Letta pattern)\nSELECT content, summary, outcome\nFROM memory\nWHERE bead_id = $current_bead\n  AND outcome = 'failed'\nORDER BY created_at DESC\nLIMIT 5;\n\n-- Graph: what fixes worked for errors in similar crates?\nSELECT ->fixed_by->error_pattern.strategy AS strategies\nFROM error_pattern\nWHERE category = $category\n  AND rustc_code = $code;\n```\n\n### 9. Migration Path\n\n1. **Phase 1 (now):** Deploy SurrealDB on ai-proxy (beefcake-swarm-3is.1.2)\n2. **Phase 2:** Implement Error KB in SurrealDB (beefcake-swarm-3is.5.2) \u2014 new code, no migration\n3. **Phase 3:** Implement Archival Memory in SurrealDB (beefcake-swarm-3is.5.4) \u2014 new code, no migration  \n4. **Phase 4:** Keep existing RocksDB for ensemble coordination (no change to store.rs)\n5. **Future:** If coordination needs multi-agent access, migrate remaining state to SurrealDB\n\n### 10. Risk Mitigation\n\n| Risk | Mitigation |\n|------|-----------|\n| SurrealDB latency on hot path | Don't use it for hot path \u2014 keep direct RocksDB |\n| SurrealDB instability (young project) | Isolated on ai-proxy, can fallback to direct RocksDB KB |\n| Network failure (ai-proxy down) | Degrade gracefully \u2014 skip KB lookup, use empty playbooks |\n| Embedding generation overhead | Use small model (384-dim) or hash-based signatures initially |\n| Schema evolution | SurrealQL supports `DEFINE FIELD` additions without migration |\n\n### 11. Conclusion\n\nThe hybrid approach gives us the best of both worlds:\n- **Direct RocksDB** stays for what it does well: fast, simple, per-issue state (\u03bcs latency)\n- **SurrealDB** handles what RocksDB can't: graph traversals, vector similarity, aggregations, multi-agent access, and the full UKG vision\n\nBoth beefcake-swarm-3r9 (Error KB) and beefcake-swarm-b30 (Archival Memory) should target SurrealDB, NOT add more column families to the existing RocksDB store. The existing store.rs remains untouched.\n\n### Sources\n- SurrealDB Architecture: https://surrealdb.com/docs/surrealdb/introduction/architecture\n- SurrealDB Graph Model: https://surrealdb.com/docs/surrealdb/models/graph\n- SurrealDB Vector Model: https://surrealdb.com/docs/surrealdb/models/vector\n- RELATE Statement: https://surrealdb.com/docs/surrealql/statements/relate\n- Performance Best Practices: https://surrealdb.com/docs/surrealdb/reference-guide/performance-best-practices\n- Embedded SurrealDB Perf Issue: https://github.com/surrealdb/surrealdb/issues/4767\n- SurrealDB Scalability: https://surrealdb.com/blog/surrealdb-scalability\n- crud-bench: https://github.com/surrealdb/crud-bench\n- MemGPT Paper: https://arxiv.org/abs/2310.08560\n- Letta/MemGPT Docs: https://docs.letta.com/concepts/memgpt/\n- Long-Term Memory Patterns: https://serokell.io/blog/design-patterns-for-long-term-memory-in-llm-powered-architectures", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:44Z", "closed_at": "2026-02-13T08:34:41Z", "close_reason": "Research complete. Recommendation: Hybrid architecture \u2014 keep direct RocksDB for hot-path ensemble state, use networked SurrealDB on ai-proxy for Error KB and Archival Memory. Full comparison, schema design, and migration path documented in notes.", "labels": ["delegate:pal-consensus", "epic:ukg", "research"], "dependencies": [{"issue_id": "beefcake-swarm-3is.1.5", "depends_on_id": "beefcake-swarm-3is.1", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 3, "comment_count": 0}
{"id": "beefcake-swarm-3is.2.1", "title": "Research codegraph-rust capabilities", "description": "Deep-dive jakedismo/codegraph-rust: node/edge types, SurrealDB storage support (look for surrealdb_storage.rs), incremental update support, tree-sitter dependency, performance on ~21K LOC Rust codebase. Determine if fork needed.", "notes": "PAL chat with g3-pro (Librarian). Output: feature matrix + config example. Check GitHub repo for SurrealDB native support vs needing a custom adapter.\n## codegraph-rust Research Findings (2026-02-13)\n\n### Overview\n- **Repo**: github.com/Jakedismo/codegraph-rust (73 stars, 8 forks)\n- **Language**: 100% Rust, multi-crate workspace\n- **License**: Open source\n- **Key Tech**: tree-sitter + FastML parsing, RocksDB + FAISS (primary), SurrealDB (experimental graph backend)\n\n### Dual Storage Backends\n\n**Primary: RocksDB + FAISS**\n- Graph structure stored in RocksDB (embedded KV store)\n- Vector similarity via FAISS\n- High performance, no external DB dependency\n- Supports incremental updates (reprocess only changed files)\n- FAISS limitation: adding new vectors may require partial index rebuild\n\n**Experimental: SurrealDB Graph Backend**\n- File: `crates/codegraph-graph/src/surrealdb_storage.rs` (CONFIRMED EXISTS)\n- Uses official SurrealDB Rust SDK (`Surreal<Any>` type)\n- Connects via WebSocket (ws://localhost:3004 default)\n- Namespace: \"ouroboros\", Database: \"codegraph\"\n- Schema files: `schema/codegraph.surql` (relational), `schema/codegraph_graph_experimental.surql` (graph)\n- Supports auth (username/password)\n- Migrations: `crates/codegraph-graph/src/surrealdb_migrations.rs`\n\n### SurrealDB Tables\n| Table | Purpose |\n|-------|---------|\n| `nodes` | Code entities (functions, classes, methods, variables) |\n| `edges` | Relationships (calls, imports, inherits, contains, defines, uses, flows_to, returns, mutates, extends, implements, references, depends_on, violates_boundary \u2014 ~20 types) |\n| `chunks` | Text chunks linked to parent nodes |\n| `file_metadata` | Per-file stats (content_hash, language, node_count, edge_count) |\n| `symbol_embeddings` | Cached identifier-level embeddings |\n| `project_metadata` | Project-level metadata |\n\n### Embedding Support\n- Multi-dimensional: 384, 768, 1024, 1536, 2048, 3072, 4096\n- HNSW indexes on all embedding dimensions (DIST COSINE, EFC 100, M 16)\n- Providers: Ollama, LM Studio, OpenAI, Jina AI\n- Full-text search indexes on `content` and `name` fields\n\n### Node Types\nFunction, Class, Method, Variable (core AST types from tree-sitter)\n\n### Edge Types (~20 total)\ncalls, defines, imports, uses, extends, implements, references, contains, inherits, flows_to, returns, mutates, depends_on, violates_boundary, and more\n\n### Indexing Tiers\n| Tier | Features | Use Case |\n|------|----------|----------|\n| fast | AST nodes, core edges only. Disables build context, LSP, enrichment, module linking, dataflow | Quick indexing, CI |\n| balanced | + build context, LSP symbols, enrichment, module linking, docs/contracts | Development use |\n| full | All analyzers, LSP definitions, dataflow, architecture analysis, no edge filtering | Complete analysis |\n\n### MCP Tools (4 agentic tools)\n1. **agentic_context** \u2014 Semantic search, context building, question answering. Focus: search|builder|question\n2. **agentic_impact** \u2014 Dependency chains, call flows, change impact. Focus: dependencies|call_chain\n3. **agentic_architecture** \u2014 System structure, API surfaces, architectural patterns. Focus: structure|api_surface\n4. **agentic_quality** \u2014 Complexity hotspots, coupling metrics, refactoring priorities. Focus: complexity|coupling|hotspots\n\nAll tools run reasoning agents internally (plan \u2192 search \u2192 analyze graph \u2192 synthesize).\n\n### SurrealDB Graph Functions\n- `semantic_search_nodes_via_chunks` \u2014 Vector search over code chunks\n- `get_transitive_dependencies` \u2014 Transitive dependency resolution\n- `trace_call_chain` \u2014 Call chain tracing\n- `calculate_coupling_metrics` \u2014 Module coupling analysis\n\n### Performance Reference\n- Sample run: 1,505 files, 2.4M lines, 30K functions, 880 classes, 539K embeddings\n- HNSW query: 2-5ms\n- Supports 14 languages via tree-sitter: Rust, Python, TypeScript, JavaScript, Go, Java, C++, C, Swift, Kotlin, C#, Ruby, PHP, Dart\n\n### Agent Architecture\n- Uses **Rig** framework (same as our swarm-agents crate!)\n- Reasoning strategies: LATS (tree search), ReAct (linear), Reflexion (self-correcting)\n- Context-adaptive: adjusts depth based on LLM context window size\n\n### Assessment: Fork Needed?\n**Likely NO fork needed** \u2014 codegraph-rust already has:\n- \u2705 Native SurrealDB storage backend (`surrealdb_storage.rs`)\n- \u2705 SurrealDB graph experimental schema\n- \u2705 Multi-dimensional embeddings compatible with our Nomic 3584-dim (closest: 4096)\n- \u2705 Rust language with advanced analysis\n- \u2705 Rig framework (same as our codebase)\n- \u2705 MCP server for tool exposure\n- \u2705 Incremental indexing\n\n**Potential customization points (config, not fork):**\n- Embedding dimension: may need to add 3584 HNSW index to schema (currently supports 384-4096)\n- SurrealDB connection config: point to our SurrealDB instance\n- Indexing tier: use \"full\" for ~21K LOC codebase (small enough)\n- Could run as MCP server alongside our coordination MCP\n\n**Risk factors:**\n- SurrealDB backend is labeled \"experimental\"\n- 73 stars = small community, may need to contribute fixes upstream\n- FAISS incremental updates have limitations (partial rebuild needed)\n- No published benchmarks for Rust-specific analysis quality", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:19Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:44Z", "closed_at": "2026-02-13T08:33:01Z", "close_reason": "Research complete. codegraph-rust has native SurrealDB support via experimental backend. No fork needed \u2014 configure connection and add 3584-dim HNSW index.", "labels": ["delegate:g3-pro", "epic:ukg", "research"], "dependencies": [{"issue_id": "beefcake-swarm-3is.2.1", "depends_on_id": "beefcake-swarm-3is.2", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.3.1", "title": "Research CocoIndex custom Target for SurrealDB", "description": "Research CocoIndex Target interface for custom SurrealDB target. Can we run dual-write (Postgres + SurrealDB) in a single flow? Validate against actual CocoIndex v1 API (not the outdated sketch in COCOINDEX_GRAPH_RAG.md).", "notes": "PAL consensus. The existing indexing/index_flow_v2.py uses @flow_def pattern. Output: validated implementation approach. Check CocoIndex docs for custom target/sink interface.\n## CocoIndex Custom SurrealDB Target \u2014 Research Findings (2026-02-13)\n\n### Current Pipeline Analysis\n- File: `indexing/index_flow_v2.py`\n- Pattern: `@flow_def(name=\"code_indexing\")` with `FlowBuilder` + `DataScope`\n- Source: `LocalFile` from rust-daq repo\n- Transform: `DetectProgrammingLanguage` \u2192 `SplitRecursively` (1000 chars) \u2192 `EmbedText` (Nomic 3584-dim via vasp-02)\n- Target: `Postgres` with pgvector HNSW index (cosine similarity)\n- Live updates: 30-second refresh interval\n- Query handler: `semantic_search` over `code_chunks` table\n\n### COCOINDEX_GRAPH_RAG.md Assessment\n**OUTDATED** \u2014 The document references `cocoindex.typing.Target` and `batch_write()` method, which do NOT match the current CocoIndex v1 API. The actual API uses:\n- `cocoindex.op.TargetSpec` (not `cocoindex.typing.Target`)\n- `@cocoindex.op.target_connector(spec_cls=...)` decorator\n- `mutate()` method (not `batch_write()`)\n- `apply_setup_change()` for infrastructure lifecycle\n- `get_persistent_key()` for target identity\n\n### CocoIndex v1 Custom Target API (Validated)\n\n#### 1. TargetSpec (Configuration)\n```python\nclass SurrealDBTarget(cocoindex.op.TargetSpec):\n    url: str           # ws://localhost:8000\n    namespace: str     # \"ouroboros\"\n    database: str      # \"codegraph\"\n    table_name: str    # \"doc_chunks\"\n    auth_ref: cocoindex.AuthEntryReference | None = None\n```\n\n#### 2. TargetConnector (Implementation)\n```python\n@cocoindex.op.target_connector(spec_cls=SurrealDBTarget)\nclass SurrealDBTargetConnector:\n    @staticmethod\n    def get_persistent_key(spec, target_name) -> str:\n        return f\"{spec.url}/{spec.namespace}/{spec.database}/{spec.table_name}\"\n\n    @staticmethod\n    def apply_setup_change(key, previous, current):\n        # Create/delete SurrealDB table + HNSW indexes\n        if previous is None and current is not None:\n            # CREATE TABLE, DEFINE INDEX for embeddings\n            pass\n        if previous is not None and current is None:\n            # DROP TABLE\n            pass\n\n    @staticmethod\n    def mutate(*all_mutations):\n        for spec, mutations in all_mutations:\n            for key, value in mutations.items():\n                if value is None:\n                    # DELETE chunk:key\n                    pass\n                else:\n                    # UPSERT chunk:key CONTENT {...}\n                    pass\n```\n\n#### 3. Key Methods\n| Method | Purpose |\n|--------|---------|\n| `get_persistent_key()` | Returns unique identifier for target instance |\n| `apply_setup_change()` | Creates/destroys target infrastructure (tables, indexes) |\n| `mutate()` | Applies data changes (insert/update/delete) \u2014 REQUIRED |\n| `prepare()` | Optional: pre-connect, validate, cache connections |\n| `describe()` | Optional: logging/debugging |\n\n### Dual-Write Architecture: CONFIRMED POSSIBLE\n\nCocoIndex supports multiple collectors and multiple export() calls in a single flow. Two approaches:\n\n#### Option A: Two Collectors (Recommended)\n```python\n@flow_def(name=\"code_indexing\")\ndef my_flow(builder, scope):\n    files = builder.add_source(LocalFile(...))\n    row = files.row()\n    row[\"language\"] = row[\"filename\"].transform(DetectProgrammingLanguage())\n    row[\"chunks\"] = row[\"content\"].transform(SplitRecursively(), ...)\n    chunks = row[\"chunks\"].row()\n    chunks[\"embedding\"] = text_to_embedding(chunks[\"text\"])\n\n    # Collector 1: Postgres (existing)\n    pg_collector = scope.add_collector()\n    pg_collector.collect(filename=row[\"filename\"], ...)\n    pg_collector.export(\"postgres_export\", Postgres(...), ...)\n\n    # Collector 2: SurrealDB (new)\n    surreal_collector = scope.add_collector()\n    surreal_collector.collect(filename=row[\"filename\"], ...)\n    surreal_collector.export(\"surreal_export\", SurrealDBTarget(...), ...)\n```\n\n#### Option B: Single Collector, Two Exports\nMay also work \u2014 two export() calls from same collector. Less documented but architecturally plausible.\n\n### SurrealDB Python SDK\n- Package: `surrealdb` on PyPI (v1.0.8, Jan 2026)\n- Protocols: HTTP, WebSocket, embedded\n- Async + sync support\n- CBOR serialization (binary, efficient)\n- Compatible: SurrealDB v2.0.0 \u2014 v2.3.6\n- Mature: 227 tests, 100% coverage, Python 3.9-3.13\n\n### Built-in Target Precedent\nCocoIndex already has a **Neo4j** target (graph database), proving graph DB targets are first-class citizens. The SurrealDB target follows the same pattern:\n- Nodes \u2192 `collector.export(Neo4j(mapping=Nodes(label=\"...\")))`\n- Could do similar: `collector.export(SurrealDBTarget(table_name=\"doc_chunks\"))`\n\n### Implementation Approach (Recommended)\n\n1. **Create `indexing/targets/surreal_target.py`** \u2014 Custom TargetSpec + Connector\n2. **SurrealDB connection**: Use `surrealdb` Python SDK with WebSocket\n3. **Schema**: `apply_setup_change()` creates table with HNSW vector indexes matching codegraph-rust schema (embedding_3584 field, COSINE distance)\n4. **Mutations**: `mutate()` does UPSERT/DELETE via SurrealQL parameterized queries\n5. **Dual-write**: Add second collector to existing flow in `index_flow_v2.py`\n6. **Bridge edges**: Post-process to create `documents` edges between doc chunks and code nodes (from codegraph-rust)\n\n### Schema Alignment with codegraph-rust\nThe SurrealDB target should write to tables compatible with codegraph-rust's experimental schema:\n- Table: `chunks` (matching codegraph-rust's chunk table)\n- Fields: `project_id`, `parent_node`, `text`, `embedding_3584` (new dimension)\n- Or separate table: `doc_chunks` to avoid collision with code chunks\n\n### Risk Assessment\n| Risk | Mitigation |\n|------|------------|\n| SurrealDB Python SDK maturity | Stable v1.0.8, well-tested |\n| Custom Target API stability | CocoIndex v1 API, blog + docs consistent |\n| Dual-write performance | Async SurrealDB writes, CocoIndex handles batching |\n| Schema collision with codegraph-rust | Use separate table (`doc_chunks`) or namespace |\n| 3584-dim not in codegraph schema | Add custom HNSW index definition |\n\n### Conclusion\n**Dual-write is fully supported and straightforward.** The custom target requires ~100 lines of Python. The existing pipeline needs minimal modification (add second collector + export). No fork of CocoIndex needed \u2014 the custom target API is designed for exactly this use case.", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:44Z", "closed_at": "2026-02-13T08:35:55Z", "close_reason": "Research complete. CocoIndex custom Target API validated \u2014 dual-write (Postgres + SurrealDB) confirmed possible via multiple collectors. Custom SurrealDB target needs ~100 LOC Python. COCOINDEX_GRAPH_RAG.md is outdated and needs updating.", "labels": ["delegate:pal-consensus", "epic:ukg", "research"], "dependencies": [{"issue_id": "beefcake-swarm-3is.3.1", "depends_on_id": "beefcake-swarm-3is.3", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.4.1", "title": "Design KG query interface for WorkPacket enrichment", "description": "Design how KG queries populate WorkPacket.relevant_heuristics and relevant_playbooks. Define: SurrealQL queries per error category, token budget allocation (2K implementer, 4K integrator, 8K cloud), integration point in pipeline (pack_retry vs generate). Output: Rust trait + SurrealQL query library.", "notes": "PAL consensus \u2014 this is THE critical architectural decision. All 3 frontier models weigh in. Files: coordination/src/context_packer/packer.rs, coordination/src/work_packet/types.rs\n# KG Query Interface Design for WorkPacket Enrichment\n\n**Author**: Opus 4.6 (direct analysis \u2014 PAL MCP consensus unavailable in teammate env)\n**Date**: 2026-02-13\n**Status**: DESIGN COMPLETE\n\n---\n\n## 1. Executive Summary\n\nThe WorkPacket struct has two empty fields \u2014 `relevant_heuristics` and `relevant_playbooks` \u2014 that should be populated by querying a SurrealDB knowledge graph containing code structure, documentation embeddings, bridge edges, and fix patterns from past successful error resolutions. This design defines the Rust trait interface, SurrealQL query templates, token budget allocation, integration point, and degradation strategy.\n\n---\n\n## 2. Trait Definition: `KgEnricher`\n\n```rust\n// coordination/src/kg/enricher.rs\n\nuse crate::escalation::state::SwarmTier;\nuse crate::feedback::error_parser::ErrorCategory;\nuse crate::work_packet::types::KeySymbol;\n\n/// Result of knowledge graph enrichment for a WorkPacket.\n/// All fields are populated on best-effort basis; empty = no enrichment available.\n#[derive(Debug, Clone, Default)]\npub struct KgEnrichment {\n    /// General heuristics: short actionable rules (1-2 sentences each).\n    /// Example: \"E0382 (use after move) \u2014 consider Clone, Rc<T>, or ownership restructuring\"\n    pub heuristics: Vec<String>,\n    /// Playbooks: step-by-step procedures from past successful fixes.\n    /// Example: \"BorrowChecker cascade in parser.rs: (1) Made field Clone (2) Changed &mut \u2192 &self...\"\n    pub playbooks: Vec<String>,\n    /// Additional symbols discovered via graph traversal (callers/callees/trait impls).\n    pub related_symbols: Vec<KeySymbol>,\n    /// Wall-clock time spent querying KG, in milliseconds.\n    pub query_time_ms: u64,\n    /// Whether KG was reachable. False = degraded mode, all vecs empty.\n    pub kg_available: bool,\n}\n\nimpl KgEnrichment {\n    /// Estimate token count of heuristics + playbooks combined.\n    /// Uses 4 chars \u2248 1 token heuristic, matching WorkPacket::estimated_tokens().\n    pub fn estimated_tokens(&self) -> usize {\n        let chars: usize = self.heuristics.iter().map(|h| h.len()).sum::<usize>()\n            + self.playbooks.iter().map(|p| p.len()).sum::<usize>();\n        chars / 4\n    }\n\n    /// Trim to fit within a token budget by dropping lowest-priority items.\n    /// Heuristics are trimmed from the end first (least relevant),\n    /// then playbooks from the end.\n    pub fn trim_to_budget(&mut self, max_tokens: usize) {\n        while self.estimated_tokens() > max_tokens && !self.heuristics.is_empty() {\n            self.heuristics.pop();\n        }\n        while self.estimated_tokens() > max_tokens && !self.playbooks.is_empty() {\n            self.playbooks.pop();\n        }\n    }\n}\n\n/// Trait for querying the knowledge graph to enrich WorkPackets.\n///\n/// Implementations handle SurrealDB connectivity, query construction,\n/// timeout enforcement, and graceful degradation internally.\n/// The caller never sees an error \u2014 degraded mode returns empty enrichment.\n#[async_trait::async_trait]\npub trait KgEnricher: Send + Sync {\n    /// Query the KG for heuristics and playbooks relevant to the current error context.\n    ///\n    /// # Arguments\n    /// - `error_categories`: Active error types from VerifierReport\n    /// - `error_codes`: Specific rustc error codes (E0382, E0277, etc.)\n    /// - `affected_files`: Files with errors or modifications\n    /// - `key_symbols`: Symbols extracted from affected files\n    /// - `tier`: Target model tier (determines token budget and timeout)\n    /// - `iteration`: Current iteration number (higher = more aggressive retrieval)\n    ///\n    /// # Returns\n    /// KgEnrichment with heuristics and playbooks, already trimmed to tier budget.\n    /// On KG unavailability, returns KgEnrichment::default() with kg_available=false.\n    async fn enrich(\n        &self,\n        error_categories: &[ErrorCategory],\n        error_codes: &[String],\n        affected_files: &[String],\n        key_symbols: &[KeySymbol],\n        tier: SwarmTier,\n        iteration: u32,\n    ) -> KgEnrichment;\n}\n```\n\n### NoopKgEnricher (for testing / Phase 3 migration)\n\n```rust\n/// No-op implementation that returns empty enrichment.\n/// Use during testing or before SurrealDB is deployed.\npub struct NoopKgEnricher;\n\n#[async_trait::async_trait]\nimpl KgEnricher for NoopKgEnricher {\n    async fn enrich(\n        &self, _: &[ErrorCategory], _: &[String], _: &[String],\n        _: &[KeySymbol], _: SwarmTier, _: u32,\n    ) -> KgEnrichment {\n        KgEnrichment {\n            kg_available: false,\n            ..Default::default()\n        }\n    }\n}\n```\n\n---\n\n## 3. SurrealQL Query Templates\n\n### 3.1 Fix Pattern Retrieval (highest value)\n\n```surql\n-- Query A: Find past successful fixes for matching error categories\n-- Input: $error_categories (array of strings), $symbols (array of strings), $file_stem (string)\n-- Ordered by: match quality (category + symbol overlap) \u00d7 success_count\n\nSELECT\n    id,\n    error_category,\n    error_code,\n    affected_symbol,\n    file_path,\n    fix_description,\n    fix_diff_summary,\n    success_count,\n    last_applied\nFROM fix_pattern\nWHERE error_category IN $error_categories\nORDER BY\n    (IF affected_symbol IN $symbols THEN 10 ELSE 0 END)\n    + (IF string::contains(file_path, $file_stem) THEN 5 ELSE 0 END)\n    + success_count\n    DESC\nLIMIT $max_patterns;\n```\n\n**`$max_patterns`** per tier: Implementer=3, Integrator=5, Cloud=8\n\n### 3.2 Symbol Neighborhood (graph traversal)\n\n```surql\n-- Query B: Find call graph neighborhood of affected symbols\n-- Returns callers, callees, trait implementations, and containing module\n\nSELECT\n    name,\n    kind,\n    file_path,\n    line_number,\n    <-calls<-symbol.name AS callers,\n    ->calls->symbol.name AS callees,\n    ->implements->symbol.name AS trait_impls,\n    <-defines<-symbol.name AS parent_mod\nFROM symbol\nWHERE name IN $symbols\nLIMIT 10;\n```\n\n### 3.3 Documentation Semantic Search\n\n```surql\n-- Query C: Vector similarity search on doc chunks\n-- $query_embedding is computed from: error_message + affected_symbol names\n-- Uses 3584-dim embeddings (Nomic Embed V2 or similar)\n\nSELECT\n    id,\n    title,\n    content,\n    source_file,\n    vector::similarity::cosine(embedding, $query_embedding) AS relevance_score\nFROM doc_chunk\nWHERE vector::similarity::cosine(embedding, $query_embedding) > 0.65\nORDER BY relevance_score DESC\nLIMIT $max_docs;\n```\n\n**`$max_docs`** per tier: Implementer=1, Integrator=2, Cloud=4\n\n### 3.4 Bridge Edge Lookup (docs \u2194 code)\n\n```surql\n-- Query D: Find documentation linked to affected symbols\n-- Bridge edges connect code symbols to their doc chunks\n\nSELECT\n    <-documents<-doc_chunk.{id, title, content} AS docs,\n    name,\n    kind\nFROM symbol\nWHERE name IN $symbols\n    AND count(->documented_by->doc_chunk) > 0;\n```\n\n### 3.5 Error Code Pattern Lookup\n\n```surql\n-- Query E: Canonical fix patterns keyed by rustc error code\n-- These are curated heuristics, not learned from past fixes\n\nSELECT\n    error_code,\n    category,\n    heuristic_text,\n    common_fixes,\n    complexity_level\nFROM error_heuristic\nWHERE error_code IN $error_codes\nORDER BY frequency DESC;\n```\n\n---\n\n## 4. Heuristics vs Playbooks: Content Semantics\n\n### `relevant_heuristics: Vec<String>`\n\n**Definition**: Short, general, reusable rules. Each heuristic is 1-2 sentences describing a pattern or principle the model should consider.\n\n**Sources** (priority order):\n1. `error_heuristic` table (Query E) \u2014 curated per error code\n2. Semantic doc search (Query C) \u2014 distilled to actionable tips\n3. Bridge docs (Query D) \u2014 API usage notes linked to symbols\n\n**Format per entry**:\n```\n\"[ErrorCode] Category \u2014 Actionable guidance. Example: ...\"\n```\n\n**Examples**:\n```\n\"[E0382] BorrowChecker \u2014 Value used after move. Consider: (a) add .clone() if type is Clone, (b) use &ref instead of move, (c) restructure to avoid reuse after move.\"\n\"[E0277] TraitBound \u2014 Missing trait impl. Check if the bound is on a generic parameter you control (add where clause) vs a concrete type (impl the trait or use a wrapper).\"\n\"[E0106] Lifetime \u2014 Missing lifetime annotation. The compiler needs explicit lifetimes when a function returns a reference. Add lifetime parameter to fn signature and annotate the return type.\"\n```\n\n### `relevant_playbooks: Vec<String>`\n\n**Definition**: Multi-step procedures derived from past successful fixes in this codebase. Each playbook describes a specific fix sequence that resolved a similar error on similar code.\n\n**Sources** (priority order):\n1. `fix_pattern` table (Query A) \u2014 past successful fixes with highest success_count\n2. Symbol neighborhood context (Query B) \u2014 to qualify which playbook applies\n\n**Format per entry**:\n```\n\"[Pattern: <name>] Applied <N> times, last: <date>. Steps: (1)... (2)... (3)... Files: <files>\"\n```\n\n**Examples**:\n```\n\"[Pattern: borrow-cascade-parser] Applied 4 times, last: 2026-02-10. Steps: (1) Clone the borrowed field before entering the loop. (2) Use a local variable to hold the clone. (3) Replace all &mut self methods that only read with &self. Files: src/parser.rs, src/lexer.rs\"\n\"[Pattern: async-send-bound] Applied 2 times, last: 2026-02-08. Steps: (1) Box the future with Pin<Box<dyn Future + Send>>. (2) Add Send bound to the async trait method. (3) Ensure all captured variables are Send. Files: src/handler.rs\"\n```\n\n---\n\n## 5. Token Budget Allocation\n\nToken budgets for KG enrichment are a SUBSET of the total WorkPacket context budget.\n\n| Tier | Total Context Budget | KG Enrichment Budget | Heuristic Allocation | Playbook Allocation | Timeout |\n|------|---------------------|---------------------|---------------------|--------------------|---------| \n| Implementer | 8,000 tok | 2,000 tok | 500 tok (~2-3 items) | 1,500 tok (~2-3 items) | 500ms |\n| Integrator | 24,000 tok | 4,000 tok | 1,500 tok (~5-6 items) | 2,500 tok (~4-5 items) | 1,000ms |\n| Adversary | 24,000 tok | 4,000 tok | 2,000 tok (~6-8 items) | 2,000 tok (~3-4 items) | 1,000ms |\n| Cloud | 32,000 tok | 8,000 tok | 3,000 tok (~10-12 items) | 5,000 tok (~6-8 items) | 2,000ms |\n\n**Rationale for allocation split**:\n- **Implementer** (14B): Gets proportionally MORE playbooks (concrete steps) because smaller models benefit more from explicit step-by-step instructions than general principles.\n- **Integrator** (72B): Balanced split \u2014 can use both principles and examples.\n- **Adversary** (80B MoE): More heuristics \u2014 adversarial review benefits from knowing the general patterns to check against.\n- **Cloud**: More playbooks \u2014 frontier models use concrete codebase examples to ground their architecture decisions in local context.\n\n**Iteration scaling**: On iteration > 1, increase budget by 25% (capped at tier max) to provide more context for stuck problems:\n\n```rust\nfn kg_token_budget(tier: SwarmTier, iteration: u32) -> usize {\n    let base = match tier {\n        SwarmTier::Implementer => 2_000,\n        SwarmTier::Integrator | SwarmTier::Adversary => 4_000,\n        SwarmTier::Cloud => 8_000,\n    };\n    if iteration > 1 {\n        (base as f64 * 1.25).min(base as f64 * 1.5) as usize\n    } else {\n        base\n    }\n}\n```\n\n---\n\n## 6. Integration Point: `ContextPacker::pack_retry()`\n\n### Decision: Enrich in `ContextPacker`, NOT in `WorkPacketGenerator`\n\n**Rationale**:\n- `WorkPacketGenerator::generate()` is synchronous (no async) \u2014 adding SurrealDB queries would require making it async, which cascades through the call chain.\n- `ContextPacker` is the orchestration layer that already knows the tier and manages token budgets.\n- Enrichment should happen AFTER `generate()` returns (so we have the packet's error categories and symbols) but BEFORE `trim_to_budget()` (so enrichment content participates in budget trimming).\n\n### Modified `ContextPacker`\n\n```rust\npub struct ContextPacker {\n    working_dir: PathBuf,\n    generator: WorkPacketGenerator,\n    file_walker: FileWalker,\n    tier: SwarmTier,\n    max_context_tokens: usize,\n    kg_enricher: Arc<dyn KgEnricher>,  // NEW: injected dependency\n}\n\nimpl ContextPacker {\n    pub fn new(working_dir: impl AsRef<Path>, tier: SwarmTier) -> Self {\n        Self::with_enricher(working_dir, tier, Arc::new(NoopKgEnricher))\n    }\n\n    pub fn with_enricher(\n        working_dir: impl AsRef<Path>,\n        tier: SwarmTier,\n        kg_enricher: Arc<dyn KgEnricher>,\n    ) -> Self {\n        let wd = working_dir.as_ref().to_path_buf();\n        Self {\n            generator: WorkPacketGenerator::new(&wd),\n            file_walker: FileWalker::new(&wd),\n            tier,\n            max_context_tokens: max_context_tokens(tier),\n            working_dir: wd,\n            kg_enricher,\n        }\n    }\n\n    /// Retry pack with KG enrichment.\n    pub async fn pack_retry(\n        &self,\n        bead_id: &str,\n        objective: &str,\n        escalation_state: &EscalationState,\n        verifier_report: &VerifierReport,\n    ) -> WorkPacket {\n        let mut packet = self.generator.generate(\n            bead_id, objective,\n            escalation_state.current_tier,\n            escalation_state,\n            Some(verifier_report),\n        );\n\n        // --- KG ENRICHMENT (new) ---\n        let error_codes: Vec<String> = verifier_report.failure_signals\n            .iter()\n            .filter_map(|s| s.code.clone())\n            .collect();\n\n        let enrichment = self.kg_enricher.enrich(\n            &packet.unique_error_categories(),\n            &error_codes,\n            &packet.files_touched,\n            &packet.key_symbols,\n            self.tier,\n            packet.iteration,\n        ).await;\n\n        packet.relevant_heuristics = enrichment.heuristics;\n        packet.relevant_playbooks = enrichment.playbooks;\n\n        // Add discovered related symbols to key_symbols (deduplicated)\n        for sym in enrichment.related_symbols {\n            if !packet.key_symbols.iter().any(|s| s.name == sym.name && s.file == sym.file) {\n                packet.key_symbols.push(sym);\n            }\n        }\n        // --- END KG ENRICHMENT ---\n\n        self.trim_to_budget(&mut packet);\n        packet\n    }\n}\n```\n\n### Key change: `pack_retry()` becomes `async`\n\nThis is the minimum-blast-radius change. `pack_initial()` can remain sync (no errors to enrich from). Callers of `pack_retry()` in the swarm orchestrator loop are already async (tokio runtime).\n\n---\n\n## 7. SurrealDB Enricher Implementation Sketch\n\n```rust\n// coordination/src/kg/surreal_enricher.rs\n\nuse surrealdb::Surreal;\nuse surrealdb::engine::remote::ws::Client;\n\npub struct SurrealKgEnricher {\n    db: Surreal<Client>,\n    /// Timeout per tier (set in constructor from tier budgets)\n    default_timeout: Duration,\n    /// LRU cache: (error_categories_hash, file_hash) \u2192 KgEnrichment\n    cache: Arc<Mutex<LruCache<u64, KgEnrichment>>>,\n}\n\nimpl SurrealKgEnricher {\n    pub async fn connect(url: &str, ns: &str, db_name: &str) -> Result<Self, SurrealError> {\n        let db = Surreal::new::<surrealdb::engine::remote::ws::Ws>(url).await?;\n        db.use_ns(ns).use_db(db_name).await?;\n        Ok(Self {\n            db,\n            default_timeout: Duration::from_millis(1000),\n            cache: Arc::new(Mutex::new(LruCache::new(NonZeroUsize::new(256).unwrap()))),\n        })\n    }\n}\n\n#[async_trait::async_trait]\nimpl KgEnricher for SurrealKgEnricher {\n    async fn enrich(\n        &self,\n        error_categories: &[ErrorCategory],\n        error_codes: &[String],\n        affected_files: &[String],\n        key_symbols: &[KeySymbol],\n        tier: SwarmTier,\n        iteration: u32,\n    ) -> KgEnrichment {\n        let timeout = match tier {\n            SwarmTier::Implementer => Duration::from_millis(500),\n            SwarmTier::Integrator | SwarmTier::Adversary => Duration::from_millis(1000),\n            SwarmTier::Cloud => Duration::from_millis(2000),\n        };\n\n        let budget = kg_token_budget(tier, iteration);\n\n        // Check cache first\n        let cache_key = hash_query_inputs(error_categories, affected_files, key_symbols);\n        if let Some(cached) = self.cache.lock().unwrap().get(&cache_key) {\n            return cached.clone();\n        }\n\n        // Run queries with timeout\n        let result = tokio::time::timeout(timeout, async {\n            let start = std::time::Instant::now();\n\n            // Run fix pattern + error heuristic queries in parallel\n            let (fix_patterns, error_heuristics) = tokio::join!(\n                self.query_fix_patterns(error_categories, key_symbols, affected_files, tier),\n                self.query_error_heuristics(error_codes),\n            );\n\n            // Build enrichment\n            let mut enrichment = KgEnrichment {\n                heuristics: error_heuristics.unwrap_or_default(),\n                playbooks: fix_patterns.unwrap_or_default(),\n                related_symbols: vec![], // populated by symbol neighborhood if budget allows\n                query_time_ms: start.elapsed().as_millis() as u64,\n                kg_available: true,\n            };\n\n            // If budget allows, also query symbol neighborhood\n            let heuristic_budget = budget * 2 / 5; // 40% heuristics\n            let playbook_budget = budget * 3 / 5;  // 60% playbooks\n            // (Implementer ratio inverted: 25% heuristics, 75% playbooks)\n            let (h_budget, p_budget) = match tier {\n                SwarmTier::Implementer => (budget / 4, budget * 3 / 4),\n                SwarmTier::Adversary => (budget / 2, budget / 2),\n                _ => (heuristic_budget, playbook_budget),\n            };\n\n            // Trim heuristics and playbooks separately\n            while enrichment.heuristics.iter().map(|h| h.len()).sum::<usize>() / 4 > h_budget\n                && !enrichment.heuristics.is_empty()\n            {\n                enrichment.heuristics.pop();\n            }\n            while enrichment.playbooks.iter().map(|p| p.len()).sum::<usize>() / 4 > p_budget\n                && !enrichment.playbooks.is_empty()\n            {\n                enrichment.playbooks.pop();\n            }\n\n            enrichment\n        }).await;\n\n        match result {\n            Ok(enrichment) => {\n                // Cache the result\n                self.cache.lock().unwrap().put(cache_key, enrichment.clone());\n                enrichment\n            }\n            Err(_timeout) => {\n                tracing::warn!(\n                    tier = %tier,\n                    \"KG enrichment timed out after {:?}, proceeding without enrichment\",\n                    timeout,\n                );\n                KgEnrichment {\n                    kg_available: false,\n                    ..Default::default()\n                }\n            }\n        }\n    }\n}\n```\n\n---\n\n## 8. Graceful Degradation Strategy\n\n### Failure Modes & Responses\n\n| Failure Mode | Detection | Response | Impact |\n|---|---|---|---|\n| SurrealDB unreachable | TCP connect timeout | Return `KgEnrichment::default()` with `kg_available=false` | Zero \u2014 matches current behavior |\n| Query timeout | `tokio::time::timeout` expires | Return partial results or empty | Minimal \u2014 packet generated without enrichment |\n| Malformed query results | Serde deserialization error | Log warning, skip that query | Other queries still contribute |\n| Empty KG (no data yet) | Queries return 0 rows | Return empty enrichment | Expected during initial deployment |\n| Cache hit | LRU cache lookup | Return cached enrichment (fast path) | Positive \u2014 reduces query load |\n\n### Degradation Hierarchy\n\n```\n1. Full KG enrichment (heuristics + playbooks + related symbols)\n2. Partial enrichment (some queries succeeded, others timed out)\n3. Cache-only (KG unreachable but cached result exists for similar query)\n4. No enrichment (KG unavailable, no cache hit) \u2014 identical to current behavior\n```\n\n### Key Design Principle\n\nThe `KgEnricher::enrich()` method returns `KgEnrichment` directly (NOT `Result<KgEnrichment, _>`). This is intentional: the caller never needs to handle errors. All error handling is internal to the implementation. The `kg_available` field provides observability without forcing error handling on the caller.\n\n### Monitoring\n\n```rust\n// Add to KgEnrichment for operational visibility\npub struct KgEnrichment {\n    // ... existing fields ...\n    /// Number of queries that succeeded out of total attempted\n    pub queries_succeeded: u8,\n    pub queries_attempted: u8,\n}\n```\n\nLog structured events for dashboarding:\n```\ntracing::info!(\n    tier = %tier,\n    iteration = iteration,\n    heuristic_count = enrichment.heuristics.len(),\n    playbook_count = enrichment.playbooks.len(),\n    kg_available = enrichment.kg_available,\n    query_time_ms = enrichment.query_time_ms,\n    \"KG enrichment complete\"\n);\n```\n\n---\n\n## 9. Module Layout\n\n```\ncoordination/src/kg/\n\u251c\u2500\u2500 mod.rs           // pub mod enricher; pub mod surreal_enricher;\n\u251c\u2500\u2500 enricher.rs      // KgEnricher trait + KgEnrichment + NoopKgEnricher\n\u251c\u2500\u2500 surreal_enricher.rs  // SurrealKgEnricher implementation\n\u251c\u2500\u2500 queries.rs       // SurrealQL query string constants\n\u2514\u2500\u2500 cache.rs         // LRU cache wrapper with hash helpers\n```\n\nAdd `mod kg;` to `coordination/src/lib.rs`.\n\n### Cargo.toml additions\n\n```toml\n[dependencies]\nsurrealdb = { version = \"2\", features = [\"kv-mem\"], optional = true }\nasync-trait = \"0.1\"\nlru = \"0.12\"\n\n[features]\ndefault = []\nkg = [\"surrealdb\"]\n```\n\nFeature-gated: `kg` feature enables SurrealDB enricher. Without it, only `NoopKgEnricher` is available. This keeps the build fast for developers who don't have SurrealDB running.\n\n---\n\n## 10. Query Pipeline Per Tier\n\n### Implementer (14B, 2K budget, 500ms timeout)\n\n```\nParallel:\n  \u251c\u2500\u2500 Query A: fix_patterns (LIMIT 3) \u2192 playbooks\n  \u2514\u2500\u2500 Query E: error_heuristics \u2192 heuristics\nSequential (if time remains):\n  \u2514\u2500\u2500 Query D: bridge docs for top symbol \u2192 append to heuristics\n```\n\n### Integrator (72B, 4K budget, 1000ms timeout)\n\n```\nParallel:\n  \u251c\u2500\u2500 Query A: fix_patterns (LIMIT 5) \u2192 playbooks\n  \u251c\u2500\u2500 Query E: error_heuristics \u2192 heuristics\n  \u2514\u2500\u2500 Query B: symbol neighborhood \u2192 related_symbols\nSequential (if time remains):\n  \u2514\u2500\u2500 Query C: semantic doc search (LIMIT 2) \u2192 append to heuristics\n```\n\n### Cloud (frontier, 8K budget, 2000ms timeout)\n\n```\nParallel:\n  \u251c\u2500\u2500 Query A: fix_patterns (LIMIT 8) \u2192 playbooks\n  \u251c\u2500\u2500 Query E: error_heuristics \u2192 heuristics\n  \u251c\u2500\u2500 Query B: symbol neighborhood \u2192 related_symbols\n  \u2514\u2500\u2500 Query C: semantic doc search (LIMIT 4) \u2192 heuristics\nSequential:\n  \u2514\u2500\u2500 Query D: bridge docs for all symbols \u2192 append to heuristics\n```\n\n---\n\n## 11. SurrealDB Schema Requirements\n\nThe following tables must exist in the KG for queries to work:\n\n```surql\n-- Fix patterns (populated by post-fix analysis)\nDEFINE TABLE fix_pattern SCHEMAFULL;\nDEFINE FIELD error_category ON fix_pattern TYPE string;\nDEFINE FIELD error_code ON fix_pattern TYPE option<string>;\nDEFINE FIELD affected_symbol ON fix_pattern TYPE string;\nDEFINE FIELD file_path ON fix_pattern TYPE string;\nDEFINE FIELD fix_description ON fix_pattern TYPE string;\nDEFINE FIELD fix_diff_summary ON fix_pattern TYPE string;\nDEFINE FIELD success_count ON fix_pattern TYPE int DEFAULT 1;\nDEFINE FIELD last_applied ON fix_pattern TYPE datetime;\nDEFINE INDEX idx_fix_category ON fix_pattern FIELDS error_category;\n\n-- Code symbols (populated by codegraph-rust indexer)\nDEFINE TABLE symbol SCHEMAFULL;\nDEFINE FIELD name ON symbol TYPE string;\nDEFINE FIELD kind ON symbol TYPE string; -- struct, trait, fn, enum, impl\nDEFINE FIELD file_path ON symbol TYPE string;\nDEFINE FIELD line_number ON symbol TYPE option<int>;\nDEFINE INDEX idx_symbol_name ON symbol FIELDS name;\n\n-- Edges\nDEFINE TABLE calls SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE defines SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE implements SCHEMAFULL TYPE RELATION FROM symbol TO symbol;\nDEFINE TABLE documented_by SCHEMAFULL TYPE RELATION FROM symbol TO doc_chunk;\n\n-- Documentation chunks (populated by CocoIndex pipeline)\nDEFINE TABLE doc_chunk SCHEMAFULL;\nDEFINE FIELD title ON doc_chunk TYPE string;\nDEFINE FIELD content ON doc_chunk TYPE string;\nDEFINE FIELD source_file ON doc_chunk TYPE string;\nDEFINE FIELD embedding ON doc_chunk TYPE array; -- 3584-dim float32\nDEFINE INDEX idx_doc_embedding ON doc_chunk FIELDS embedding MTREE DIMENSION 3584;\n\n-- Error heuristics (curated reference)\nDEFINE TABLE error_heuristic SCHEMAFULL;\nDEFINE FIELD error_code ON error_heuristic TYPE string;\nDEFINE FIELD category ON error_heuristic TYPE string;\nDEFINE FIELD heuristic_text ON error_heuristic TYPE string;\nDEFINE FIELD common_fixes ON error_heuristic TYPE array;\nDEFINE FIELD complexity_level ON error_heuristic TYPE int;\nDEFINE FIELD frequency ON error_heuristic TYPE int DEFAULT 0;\nDEFINE INDEX idx_heuristic_code ON error_heuristic FIELDS error_code;\n```\n\n---\n\n## 12. Open Questions for Implementation Phase\n\n1. **Embedding model**: Which model generates the `$query_embedding` for semantic doc search? Candidates: Nomic Embed V2 (3584-dim, matches schema), or a smaller model if latency is a concern. Recommendation: Use the same model that CocoIndex used to generate `doc_chunk.embedding`.\n\n2. **Fix pattern ingestion**: When does `fix_pattern` get populated? Recommendation: After a successful verifier pass, extract a diff summary + error categories from the just-closed iteration and INSERT into `fix_pattern`. This should be a post-success hook in the orchestrator loop.\n\n3. **SurrealDB deployment**: Where does SurrealDB run? Options: (a) on slurm-ctl as a systemd service (recommended \u2014 stable, NFS-accessible), (b) as a SLURM job on vasp-02 (co-located with 14B model). Depends on beefcake-swarm-3is.1 (SurrealDB deployment research).\n\n4. **Error heuristic seeding**: The `error_heuristic` table needs initial population. Recommendation: Seed from the Rust compiler error index (https://doc.rust-lang.org/error_codes/) with hand-curated common_fixes for the top 20 error codes.\n\n---\n\n## 13. Migration Path\n\n### Phase 3 (current): Design only\n- This document defines the interface\n- NoopKgEnricher ships as default\n\n### Phase 4 (implementation):\n1. Add `kg` module with trait + noop impl\n2. Make `pack_retry()` async\n3. Update callers in swarm-agents orchestrator\n4. Implement SurrealKgEnricher behind `kg` feature flag\n5. Add fix_pattern ingestion hook to orchestrator\n6. Seed error_heuristic table\n7. Integration test with in-memory SurrealDB (`kv-mem` feature)\n\n---\n\nEND OF DESIGN", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "closed_at": "2026-02-13T08:31:48Z", "close_reason": "Design complete: KgEnricher trait, SurrealQL queries, token budgets, integration point (async pack_retry), graceful degradation strategy, schema requirements, and migration path all specified.", "labels": ["delegate:pal-consensus", "design", "epic:ukg"], "dependencies": [{"issue_id": "beefcake-swarm-3is.4.1", "depends_on_id": "beefcake-swarm-3is.4", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3is.5.1", "title": "Research self-learning architectures for coding agents", "description": "Research how SWE-agent, OpenHands, Goose, Cursor, Aider implement learning from past fixes. How do they store fix patterns, retrieve them, define success signals, prevent learning bad patterns? Also research DSPy feedback loops. Output: comparison matrix + recommended architecture.", "notes": "PAL consensus + also try Cloud Council tool (test its functionality). This is the highest-value research task. Compare: episodic memory, RAG over past runs, structured pattern extraction, embedding-based retrieval.\n\n## COMPREHENSIVE RESEARCH: Self-Learning Architectures for Coding Agents\n\n### Research Date: 2026-02-13\n### Methodology: Web search + deep dives on 7 systems + SurrealDB graph patterns + episodic memory literature\n\n---\n\n## 1. COMPARISON MATRIX\n\n| System | What Gets Stored | Retrieval Method | Success Signal | Bad Pattern Prevention | Storage Backend |\n|--------|-----------------|------------------|----------------|----------------------|-----------------|\n| SWE-agent (SWE-Bench-CL) | Problem summaries, solutions, rationales, tool usage, success status | FAISS vector index, cosine similarity on embeddings (text-embedding-3-small), top-k retrieval | Task completion status, forward/backward knowledge transfer metrics, CL-Plasticity/Stability | Success status tracked per memory; low inter-task similarity limits spurious transfer | FAISS vector store |\n| OpenHands | Agent orchestration state, task decomposition patterns, CI/CD pipeline results | REST API with sandboxed runtimes, human feedback loop | Human verification at checkpoints, CI/CD pipeline pass | 90/10 automation/human split; human oversight on strategy | In-memory + external integrations |\n| Goose (Block) | Recipes (YAML workflow definitions) with goals, required extensions, structured inputs, sub-recipes | MCP-based tool discovery, recipe matching | Recipe execution success, tool chain completion | Open-source community review; recipe versioning | File-based YAML recipes, versioned in git |\n| Cursor Composer | Project rules (.mdc files), user rules, team rules, agent rules (AGENTS.md) | Codebase-wide semantic search, self-summarization for long contexts | RL training signal (test pass, linter pass), reinforcement learning at scale | RL training in real codebases filters low-quality patterns naturally | Rules files in .cursor/rules/, MoE model weights |\n| Aider | Git commit history with descriptive messages, conversation context | Git-integrated diff history, /run command for test feedback | Test execution results, compilation success, git commit success | Git-based rollback; human review of each commit | Git repository (commit history as memory) |\n| DSPy | Optimized prompts, few-shot examples, bootstrap demonstrations | BootstrapFewShot selection, MIPROv2 instruction generation, GEPA (Genetic-Pareto) evolution | Metric-driven evaluation (user-defined), feedback loops | Metric quality bounds bad learning; GEPA reflective optimization allows course correction | In-memory pipeline parameters, serialized optimized programs |\n| Letta/MemGPT | Core memory blocks (in-context), archival memory (explicit knowledge), recall memory (conversation history), skills (.md files) | Vector search, full-text search, hybrid search across messages/tools; graph traversal | Task completion, user satisfaction, memory relevance | Sleep-time agents for async memory refinement; memory blocks with explicit constraints/limits | Postgres (default), SQLite, vector indexes, git-versioned skills |\n\n---\n\n## 2. DETAILED SYSTEM ANALYSES\n\n### 2.1 SWE-agent / SWE-Bench-CL (Princeton)\n\n**Architecture:** Agent-Computer Interface (ACI) design. The SWE-Bench-CL paper introduces continual learning where agents vectorize task experiences.\n\n**Storage Details:**\n- After each task: problem summary, solution approach, rationale, tool usage patterns, and success/failure status are vectorized\n- Uses OpenAI text-embedding-3-small for embeddings\n- FAISS vector index for storage and retrieval\n\n**Retrieval:**\n- On new task: query memory with current problem statement\n- Cosine similarity ranking\n- Top-k memories prepended to agent's initial prompt\n- Prioritizes experiences from same task sequence\n\n**Key Insight:** Low inter-task structural similarity is a major challenge. Contextual sensitivity (\"prompt poisoning\") means retrieved memories can hurt as much as help if not well-filtered.\n\n**Metrics:** CL-Plasticity (ability to learn new), CL-Stability (retention of old), forward/backward knowledge transfer.\n\n### 2.2 OpenHands (All Hands AI)\n\n**Architecture:** Multi-agent orchestration platform. Agents operate in sandboxed Docker environments.\n\n**Learning Model:** Primarily relies on human-in-the-loop rather than automated self-learning. The platform targets 90% automation / 10% human effort, with humans providing strategy and verification at checkpoints.\n\n**Key Insight:** OpenHands' value is in orchestration (coordinating agent fleets for large refactors), not in autonomous self-learning. Learning happens implicitly through the human feedback loop and agent orchestration patterns.\n\n### 2.3 Goose (Block)\n\n**Architecture:** MCP-based extensible agent framework. Uses \"recipes\" for reusable workflows.\n\n**Storage:** Recipes are YAML workflow definitions containing:\n- Goals\n- Required MCP extensions\n- Structured inputs\n- Sub-recipes (composable workflows)\n- Shared via git repositories\n\n**Learning:** Recipes represent manually curated \"learned\" patterns. No automated self-learning from past fixes. Instead, teams share and version recipes.\n\n**Key Insight:** Recipe-based approach is deterministic and auditable but doesn't learn autonomously. Good model for encoding expert knowledge (human writes recipe once, agents reuse forever).\n\n### 2.4 Cursor Composer\n\n**Architecture:** Custom MoE model trained via reinforcement learning in real codebases.\n\n**Learning:**\n- Model trained in agentic setting with access to semantic search + test runners\n- RL training teaches practical behaviors: running tests, fixing linters, navigating large projects\n- Self-summarization for long context management\n- Rules system for persistent project/user/team preferences\n\n**Key Insight:** Cursor's \"learning\" is baked into model weights via RL, not retrievable patterns. The Rules system (.cursor/rules/*.mdc) is the closest to explicit pattern storage, but requires manual authoring.\n\n### 2.5 Aider\n\n**Architecture:** Terminal-based pair programming assistant with deep git integration.\n\n**Learning:** Git-centric implicit learning:\n- Auto-commits with descriptive messages create an audit trail\n- /run command feeds test output back for iterative fixing\n- Architect mode \u2192 Code mode \u2192 Ask mode workflow\n- No explicit pattern storage or retrieval\n\n**Key Insight:** Aider's \"memory\" IS the git history. Clean commit messages serve as a form of episodic memory. The /run feedback loop (run tests \u2192 see errors \u2192 fix \u2192 repeat) is a lightweight self-learning mechanism but doesn't persist across sessions.\n\n### 2.6 DSPy (Stanford)\n\n**Architecture:** Declarative framework for programming LMs. Key innovation: programs (not prompts) that self-optimize.\n\n**Storage:**\n- Optimized few-shot examples (bootstrapped from training data)\n- Optimized instruction strings\n- Pipeline parameters (serializable)\n\n**Optimizers:**\n- BootstrapFewShot: Finds most effective training examples\n- MIPROv2: Generates optimized prompt instructions\n- GEPA (2025): Genetic-Pareto reflective optimizer that evolves textual components\n\n**Feedback Loop:**\n1. Define metric (e.g., test pass rate)\n2. Optimizer generates candidate prompts/examples\n3. Evaluate against metric\n4. Select best performers\n5. Iterate (can achieve 25-65% improvement over standard few-shot)\n\n**Key Insight:** DSPy is the most rigorous self-learning framework. Its metric-driven approach prevents bad learning naturally (bad patterns score low). However, it optimizes prompts/examples, not stored fix patterns per se. **Most applicable to beefcake-swarm:** DSPy's feedback loop model could be adapted to optimize how work packets are constructed.\n\n### 2.7 Letta/MemGPT (UC Berkeley)\n\n**Architecture:** Most sophisticated memory system. Three-tier OS-inspired memory hierarchy:\n\n**Tier 1 - Core Memory (RAM equivalent):**\n- In-context memory blocks with labels, descriptions, values, character limits\n- Always visible to the agent\n- Agent can self-edit these blocks\n- Used for: current user info, agent persona, active task context\n\n**Tier 2 - Recall Memory (Conversation History):**\n- Complete interaction history\n- Searchable via vector, full-text, or hybrid search\n- Supports pagination for large histories\n\n**Tier 3 - Archival Memory (Disk equivalent):**\n- Explicitly stored processed knowledge\n- Agent autonomously decides what to archive\n- Vector-indexed for semantic retrieval\n- No size limit\n\n**Additional: Skills (.md files):**\n- Learned task patterns stored as markdown\n- Versioned in git\n- Shareable between agents\n- Captures repeated patterns (DB migrations, API changes)\n\n**Memory Management:**\n- Sleep-time agents: async processes that refine/consolidate memory\n- Recursive summarization for context compression\n- Eviction strategies for less relevant tokens\n- Git-based versioning of agent memories\n\n**Key Insight:** Letta's architecture is the most directly applicable to beefcake-swarm. The tiered memory model maps naturally to: Core Memory = current work packet context, Recall Memory = past fix attempts for this issue, Archival Memory = cross-issue pattern knowledge base.\n\n---\n\n## 3. SURREALDB-SPECIFIC PATTERNS FOR FIX STORAGE\n\n### 3.1 Recommended Schema\n\n```surql\n-- Error pattern node\nDEFINE TABLE error_pattern SCHEMAFULL;\nDEFINE FIELD error_code ON error_pattern TYPE string;\nDEFINE FIELD error_category ON error_pattern TYPE string;  -- borrow_checker, lifetime, trait_bounds, type_mismatch, async_send\nDEFINE FIELD error_signature ON error_pattern TYPE string;  -- normalized error message\nDEFINE FIELD embedding ON error_pattern TYPE array<float>;  -- vector embedding of error context\nDEFINE FIELD frequency ON error_pattern TYPE int DEFAULT 0;\nDEFINE FIELD first_seen ON error_pattern TYPE datetime;\nDEFINE FIELD last_seen ON error_pattern TYPE datetime;\n\n-- Fix strategy node\nDEFINE TABLE fix_strategy SCHEMAFULL;\nDEFINE FIELD description ON fix_strategy TYPE string;\nDEFINE FIELD diff_template ON fix_strategy TYPE string;  -- generalized diff pattern\nDEFINE FIELD model_tier ON fix_strategy TYPE string;  -- which model tier produced this fix\nDEFINE FIELD confidence ON fix_strategy TYPE float;  -- 0.0-1.0 based on success rate\nDEFINE FIELD embedding ON fix_strategy TYPE array<float>;\nDEFINE FIELD times_applied ON fix_strategy TYPE int DEFAULT 0;\nDEFINE FIELD times_succeeded ON fix_strategy TYPE int DEFAULT 0;\n\n-- File context node\nDEFINE TABLE file_context SCHEMAFULL;\nDEFINE FIELD file_path ON file_context TYPE string;\nDEFINE FIELD crate_name ON file_context TYPE string;\nDEFINE FIELD key_symbols ON file_context TYPE array<string>;\n\n-- Relationships (graph edges)\n-- error_pattern -[FIXED_BY]-> fix_strategy\nDEFINE TABLE fixed_by SCHEMAFULL;\nDEFINE FIELD success_rate ON fixed_by TYPE float;\nDEFINE FIELD last_applied ON fixed_by TYPE datetime;\n\n-- error_pattern -[OCCURS_IN]-> file_context\nDEFINE TABLE occurs_in SCHEMAFULL;\nDEFINE FIELD frequency ON occurs_in TYPE int;\n\n-- fix_strategy -[MODIFIES]-> file_context\nDEFINE TABLE modifies SCHEMAFULL;\nDEFINE FIELD change_type ON modifies TYPE string;  -- add, remove, modify\n\n-- fix_strategy -[ESCALATED_FROM]-> fix_strategy\nDEFINE TABLE escalated_from SCHEMAFULL;\nDEFINE FIELD reason ON escalated_from TYPE string;\n\n-- error_pattern -[CO_OCCURS_WITH]-> error_pattern\nDEFINE TABLE co_occurs_with SCHEMAFULL;\nDEFINE FIELD frequency ON co_occurs_with TYPE int;\n```\n\n### 3.2 Retrieval Strategy: Hybrid (Graph + Vector)\n\n**Primary Query Pattern (Concept-Based):**\n1. Embed the current error context\n2. Vector search on error_pattern.embedding for top-5 similar errors\n3. Graph traverse: error_pattern -[FIXED_BY]-> fix_strategy (filter by confidence > 0.7)\n4. Rank fix strategies by: (semantic_similarity * 0.4) + (confidence * 0.3) + (recency * 0.2) + (model_tier_match * 0.1)\n\n```surql\n-- Example hybrid query\nLET $query_embedding = <embedding from current error>;\nLET $similar_errors = (\n    SELECT *, embedding <|5,40|> $query_embedding AS similarity\n    FROM error_pattern\n    WHERE error_category = $current_category\n);\nLET $fix_candidates = (\n    SELECT <-fixed_by<-fix_strategy.* AS strategy, \n           <-fixed_by.success_rate AS success_rate\n    FROM $similar_errors\n    WHERE <-fixed_by.success_rate > 0.7\n    ORDER BY success_rate DESC\n);\n```\n\n**Secondary Pattern (Co-occurrence Graph Walk):**\nWhen multiple errors appear together (common in Rust cascade failures):\n1. Find current error in graph\n2. Walk co_occurs_with edges to find related errors\n3. Find fixes that address the ROOT error (not the cascade)\n4. This prevents applying band-aid fixes to symptom errors\n\n### 3.3 Graph Traversal vs Vector Search vs Hybrid\n\n| Approach | Best For | Weakness |\n|----------|----------|----------|\n| Pure Vector | Novel errors never seen before | Misses structural relationships (A always follows B) |\n| Pure Graph | Known error cascades, dependency chains | Can't handle novel errors |\n| Hybrid (RECOMMENDED) | Both known and novel patterns | Slightly more complex queries |\n\n**Recommendation:** Start with hybrid. Vector search handles cold start and novel errors. Graph traversal improves as the KB grows and captures structural patterns the vector space misses.\n\n---\n\n## 4. COLD START / BOOTSTRAP STRATEGY\n\n### 4.1 Phase 1: Seed from Existing Data\n1. **Mine git history:** Parse past commits for error\u2192fix pairs. Use `git log --all -p` + rustc error parsing to extract (error_signature, diff, success=true) tuples\n2. **Mine beads notes:** Extract error descriptions and resolution notes from closed issues\n3. **Import SWE-Bench patterns:** Use published SWE-Bench fix patterns as initial seed (Rust-specific subset)\n\n### 4.2 Phase 2: Active Learning\n1. Every verifier pass/fail generates a (error_context, fix_attempt, outcome) tuple\n2. On success: create/update error_pattern \u2192 fix_strategy edge with incremented success count\n3. On failure: create/update edge with failure count, potentially create escalated_from link\n\n### 4.3 Phase 3: Consolidation\n1. Run periodic \"sleep-time\" consolidation (borrowing from Letta):\n   - Merge similar error_pattern nodes (cosine similarity > 0.95)\n   - Prune fix_strategy nodes with success_rate < 0.1 and times_applied > 5\n   - Generalize diff_templates from specific to abstract patterns\n\n### 4.4 Cold Start Mitigations\n- **Content-based fallback:** If KB has no relevant patterns, fall back to error_category-based heuristics (the router already classifies errors by type)\n- **Model-as-prior:** Use the LLM's pretrained knowledge as the initial \"memory\" until the KB builds up\n- **Transfer from documentation:** Seed KB with Rust compiler error explanations (rustc --explain EXXXX) mapped to common fix strategies\n\n---\n\n## 5. RETENTION / DECAY POLICIES\n\n### 5.1 Recommended Policy\n```\nConfidence decay: confidence *= 0.95 per week if not revalidated\nMinimum threshold: Remove patterns with confidence < 0.1 after 30 days\nFrequency boost: Recently-used patterns get confidence += 0.05 per successful application\nMaximum age: Patterns not used in 90 days get flagged for review\nNever delete: Patterns with success_rate > 0.9 and times_applied > 10 (these are \"golden rules\")\n```\n\n### 5.2 Preventing Bad Pattern Learning\n1. **Minimum evidence threshold:** Require times_applied >= 3 before a fix_strategy's confidence exceeds 0.5\n2. **Blind validation:** The validator agent (14B, no implementer context) independently verifies fixes - only validated fixes update confidence positively\n3. **Cascade detection:** If a fix introduces new errors, decrement confidence by 0.2 and create a co_occurs_with edge to the new error\n4. **Human veto:** Escalated-to-human fixes automatically get confidence capped at 0.3 until human approves\n5. **A/B testing:** Periodically apply KB-suggested fix vs. fresh LLM attempt on same error; only keep KB pattern if it outperforms\n\n---\n\n## 6. RECOMMENDED ARCHITECTURE FOR BEEFCAKE-SWARM\n\n### 6.1 Architecture: Hybrid Letta-Inspired + DSPy-Optimized + SurrealDB Graph\n\n**Layer 1: Working Memory (Work Packet)**\n- Current error context, file contexts, key symbols\n- Maps to Letta's \"Core Memory\"\n- Already implemented in work_packet/ module\n\n**Layer 2: Episodic Memory (SurrealDB Graph)**\n- Error patterns, fix strategies, relationships\n- Graph structure enables cascade detection and co-occurrence analysis\n- Vector embeddings enable novel error matching\n- Maps to Letta's \"Archival Memory\"\n\n**Layer 3: Strategic Memory (DSPy-optimized prompts)**\n- Optimized prompt templates per error category\n- Few-shot examples that have been validated as effective\n- Periodically re-optimized using DSPy's BootstrapFewShot against test suite metrics\n- Maps to Letta's \"Recall Memory\" but more structured\n\n**Feedback Loop (DSPy-inspired):**\n1. Error occurs \u2192 query Layer 2 (SurrealDB) for similar patterns\n2. If match found: inject fix strategy into work packet\n3. If no match: use Layer 3 optimized prompts for error category\n4. If still failing: escalate per existing escalation ladder\n5. After resolution: update Layer 2 with new evidence\n6. Periodically: re-optimize Layer 3 prompts using accumulated evidence\n\n### 6.2 Integration Points with Existing Codebase\n\n| Existing Module | Integration |\n|----------------|-------------|\n| verifier/ | Generates (error, outcome) tuples \u2192 feeds Layer 2 |\n| escalation/ | Queries Layer 2 before escalating; uses past fix data to skip unnecessary tiers |\n| router/ | Uses Layer 2 error_category statistics to improve routing accuracy |\n| work_packet/ | Enriched with Layer 2 fix suggestions before sending to model |\n| feedback/ | Closes the loop: compilation results update Layer 2 confidence scores |\n| ensemble/ | Multi-model voting results stored as evidence in Layer 2 |\n\n### 6.3 Implementation Priority\n1. SurrealDB schema + basic CRUD for error_pattern and fix_strategy\n2. Integration with verifier output parser (error \u2192 pattern extraction)\n3. Vector embedding pipeline (embed error contexts)\n4. Work packet enrichment (query KB before sending to model)\n5. Confidence update loop (success/failure feedback)\n6. DSPy prompt optimization layer (requires accumulated data)\n7. Sleep-time consolidation agent (periodic cleanup)\n\n---\n\n## 7. KEY TAKEAWAYS\n\n1. **No existing system does exactly what beefcake-swarm needs.** SWE-agent has the closest retrieval architecture (vector-based episodic memory), but lacks graph structure for error cascades. Letta has the best memory hierarchy, but isn't code-fix-specific. DSPy has the best feedback loop, but optimizes prompts not fix patterns.\n\n2. **The hybrid approach is novel and valuable.** Combining graph structure (for Rust error cascades, which are inherently relational) with vector search (for novel errors) with DSPy-style metric optimization (for prompt quality) creates a system more capable than any single existing approach.\n\n3. **SurrealDB is a strong fit** because it natively supports document + graph + vector in one database, eliminating the need to sync between a vector DB and a graph DB.\n\n4. **The cold start problem is solvable** by mining existing git history and beads notes, plus using the Rust compiler's own error explanations as seed data.\n\n5. **Blind validation (existing validator agent) is a critical anti-bad-learning mechanism** that most other systems lack. The beefcake-swarm architecture already has this with the independent 14B validator.\n\n---\n\nSources:\n- SWE-Bench-CL: https://arxiv.org/html/2507.00014v1\n- SWE-agent: https://github.com/princeton-nlp/SWE-agent\n- OpenHands: https://github.com/OpenHands/OpenHands\n- Goose: https://github.com/block/goose\n- Cursor: https://dasroot.net/posts/2026/02/cursor-ai-deep-dive-technical-architecture-advanced-features-best-practices/\n- Aider: https://github.com/Aider-AI/aider\n- DSPy: https://github.com/stanfordnlp/dspy / https://arxiv.org/abs/2310.03714\n- Letta/MemGPT: https://docs.letta.com/concepts/memgpt/ / https://www.letta.com/blog/letta-code / https://www.letta.com/blog/letta-v1-agent\n- SurrealDB KG-RAG: https://surrealdb.com/blog/knowledge-graph-rag-two-query-patterns-for-smarter-ai-agents\n- Episodic Memory in AI Agents: https://arxiv.org/html/2601.11653 / https://openreview.net/pdf?id=U51WxL382H\n- Agent Memory Systems: https://www.digitalapplied.com/blog/ai-agent-memory-systems-complete-guide", "status": "closed", "priority": 4, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T08:11:21Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:45Z", "closed_at": "2026-02-13T08:33:19Z", "close_reason": "Comprehensive research completed. Analyzed 7 systems (SWE-agent, OpenHands, Goose, Cursor, Aider, DSPy, Letta/MemGPT). Produced comparison matrix, SurrealDB schema design, hybrid retrieval strategy, cold start plan, retention policies, and recommended 3-layer architecture (Work Packet + SurrealDB Graph + DSPy Prompts). Key finding: no existing system does exactly what beefcake-swarm needs - the hybrid graph+vector+metric-optimization approach is novel.", "labels": ["delegate:pal-consensus", "epic:ukg", "research"], "dependencies": [{"issue_id": "beefcake-swarm-3is.5.1", "depends_on_id": "beefcake-swarm-3is.5", "type": "parent-child", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-3kk", "title": "Configure rig-core client timeouts", "description": "The Implementer and Validator create rig-core CompletionsClient instances without configuring timeouts. If the inference server hangs (GPU OOM, SLURM preemption, network partition), the HTTP request blocks indefinitely and the orchestrator loop stalls.\n\nFix: Check rig-core's client builder for timeout configuration:\n1. Set connect_timeout (5s) and request_timeout (5min for 72B reasoning, 2min for 14B fast)\n2. Add these as fields in SwarmConfig per endpoint\n3. If rig-core doesn't support timeouts directly, wrap the implement/validate calls with tokio::time::timeout()\n\nAlso consider: retry logic for transient HTTP failures (502, 503 from inference server starting up).\n\nFiles: crates/swarm-agents/src/implementer.rs, crates/swarm-agents/src/validator.rs, crates/swarm-agents/src/config.rs\nFound by: G3-Pro deep review", "status": "closed", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:34Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:34Z", "closed_at": "2026-02-16T13:01:34Z", "close_reason": "Partially addressed \u2014 10-min tokio::time::timeout wraps manager.prompt() in orchestrator.rs. Original references to implementer.rs/validator.rs are stale. Reopen if per-endpoint timeout config is needed later.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-3qg", "title": "Fix redundant verifier run in orchestrator loop", "description": "The orchestrator loop in main.rs runs the verifier TWICE per retry iteration: once at the end of iteration N (line 253) to check implementer output, then again at the start of iteration N+1 (line 217) to build retry context for pack_retry(). Rust compilation is expensive (~seconds per run); doubling it wastes time and cluster resources.\n\nFix: Carry the VerifierReport forward from the end of each loop iteration into the next via Option<VerifierReport>. Only run the verifier once per iteration. The report from the failed iteration already contains everything pack_retry() needs.\n\nFiles: crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:11:26Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:07Z", "closed_at": "2026-02-16T13:02:07Z", "close_reason": "Stale \u2014 references main.rs:253/217 which no longer exist. Current orchestrator.rs uses last_report to carry verifier results forward between iterations. No redundant verifier runs.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-4b7", "title": "Install notebooklm-mcp-cli and update global settings", "description": "Install jacob-bd/notebooklm-mcp-cli via uv, add Bash(nlm:*) to global settings permissions", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:38:54Z", "created_by": "claude-code", "updated_at": "2026-02-17T20:18:32Z", "closed_at": "2026-02-17T20:18:32Z", "close_reason": "Completed: nlm CLI installed via uv, global settings updated, auth configured", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-4j8", "title": "Use git status -z for robust porcelain parsing", "description": "FileWalker::modified_files() and WorktreeBridge merge checks parse 'git status --porcelain' output by slicing at [3..]. While porcelain v1 format is stable for normal cases, renamed files produce 'R  old -> new' format, and filenames containing spaces or special characters can cause incorrect parsing.\n\nFix: Switch to 'git status -z --porcelain' which uses NUL byte separators instead of newlines, handles renames as two separate NUL-separated entries, and correctly handles filenames with spaces, quotes, and unicode characters. Parse by splitting on \\0 instead of \\n.\n\nFiles: coordination/src/context_packer/file_walker.rs, coordination/src/work_packet/generator.rs\nFound by: G3-Pro deep review", "status": "closed", "priority": 3, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:14:36Z", "created_by": "TheFermiSea", "updated_at": "2026-02-12T11:14:26Z", "closed_at": "2026-02-12T11:14:26Z", "close_reason": "Fixed in PR #1 review commit a26ba46", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-5ja", "title": "Auto-fix commits .swarm-progress.txt into worktree history", "description": "merge_and_remove() cleans up .swarm-progress.txt from the working directory, but the auto-fix step (cargo clippy --fix + cargo fmt + git commit) commits the progress file into the branch before merge_and_remove runs. The artifact then gets merged into the target branch. Fix: exclude .swarm-progress.txt and .swarm-session.json from auto-fix commits, or add them to .gitignore in the worktree.", "status": "closed", "priority": 2, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T23:46:50Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T23:47:57Z", "closed_at": "2026-02-16T23:47:57Z", "close_reason": "Fixed: worktree creation now writes .gitignore excluding orchestrator artifacts", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-7cg", "title": "Manager re-invokes workers after tests pass, wasting turns", "description": "Job 1606: workers successfully wrote test_sanitize_id_edge_cases and all 30 tests passed, but the manager kept re-invoking workers (6 invocations, each burning 25 turns) to fix formatting and re-verify. Root cause: manager doesn't recognize when the core task is done. Needs: (1) verifier should signal pass/fail to manager, (2) manager should stop iterating when verifier passes, (3) consider adding a 'task complete' signal from worker back to manager.", "status": "closed", "priority": 2, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T22:32:49Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T22:38:22Z", "closed_at": "2026-02-15T22:38:22Z", "close_reason": "Verifier all_green is now authoritative success signal, reviewer loop removed", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-7ml", "title": "Auto-fix trivial verifier failures without LLM delegation", "description": "After job 1621: the orchestrator spent 45+ minutes delegating a missing-paren syntax error to the cloud manager, which then delegated to a local worker, which then needed to read the file, understand the error, write the fix \u2014 all for a single character.\n\nIMPLEMENTATION:\nAfter verifier fails and before routing to the manager, check if errors are \"trivially auto-fixable\":\n\n1. **Syntax errors with compiler suggestions**: If rustc provides a `suggested_replacement` in the diagnostic spans, apply it directly via string replacement.\n\n2. **Format-only failures**: If fmt is the only failing gate and the file parses (no syntax errors), just run `cargo fmt --` on the file (already happens in auto-format, but currently blocked by syntax errors).\n\n3. **Missing delimiters**: Parse the fmt/check error for patterns like \"mismatched closing delimiter\" or \"unclosed delimiter\" with line/column info. For simple cases (missing `)`, `}`, `]`), insert the character at the indicated position.\n\nAdd this as a `try_auto_fix()` method in the orchestrator that runs between verifier failure and manager delegation. If auto-fix succeeds and re-verification passes, skip the manager entirely for that iteration.\n\nThis is the \"dynamic delegation\" principle: don't send trivial work to expensive models.\n\nFiles to modify:\n- `crates/swarm-agents/src/orchestrator.rs`: Add try_auto_fix() between verifier and routing\n- `coordination/src/verifier/report.rs`: Add helper to extract fixable errors with suggestions", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T19:01:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T20:14:33Z", "closed_at": "2026-02-16T20:14:33Z", "close_reason": "Implemented try_auto_fix() Janitor layer: clippy --fix + fmt before LLM delegation", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-7ny", "title": "Extract agent traits for dependency injection and testability", "description": "Implementer and Validator are concrete structs that make real HTTP calls to inference endpoints. The orchestrator loop in main.rs instantiates them directly, making it impossible to unit test the loop logic without a running inference server.\n\nFix: Define traits in a new module (e.g. crates/swarm-agents/src/agents.rs):\n\npub trait ImplementerAgent {\n    async fn implement(&self, task_description: &str) -> Result<String>;\n}\n\npub trait ValidatorAgent {\n    async fn validate(&self, diff: &str) -> Result<ValidationResult>;\n}\n\nHave the existing Implementer and Validator implement these traits. Refactor main.rs to accept trait objects (Box<dyn ImplementerAgent>) or use generics. This enables mock implementations for testing.\n\nConsider using mockall or manual mock structs for integration tests of the orchestrator loop.\n\nFiles: crates/swarm-agents/src/implementer.rs, crates/swarm-agents/src/validator.rs, crates/swarm-agents/src/main.rs\nFound by: G3-Pro deep review", "status": "closed", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:13:27Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:29Z", "closed_at": "2026-02-16T13:01:29Z", "close_reason": "Stale \u2014 references implementer.rs and validator.rs which no longer exist. Architecture refactored to agents/manager.rs + agents/coder.rs with rig agent-as-tool pattern.", "dependency_count": 0, "dependent_count": 3, "comment_count": 0}
{"id": "beefcake-swarm-8nm", "title": "Build context packer for agent context windows", "description": "Agents need a repo packer to build context windows. Options: tree-sitter AST extraction, repomap (Aider-style), or custom Rust walker. Must respect .gitignore, count tokens, fit in model context. Can leverage indexing/index_flow_v2.py for semantic search.", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:14Z", "created_by": "TheFermiSea", "updated_at": "2026-02-11T16:02:59Z", "closed_at": "2026-02-11T16:02:59Z", "close_reason": "Closed", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-8td", "title": "GENERAL_CODER_PREAMBLE has truncated scope-discipline rule", "description": "prompts.rs has a malformed bullet in GENERAL_CODER_PREAMBLE where a scope-discipline sentence is cut off before completion, reducing instruction clarity for workers. Evidence: crates/swarm-agents/src/prompts.rs around lines 208-212 includes an unfinished sentence ending at 'if a file has 10 methods and your task is'. Fix by restoring a complete, syntactically correct rule and add a prompt-text regression test that checks key preamble fragments are intact.", "notes": "Found during dogfood R3 review.", "status": "closed", "priority": 2, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:25:01Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:29:35Z", "closed_at": "2026-02-17T09:29:35Z", "close_reason": "Fixed: restored truncated scope-discipline sentence in GENERAL_CODER_PREAMBLE at prompts.rs:210.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-9cg", "title": "Implement NotebookBridge module", "description": "Rust module in swarm-agents wrapping nlm CLI. KnowledgeBase trait, registry TOML parsing, graceful degradation. Following BeadsBridge pattern", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:39:36Z", "created_by": "claude-code", "updated_at": "2026-02-17T20:19:02Z", "closed_at": "2026-02-17T20:19:02Z", "close_reason": "Completed: notebook_bridge.rs implemented with KnowledgeBase trait, TOML registry, graceful degradation", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-9o4", "title": "Add query_notebook tool for Manager agent", "description": "Rig Tool implementation so Manager can query NotebookLM on-demand during task delegation", "status": "closed", "priority": 2, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T09:40:35Z", "created_by": "claude-code", "updated_at": "2026-02-17T20:19:32Z", "closed_at": "2026-02-17T20:19:32Z", "close_reason": "Completed: QueryNotebookTool implemented as Rig tool, wired into Manager agent builders", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-9w5", "title": "Increase SLURM time limit for swarm orchestrator (1h\u21924h)", "status": "closed", "priority": 2, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T18:45:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T19:47:29Z", "closed_at": "2026-02-15T19:47:29Z", "close_reason": "Fixed in dogfood round 2: 1h\u21924h", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-a58", "title": "Fix write_file tool: handle JSON-escaped content from local models", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T18:45:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T19:47:28Z", "closed_at": "2026-02-15T19:47:28Z", "close_reason": "Fixed in dogfood round 2", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-aq1", "title": "Design rig-core Manager-Worker migration plan", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-13T12:21:56Z", "created_by": "TheFermiSea", "updated_at": "2026-02-13T12:26:39Z", "closed_at": "2026-02-13T12:26:39Z", "close_reason": "Migration plan and tool skeleton delivered", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-axn", "title": "AST-aware context packing via tree-sitter", "description": "UNANIMOUS CONSENSUS (4/4 models)\n\nThe current ContextPacker uses 'first 30 lines of each file' as context \u2014 this is spatial, not semantic. All 4 reviewing models flagged this as a critical weakness causing hallucinated APIs and wasted context budget.\n\nReplace with AST-aware context extraction:\n1. Integrate tree-sitter-rust for Rust AST parsing\n2. Extract semantic units: full function bodies at error spans, struct/enum definitions, trait bounds, impl blocks\n3. Score context by relevance: error spans > call sites > modified files > imports > headers\n4. Include symbol-to-file map so the LLM knows 'where to edit' without searching\n\nSpecific improvements to build_file_contexts():\n- Instead of first 30 lines, extract the function/impl containing the error span\n- Include trait definitions referenced by trait bound errors\n- Include callers/callees of modified functions\n- Score each FileContext with a priority (Error > Modified > Dependency > Header)\n\nConsider: headless rust-analyzer integration as a higher-fidelity alternative to tree-sitter. Start with tree-sitter for speed, upgrade to RA later.\n\nDependencies: tree-sitter = '0.24', tree-sitter-rust\nFiles: coordination/src/context_packer/packer.rs, coordination/Cargo.toml", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:34:26Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:17Z", "closed_at": "2026-02-16T13:01:17Z", "close_reason": "Duplicate of beefcake-swarm-5bk (tree-sitter AST-aware context packing). Same feature, same files.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-b30", "title": "Implement archival memory with recall for retries (Letta/MemGPT pattern)", "description": "Steal Letta/MemGPT pattern: build RocksDB-backed tiered memory system. Working memory = current WorkPacket. Archival memory = queryable store of prior diffs, verifier reports, repeated compiler errors, decisions, design notes per bead_id. On pack_retry, recall relevant prior approaches that failed (FailedApproachIndex). Add memory promotion rules: successful fix patterns get promoted, failed approaches get stored with error context. Use existing RocksDB in coordination/src/state/. Add memory schema, summarizer hook, and recall policy. Files: coordination/src/state/ (new memory module), coordination/src/context_packer/packer.rs (recall integration). Medium effort, high payoff for multi-iteration issues.", "status": "closed", "priority": 4, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:49:40Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:07:24Z", "closed_at": "2026-02-17T09:07:24Z", "close_reason": "Superseded by UKG epic beefcake-swarm-3is.5.4 (tiered archival memory) which has proper dependency chain.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-cp8", "title": "Add cd and sed to exec_tool command allowlist", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T18:45:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T19:47:28Z", "closed_at": "2026-02-15T19:47:28Z", "close_reason": "Fixed in dogfood round 2", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-d15", "title": "Add OpenTelemetry tracing spans to orchestrator loop", "description": "No observability in the orchestrator loop means blind debugging when iterations fail. Add OpenTelemetry spans around: issue selection, worktree creation, context packing, implementer call, patch application, verifier pipeline, validator call, merge. Track metrics: pass rate per tier, iterations to green, tokens consumed, wallclock per phase, escalation frequency. Use tracing-opentelemetry + opentelemetry-otlp crates. Export to stdout/file initially, OTLP endpoint later. Files: crates/swarm-agents/Cargo.toml, crates/swarm-agents/src/main.rs. Claude Opus 4.5 rated this P0.", "status": "closed", "priority": 3, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:48:53Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:07:24Z", "closed_at": "2026-02-17T09:07:24Z", "close_reason": "Duplicate of beefcake-swarm-ibu (more specific OTel scope). Closing to reduce noise.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-e74", "title": "Validate write_file paths to prevent stray files in worktree root", "description": "Job 1606: model wrote lib.rs to worktree root instead of crates/swarm-agents/src/lib.rs. Add validation in WriteFileTool::call() to warn or reject writes to the worktree root directory (paths without any '/' separator), since legitimate writes are always to subdirectories.", "status": "closed", "priority": 3, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T22:32:49Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T22:38:22Z", "closed_at": "2026-02-15T22:38:22Z", "close_reason": "Added tracing::warn for write_file paths with no directory component", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-ekl", "title": "Fix context packer empty files on retry iterations (git diff HEAD returns nothing after commit)", "status": "closed", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:10:11Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T20:14:33Z", "closed_at": "2026-02-16T20:14:33Z", "close_reason": "Fixed: git_changed_files now tries main...HEAD and extracts paths from gate stderr", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-gbh", "title": "Add unit test for WorktreeBridge::sanitize_id edge cases", "description": "Add unit tests for WorktreeBridge::sanitize_id edge cases in crates/swarm-agents/src/worktree_bridge.rs.\n\nCurrent tests (test_sanitize_id at line ~250) only cover basic path traversal. Add tests for:\n1. Unicode input: sanitize_id(\"h\u00e9llo-w\u00f6rld\") should produce valid ASCII branch name\n2. Very long IDs (>255 chars): should truncate to safe length\n3. All-dots input: sanitize_id(\"...\") should not produce empty or dangerous result\n4. Only special chars: sanitize_id(\"@#$%^&\") should produce a usable ID\n5. Empty string: sanitize_id(\"\") should return a fallback or error\n6. Path separators: sanitize_id(\"foo/bar/../baz\") must not allow traversal\n\nAdd tests in the existing #[cfg(test)] mod tests block (line ~240). Each test should assert the output is safe for use as a git branch name (no slashes, no dots-only, non-empty, reasonable length).\n\nACCEPTANCE: cargo test -p swarm-agents test_sanitize passes. cargo fmt and cargo clippy clean.", "notes": "Job 1628 false positive: swarm auto-fixed a clippy lint but did not add any sanitize_id tests. Orchestrator accepted because all gates passed (no new code = no new failures). Need task-completeness verification.", "status": "closed", "priority": 0, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T13:31:40Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T02:57:28Z", "closed_at": "2026-02-17T06:42:44Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-h1n", "title": "Add unit tests for WorktreeBridge::sanitize_id edge cases", "description": "Add tests for WorktreeBridge::sanitize_id edge cases: unicode input, very long IDs (>255 chars), IDs that are all dots, IDs with only special characters, empty string input, IDs with path separators. File: crates/swarm-agents/src/worktree_bridge.rs. The sanitize_id function should handle all of these gracefully without panicking.", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2026-02-15T22:55:12Z", "created_by": "root", "updated_at": "2026-02-16T19:46:05Z", "closed_at": "2026-02-16T19:46:05Z", "close_reason": "Duplicate of beefcake-swarm-gbh", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-i42", "title": "Add explicit planning step for complex issues (RA.Aid pattern)", "description": "Currently the orchestrator goes straight from issue to implementation (main.rs:229). For complex issues, a planning phase reduces iteration count. Steal RA.Aid Research-Plan-Act pattern: (1) Add complexity_score to issue classification based on error categories and file count, (2) For complex issues (>3 files, trait/lifetime errors, escalated tasks): inject planning step where 72B model generates a concrete plan, (3) 14B Implementer executes the plan, (4) Store plan as artifact in RocksDB, include in WorkPacket on retry. Add plan field to WorkPacket struct. Low-medium effort, medium-high payoff. All 4 models recommended this pattern.", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:49:23Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:02:23Z", "closed_at": "2026-02-16T13:02:23Z", "close_reason": "Stale \u2014 references main.rs:229. Current architecture uses cloud manager (Opus 4.6-thinking) which IS the planner \u2014 it decomposes tasks and delegates to workers. Explicit planning step already exists via the manager agent.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-lzb", "title": "Implement escalation ladder: Implementer \u2192 Integrator \u2192 Cloud \u2192 Human", "description": "Wire the coordination crate's escalation engine into swarm-agents. Implementer tries 14B first, escalates to 72B Integrator, then Cloud Council (external API), finally Human. Use existing EscalationEngine and SwarmTier types from coordination/src/escalation/.", "status": "closed", "priority": 2, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:27Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:46Z", "closed_at": "2026-02-16T13:01:46Z", "close_reason": "Blocked by beefcake-swarm-tup (closed as done). Escalation ladder is already implemented in orchestrator.rs \u2014 Implementer\u2192Integrator\u2192Cloud tiers with budget tracking in EscalationState.", "dependencies": [{"issue_id": "beefcake-swarm-lzb", "depends_on_id": "beefcake-swarm-tup", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 1, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-r93", "title": "Wire beads_bridge to orchestrator main loop", "description": "Connect beads_bridge.rs to the main orchestrator loop. List open issues, pick highest priority, update status to in_progress, close on completion. Use br CLI --json output for machine-readable parsing.", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:20Z", "created_by": "TheFermiSea", "updated_at": "2026-02-11T16:02:59Z", "closed_at": "2026-02-11T16:02:59Z", "close_reason": "Closed", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-rb2", "title": "Add worktree cleanup method for merge failure recovery", "description": "Add force_remove() method to WorktreeBridge in crates/swarm-agents/src/worktree_bridge.rs.\n\nWorktreeBridge::merge_and_remove() bails on merge conflict, leaving zombie worktrees and branches. Need a separate force_remove(issue_id) that always cleans up.\n\nADD to impl WorktreeBridge:\npub fn force_remove(&self, issue_id: &str) -> Result<()> {\n    let safe_id = Self::sanitize_id(issue_id);\n    let wt_path = self.base_dir.join(&safe_id);\n    let branch = format!(\"swarm/{safe_id}\");\n    // 1. git worktree remove --force <wt_path> (ignore errors if already gone)\n    // 2. git branch -D <branch> (ignore errors if already gone)\n    // 3. git worktree prune\n}\n\nThen in crates/swarm-agents/src/orchestrator.rs process_issue(), call force_remove() in the error/failure path (after the main loop exits without success) so zombie worktrees don't block future runs.\n\nACCEPTANCE: cargo test -p swarm-agents passes. Add a test that creates a worktree, simulates failure, calls force_remove, then verifies the worktree and branch are gone. cargo fmt and cargo clippy clean.", "status": "closed", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:11:31Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T21:07:07Z", "closed_at": "2026-02-16T21:07:07Z", "close_reason": "Implemented by swarm job 1623. cleanup() method + orchestrator wiring + 2 tests. All gates passed.", "dependency_count": 0, "dependent_count": 3, "comment_count": 0}
{"id": "beefcake-swarm-rib", "title": "Fix git object permissions in worktree commit step", "description": "Job 1606: orchestrator exits with 'insufficient permission for adding an object to repository database .git/objects'. Worktree links to main repo .git/objects which gets owned by root after git pull as root. Fix: (1) SLURM script should chown .git/objects after pull, (2) orchestrator should pull as brian not root, (3) consider git config safe.directory.", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-15T22:32:49Z", "created_by": "TheFermiSea", "updated_at": "2026-02-15T22:38:22Z", "closed_at": "2026-02-15T22:38:22Z", "close_reason": "git add now handles permissions gracefully + SLURM script chowns .git before run", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-rzr", "title": "Integrate Gastown for workspace isolation", "description": "Wire up gastown CLI calls from swarm-agents orchestrator. Create worktrees per agent task on NFS (/cluster/shared/wt/<issue_id>). Prevent file conflicts for parallel agents. Commands: gastown create, gastown merge.", "status": "closed", "priority": 1, "issue_type": "task", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:17Z", "created_by": "TheFermiSea", "updated_at": "2026-02-11T16:02:59Z", "closed_at": "2026-02-11T16:02:59Z", "close_reason": "Closed", "dependency_count": 0, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-s2f", "title": "Manager prompt must instruct stop-on-success after verifier passes", "description": "Job 1608: manager (Opus 4.6-thinking) wrote tests at depth 5, tests passed at depth 9, but continued spawning workers for 45+ more minutes. The manager prompt needs explicit instructions: once cargo test passes and verifier is green, IMMEDIATELY return the result summary. Do not continue iterating or spawning additional workers.", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T08:53:41Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T08:55:25Z", "closed_at": "2026-02-16T08:55:25Z", "close_reason": "Added CRITICAL: Stop When Done section to both cloud and local manager prompts", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-tup", "title": "Implement 2-agent loop MVP (orchestrator \u2192 implementer \u2192 verifier \u2192 validator)", "description": "Core loop: orchestrator picks beads issue, creates gastown worktree, runs implementer (72B), deterministic verifier (fmt/clippy/test), blind validator (14B). If pass: merge+close. If fail: update notes, retry.", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T15:32:10Z", "created_by": "TheFermiSea", "updated_at": "2026-02-16T13:01:13Z", "closed_at": "2026-02-16T13:01:13Z", "close_reason": "Already implemented \u2014 orchestrator.rs has the full manager\u2192workers\u2192verifier loop with escalation. All 3 deps (8nm, r93, rzr) were closed.", "dependencies": [{"issue_id": "beefcake-swarm-tup", "depends_on_id": "beefcake-swarm-8nm", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-tup", "depends_on_id": "beefcake-swarm-r93", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}, {"issue_id": "beefcake-swarm-tup", "depends_on_id": "beefcake-swarm-rzr", "type": "blocks", "created_at": "2026-02-17T16:47:40Z", "created_by": "bootstrap", "metadata": "{}"}], "dependency_count": 3, "dependent_count": 1, "comment_count": 0}
{"id": "beefcake-swarm-vl2", "title": "WorktreeBridge::create should clean up stale branches before creating", "description": "WorktreeBridge::create() in crates/swarm-agents/src/worktree_bridge.rs fails if branch swarm/<id> already exists from a previous failed run.\n\nFIX in create() method (around line 74):\n1. Before 'git worktree add -b swarm/<id>', check if branch exists: git branch --list swarm/<id>\n2. If branch exists, delete it: git branch -D swarm/<id>\n3. Also prune stale worktrees: git worktree prune\n4. Then proceed with the normal git worktree add\n\nAlso check if worktree directory already exists at base_dir/<id> and remove it first (rm -rf or git worktree remove --force).\n\nACCEPTANCE: cargo test -p swarm-agents passes. Add a test that calls create() twice with the same ID (second call should succeed after cleanup). cargo fmt and cargo clippy clean.", "status": "closed", "priority": 0, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T07:53:55Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T02:57:28Z", "closed_at": "2026-02-17T07:32:39Z", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-xom", "title": "Add patch-based editing tool (apply_patch) to reduce scope creep from full file writes", "status": "closed", "priority": 0, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-16T20:20:43Z", "created_by": "TheFermiSea", "updated_at": "2026-02-17T09:06:45Z", "closed_at": "2026-02-17T09:06:45Z", "close_reason": "Implemented as edit_file (search/replace tool) in patch_tool.rs. Research showed unified diffs are worst format for LLMs; search/replace is what Aider, OpenHands, Claude Code converged on. 7 tests, clippy clean, wired into all 3 worker builders, prompts updated to v4.5.0.", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-swarm-y2d", "title": "Sanitize issue IDs to prevent path traversal", "description": "WorktreeBridge::worktree_path() does base_dir.join(issue_id) with no validation. If issue_id contains path separators (e.g. ../../etc or ../../../tmp/evil), it could write worktrees outside the base directory.\n\nFix: Add validate_issue_id() that rejects any ID not matching [a-zA-Z0-9_-]. Call it in create(), merge_and_remove(), and worktree_path(). Beads IDs are alphanumeric-with-hyphens so this won't break real usage.\n\nFiles: crates/swarm-agents/src/worktree_bridge.rs\nFound by: G3-Pro deep review", "status": "closed", "priority": 1, "issue_type": "bug", "owner": "squires.b@gmail.com", "created_at": "2026-02-11T16:11:35Z", "created_by": "TheFermiSea", "updated_at": "2026-02-12T11:14:26Z", "closed_at": "2026-02-12T11:14:26Z", "close_reason": "Fixed in PR #1 review commit a26ba46", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
{"id": "beefcake-ws2", "title": "Deploy nlm CLI to HPC cluster for swarm NotebookLM integration", "description": "Install uv + nlm on NFS, copy auth tokens, update SLURM script with bind mounts and env vars so the swarm container can query NotebookLM notebooks", "status": "closed", "priority": 1, "issue_type": "feature", "owner": "squires.b@gmail.com", "created_at": "2026-02-17T18:28:39Z", "created_by": "claude-code", "updated_at": "2026-02-17T19:39:44Z", "closed_at": "2026-02-17T19:39:44Z", "close_reason": "nlm CLI deployed to cluster: uv+nlm installed on NFS, auth copied, SLURM script updated, verified working inside Apptainer container (version check, notebook list, query all pass)", "dependency_count": 0, "dependent_count": 0, "comment_count": 0}
